{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6fd0b50",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4646ba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /media/SharedUsers/dlc19/home/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# All imports \n",
    "##################\n",
    "\n",
    "#sys libs\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data manupulation libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "#pandarallel.initialize()\n",
    "\n",
    "#string manupulation libs\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "\n",
    "#torch libs\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "# data manipulations\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import pydicom\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e6f0a",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61506dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>50414267</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>53189527</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>53911762</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>56699142</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s56699142/ea030e7a-2e3b134...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>57375967</td>\n",
       "      <td>10000764</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000764/s57375967/096052b7-d256dc4...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  study_id  subject_id  split  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n",
       "1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n",
       "2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n",
       "3  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  56699142    10000032  train   \n",
       "4  096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4  57375967    10000764  train   \n",
       "\n",
       "                                                path  \\\n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...   \n",
       "1  files/p10/p10000032/s53189527/2a2277a9-b0ded15...   \n",
       "2  files/p10/p10000032/s53911762/68b5c4b1-227d048...   \n",
       "3  files/p10/p10000032/s56699142/ea030e7a-2e3b134...   \n",
       "4  files/p10/p10000764/s57375967/096052b7-d256dc4...   \n",
       "\n",
       "                                          raw_report  word_count  \n",
       "0                                   FINAL REPORT\\...          91  \n",
       "1                                   FINAL REPORT\\...          77  \n",
       "2                                   FINAL REPORT\\...          70  \n",
       "3                                   FINAL REPORT\\...          72  \n",
       "4                                   FINAL REPORT\\...          89  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used to make the csv with raw reports, not really important anymore once we have the reports in a csv\n",
    "cxr_root_path = \"/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/\"\n",
    "df_record = pd.read_csv('{}/cxr-record-list.csv.gz'.format(cxr_root_path), sep=',')\n",
    "df_split = pd.read_csv('{}/mimic-cxr-2.0.0-split.csv.gz'.format(cxr_root_path))\n",
    "\n",
    "df_temp = df_split.merge(df_record, on=['subject_id', 'study_id', 'dicom_id'], how='left')\n",
    "\n",
    "df_sections = pd.read_csv('{}/mimic-cxr-sections/mimic_cxr_sectioned.csv'.format(cxr_root_path))\n",
    "\n",
    "# if you already have the df_raw_reports.csv just uncomment the following line and skipp to the vocabulary class\n",
    "\n",
    "# extract the csv into a dataframe\n",
    "df_raw_reports = pd.read_csv('df_raw_reports.csv')\n",
    "# discard duplicates of the reports as there were originally more rows than reports (as there is sometimes more images per study)\n",
    "df_raw_reports= df_raw_reports.drop_duplicates(subset = [\"raw_report\"])\n",
    "#restart the index column as it was filtered from larger data and the indices were messed up\n",
    "df_raw_reports.reset_index(inplace=True)\n",
    "#discard some unimportant columns\n",
    "df_raw_reports = df_raw_reports.drop('index', 1)\n",
    "df_raw_reports = df_raw_reports.drop('Unnamed: 0', 1)\n",
    "# print the head to check if looks like intended\n",
    "df_raw_reports.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d06db048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377110, 4)\n",
      "5159\n"
     ]
    }
   ],
   "source": [
    "# Checking test dataset number\n",
    "df_split.head()\n",
    "print(df_split.shape)\n",
    "\n",
    "split = df_split[\"split\"].tolist()\n",
    "idx = 0\n",
    "for elem in split:\n",
    "    if elem == \"test\":\n",
    "        idx+=1\n",
    "\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5d1f0",
   "metadata": {},
   "source": [
    "### Creating raw reports dataframe (Skip if done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22e1cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_sections = df_sections.rename(columns={'study': 'study_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ee8635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the raw report to the sections df\n",
    "\n",
    "\n",
    "paths = df_record[\"path\"].tolist()\n",
    "study_ids = df_record[\"study_id\"].tolist()\n",
    "\n",
    "study_ids_new=[]\n",
    "\n",
    "for study in study_ids:\n",
    "    \n",
    "    new_id = 's' + str(study)\n",
    "    study_ids_new.append(new_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb3b2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p10\n",
      "p11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9ce47c1e5491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mdf_raw_reports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_raw_reports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9ce47c1e5491>\u001b[0m in \u001b[0;36msearchDirectory\u001b[0;34m(mainDirectory, df_raw_reports)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubDirectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch171/lib/python3.6/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creates dataframe with study id and raw reports, prints the name of the main directory on enter so you can see the progress\n",
    "# overall there is p10-p19 directories, should take around 25-30 min to run as far as I remember\n",
    "\n",
    "import ntpath\n",
    "\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def searchDirectory(mainDirectory,df_raw_reports):\n",
    "\n",
    "\n",
    "    directory = '/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/files/' + mainDirectory\n",
    "    report_ext = '.txt'\n",
    "    image_ext = '.jpg'\n",
    "    \n",
    "    print(mainDirectory)\n",
    "    for subDirectory in os.listdir(directory):\n",
    "        full_path = directory + '/' + subDirectory\n",
    "       \n",
    "        for root, dirs, files in os.walk(full_path):\n",
    "            \n",
    "            for filename in files:\n",
    "\n",
    "                if filename.endswith(report_ext):\n",
    "                    with open(os.path.join(root, filename), 'r') as report:\n",
    "                        contents = report.read()\n",
    "                    \n",
    "                    df_temp = {\"study_id\": path_leaf(filename)[1:9], \"raw_report\": contents}\n",
    "                    df_raw_reports = df_raw_reports.append(df_temp, ignore_index = True)\n",
    "        \n",
    "    return df_raw_reports\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "directories = ['p10','p11','p12','p13','p14','p15','p16','p17', 'p18', 'p19' ]\n",
    "\n",
    "\n",
    "df_raw_reports = pd.DataFrame([], columns = ['study_id', 'raw_report'])\n",
    "\n",
    "for directory in directories:\n",
    "    df_raw_reports = searchDirectory(directory, df_raw_reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eca47f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>50414267</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>53189527</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>53911762</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>56699142</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s56699142/ea030e7a-2e3b134...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>57375967</td>\n",
       "      <td>10000764</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000764/s57375967/096052b7-d256dc4...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  study_id  subject_id  split  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n",
       "1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n",
       "2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n",
       "3  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  56699142    10000032  train   \n",
       "4  096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4  57375967    10000764  train   \n",
       "\n",
       "                                                path  \\\n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...   \n",
       "1  files/p10/p10000032/s53189527/2a2277a9-b0ded15...   \n",
       "2  files/p10/p10000032/s53911762/68b5c4b1-227d048...   \n",
       "3  files/p10/p10000032/s56699142/ea030e7a-2e3b134...   \n",
       "4  files/p10/p10000764/s57375967/096052b7-d256dc4...   \n",
       "\n",
       "                                          raw_report  \n",
       "0                                   FINAL REPORT\\...  \n",
       "1                                   FINAL REPORT\\...  \n",
       "2                                   FINAL REPORT\\...  \n",
       "3                                   FINAL REPORT\\...  \n",
       "4                                   FINAL REPORT\\...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_reports.head()\n",
    "df_raw_reports.to_csv('raw_reports.csv', index=False)\n",
    "\n",
    "\n",
    "df_raw_reports = pd.read_csv('df_raw_reports.csv')\n",
    "\n",
    "df_raw_reports= df_raw_reports.drop_duplicates(subset = [\"raw_report\"])\n",
    "df_raw_reports.reset_index(inplace=True)\n",
    "df_raw_reports = df_raw_reports.drop('index', 1)\n",
    "df_raw_reports = df_raw_reports.drop('Unnamed: 0', 1)\n",
    "df_raw_reports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594f3bb",
   "metadata": {},
   "source": [
    "### Classes for the Data Loader\n",
    "- Include: loading data, text preprocessing, words frequency check, tokenization, tokens-IDs-conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b5be7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7c1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Vocabulary class\n",
    "##############################\n",
    "\n",
    "class Vocabulary:\n",
    "  \n",
    "    '''\n",
    "    __init__ method is called by default as soon as an object of this class is initiated\n",
    "    we use this method to initiate our vocab dictionaries\n",
    "    '''\n",
    "    def __init__(self, freq_threshold, max_size):\n",
    "        '''\n",
    "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
    "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
    "        '''\n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        \n",
    "    '''\n",
    "    a simple tokenizer to split on space and converts the sentence to list of words\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def preprocessing(text):\n",
    "        cleanedReport = re.sub(r'[^\\w\\s]','',text)            # remove punctuation (not word characters and whitespace)\n",
    "        cleanedReport = re.sub('_', '', cleanedReport)        # remove __ in the report\n",
    "        cleanedReport = re.sub(r'[\\d-]', '', cleanedReport)   # remove numbers in the report \n",
    "        cleanedReport = re.sub('\\n', '', cleanedReport)\n",
    "\n",
    "        return cleanedReport                                  # should be just a string\n",
    "    \n",
    "    '''\n",
    "    vocabulary frequency check:\n",
    "    '''\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        # calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
    "        frequencies = {}  #init the freq dict\n",
    "        #idx = 4 #index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk\n",
    "        idx = 4\n",
    "        \n",
    "        i=0\n",
    "        for sentence in sentence_list:\n",
    "            i+=1\n",
    "            if (i % 10000 == 0):\n",
    "                print(i) \n",
    "                \n",
    "            preprocessed = self.preprocessing(sentence)\n",
    "            preprocessedList = list(preprocessed.split(\" \"))\n",
    "            for word in preprocessedList:\n",
    "                if word not in frequencies.keys():\n",
    "                    frequencies[word]=1\n",
    "                else:\n",
    "                    frequencies[word]+=1\n",
    "              \n",
    "                    \n",
    "        #limit vocab by removing low freq words\n",
    "        frequencies = {k:v for k,v in frequencies.items() if v > self.freq_threshold} \n",
    "        \n",
    "        #limit vocab to the max_size specified\n",
    "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
    "            \n",
    "        return frequencies\n",
    "\n",
    "            \n",
    "    '''\n",
    "    tokenization and converting tokens to IDs\n",
    "    '''    \n",
    "    def tokenization(self, text):\n",
    "        \n",
    "        # change here to use different tokenizer models:\n",
    "\n",
    "        # add tokenizer\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        tokenized_text = [word for word in tokenized_text if word not in self.stop_words]    # Stopword removal\n",
    "        \n",
    "        tokenized_report = [\"[CLS]\"]\n",
    "        tokenized_report += tokenized_text\n",
    "        tokenized_report.append(\"[SEP]\")\n",
    "        numericalized_report = tokenizer.convert_tokens_to_ids(tokenized_report)\n",
    "        \n",
    "#         # Another encoding method which works on bert-base-uncased tokenizer but not working well with not the others...\n",
    "#         text_str = ' '.join([str(elem) for elem in text])\n",
    "#         encoded = tokenizer.encode_plus(text=text_str,  # the sentence to be encoded\n",
    "#                                         add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "#                                         return_attention_mask = True,  # Generate the attention mask\n",
    "#                                         #return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "#                                         )\n",
    "        \n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "770b1756",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Train dataset\n",
    "###############\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_train, target_column, transform=None, freq_threshold = 0,\n",
    "                vocab_max_size = 5000):\n",
    "       \n",
    "        self.df_train = df_train\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.target_text = df_train[target_column]\n",
    "\n",
    "        self.report_vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
    "        self.frequencyDict = self.report_vocab.build_vocabulary(self.target_text.tolist())  # build vocab for whole thing (list of whole thing)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_train)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "  \n",
    "        target_text = self.target_text[index]                             # string\n",
    "        target_text = self.report_vocab.preprocessing(target_text)\n",
    "        \n",
    "        for word in target_text.split():\n",
    "            if word not in self.frequencyDict.keys():\n",
    "                target_text = target_text.replace(word, \"\")\n",
    "            \n",
    "        numericalized_report = self.report_vocab.tokenization(target_text)\n",
    "        \n",
    "        # uncomment the following to convert the list to tensor and return (apparently BERT works without tensor)\n",
    "        # return torch.tensor(numericalized_report)\n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29d702",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be7ecc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a little example on just 10 reports\n",
    "#df_train_test = df_raw_reports[0:10]\n",
    "df_train_test = df_raw_reports[0:10]\n",
    "\n",
    "train_dataset = TrainDataset(df_train_test, \"raw_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "617f2d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 175, 1207, 15415, 14375, 1116, 174, 7501, 8974, 5531, 2229, 185, 1161, 11937, 7577, 3839, 9505, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 2620, 4248, 15070, 6719, 3621, 2660, 16418, 2050, 14196, 27316, 2999, 16973, 1933, 1286, 13093, 9046, 1439, 7209, 3077, 1181, 3105, 14701, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 16530, 1286, 3971, 5001, 10346, 2382, 8351, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102]\n",
      "\n",
      "\n",
      "Number of tokens:  91\n"
     ]
    }
   ],
   "source": [
    "# we can see the results, e.g.\n",
    "example = train_dataset[0]\n",
    "print(example)\n",
    "print(\"\\n\")\n",
    "print(\"Number of tokens: \", len(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4db00a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens conversion:\n",
      "['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'la', '##t', 'indication', 'f', 'new', 'onset', '##cite', '##s', 'e', '##val', 'infection', 'technique', 'chest', 'p', '##a', 'lateral', 'comparison', 'none', 'findings', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'p', '##ne', '##um', '##oth', '##orax', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', 'likely', 'represent', 'nipple', 'shadows', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'normal', 'clips', 'project', 'left', 'lung', 'potentially', 'within', 'breast', 'image', '##d', 'upper', 'abdomen', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'posterior', 'left', 'sixth', 'seventh', 'ribs', 'noted', 'impression', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]']\n",
      "\n",
      "\n",
      "Decoding:\n",
      "[CLS] final report examination chest pa lat indication f new onsetcites eval infection technique chest pa lateral comparison none findings focal consolidation pleural effusion pneumothorax bilateral nodular opacities likely represent nipple shadows cardiomediastinal silhouette normal clips project left lung potentially within breast imaged upper abdomen unremarkable chronic deformity posterior left sixth seventh ribs noted impression acute cardiopulmonary process [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Conversion back to tokens:\n",
    "print(\"Tokens conversion:\")\n",
    "print(tokenizer.convert_ids_to_tokens(example))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Decode it back into sentences (tokens of subwords will be merged together)\n",
    "print(\"Decoding:\")\n",
    "print(tokenizer.decode(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07b342",
   "metadata": {},
   "source": [
    "# Chunking to unify token lengths of each sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0997017",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Chunking Function \n",
    "####################\n",
    "\n",
    "def chunking(dataset, df, chunk_size = 128):\n",
    "    \n",
    "    concat = []\n",
    "    for i in range(len(dataset)):     # Add all tokens together in one list\n",
    "        concat.extend(dataset[df.index[i]])\n",
    "        \n",
    "    total_length = len(concat)\n",
    "    print(\"Total concatenated length: \", total_length)\n",
    "    \n",
    "    chunks = lambda concat, chunk_size: [concat[i:i+chunk_size] for i in range(0, total_length, chunk_size)]  \n",
    "    chunked_text = chunks(concat, chunk_size)\n",
    "    \n",
    "    if len(chunked_text[-1]) < chunk_size:\n",
    "        print(\"Removed last chunk: \", len(chunked_text[-1]), \" tokens\")\n",
    "        chunked_text.pop()     # remove last element \n",
    "    \n",
    "    return chunked_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6a6f1",
   "metadata": {},
   "source": [
    "##### (Example - for 3 reports):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24153857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total concatenated length:  406\n",
      "Removed last chunk:  22  tokens\n",
      "<class 'list'>\n",
      "3\n",
      "\n",
      "\n",
      "[CLS] final report chest radiographs history question right shoulder fracture patient also presents chest wall pain comparisons technique chest pa lateral findings heart mild moderately enlarged mediastinal hilar contours appear unchanged calcification along archorta well moderate unfolding moderate relative elevation right hemidiaphragm somewhat greater prior examination definite pleural effusion pneumothorax mild diffuse interstitial abnormality suggests mild fluidload pulmonary vascular congestion fissures also appear thickened degenerative changes incompletely characterized along right shoulder severe rightward spinal curvature centered along mid lumbar spine moderate leftward\n",
      "\n",
      "\n",
      "lower thoracic curvature impression findings suggesting mild pulmonary vascular congestion [SEP] [CLS] final report examination chest radiograph indication yearold man presenting hour chest pain evaluate consolidation technique chest pa lateral comparison chest radiograph portablep dated findings lung volumes low linearfrahilar streaky opacities consistent atelectasis althoughpiration possible focal consolidation edema effusion pneumothorax rounded opacities projecting left lateral hemithorax frontal view could potentially reflect pulmonary nodules heart mildly enlarged median sternotomy wires mediastinal clips appear intactsseous structures appear dense consistent history metastatic prostate cancer multilevel degenerative changes\n",
      "\n",
      "\n",
      "thoracic spine mild acutesseous abnormality impression low lung volumes atelectasis possiblepiration mild cardiomegaly without evidence edema dense bones consistent history metastatic prostate cancer possible left pulmonary nodules recommend correlation prior imaging currently available pacs recommendations recommend correlation prior imaging currently available pacs evaluate left pulmonary nodules [SEP] [CLS] final report indication large left pleural effusion suspected pneumonia patient pigtail catheter placed evaluation interval change comparison findings pa lateral chest radiographs leftsided pigtail catheter stable position moderate left pleural effusion unchanged mild right basilar atelectasis still\n"
     ]
    }
   ],
   "source": [
    "example_df = df_raw_reports.sample(n=3)\n",
    "example_dataset = TrainDataset(example_df, \"raw_report\")\n",
    "chunked_text = chunking(example_dataset, example_df)\n",
    "print(type(chunked_text))\n",
    "print(len(chunked_text))\n",
    "\n",
    "for chunk in chunked_text:\n",
    "    print(\"\\n\")\n",
    "    print(tokenizer.decode(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3bce9a",
   "metadata": {},
   "source": [
    "## Chunking all training reports (Skip if done!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bd67cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "Total concatenated length:  25994905\n",
      "Removed last chunk:  25  tokens\n"
     ]
    }
   ],
   "source": [
    "# # Uncomment if you want to do it yourself!\n",
    "# # For all training reports:\n",
    "# trainDf = df_raw_reports[0:len(df_raw_reports.index)]\n",
    "# train_dataset = TrainDataset(trainDf, \"raw_report\")\n",
    "# chunked_train_dataset = chunking(train_dataset, trainDf)    # chunk size = 128 \n",
    "\n",
    "# # Save as .csv file:\n",
    "# chunked_trainDF = pd.DataFrame(chunked_train_dataset)\n",
    "# chunked_trainDF.to_csv(\"chunked_trainDatasetAll.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3863c0e",
   "metadata": {},
   "source": [
    "Each row contains one training sample with 128 tokens, in total there are 204085 training samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646708fb",
   "metadata": {},
   "source": [
    "### Reading the chunked train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f77af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunked samples:     154696\n",
      "Number of tokens per samples:  128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>1509</td>\n",
       "      <td>2592</td>\n",
       "      <td>8179</td>\n",
       "      <td>2229</td>\n",
       "      <td>185</td>\n",
       "      <td>1161</td>\n",
       "      <td>2495</td>\n",
       "      <td>1204</td>\n",
       "      <td>12754</td>\n",
       "      <td>...</td>\n",
       "      <td>5815</td>\n",
       "      <td>14255</td>\n",
       "      <td>18834</td>\n",
       "      <td>1116</td>\n",
       "      <td>2999</td>\n",
       "      <td>26600</td>\n",
       "      <td>191</td>\n",
       "      <td>2225</td>\n",
       "      <td>21608</td>\n",
       "      <td>5332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2999</td>\n",
       "      <td>8682</td>\n",
       "      <td>2330</td>\n",
       "      <td>185</td>\n",
       "      <td>1513</td>\n",
       "      <td>12602</td>\n",
       "      <td>174</td>\n",
       "      <td>3101</td>\n",
       "      <td>17268</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>3161</td>\n",
       "      <td>1197</td>\n",
       "      <td>15342</td>\n",
       "      <td>1548</td>\n",
       "      <td>5531</td>\n",
       "      <td>22172</td>\n",
       "      <td>2229</td>\n",
       "      <td>2070</td>\n",
       "      <td>21217</td>\n",
       "      <td>3836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5351</td>\n",
       "      <td>10170</td>\n",
       "      <td>1700</td>\n",
       "      <td>7577</td>\n",
       "      <td>2070</td>\n",
       "      <td>21217</td>\n",
       "      <td>9505</td>\n",
       "      <td>8682</td>\n",
       "      <td>2330</td>\n",
       "      <td>17811</td>\n",
       "      <td>...</td>\n",
       "      <td>11019</td>\n",
       "      <td>1233</td>\n",
       "      <td>6617</td>\n",
       "      <td>11531</td>\n",
       "      <td>1116</td>\n",
       "      <td>1762</td>\n",
       "      <td>1499</td>\n",
       "      <td>2999</td>\n",
       "      <td>2060</td>\n",
       "      <td>8351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17811</td>\n",
       "      <td>20994</td>\n",
       "      <td>1286</td>\n",
       "      <td>13093</td>\n",
       "      <td>2259</td>\n",
       "      <td>3566</td>\n",
       "      <td>4311</td>\n",
       "      <td>22631</td>\n",
       "      <td>20673</td>\n",
       "      <td>2129</td>\n",
       "      <td>...</td>\n",
       "      <td>1849</td>\n",
       "      <td>2554</td>\n",
       "      <td>17811</td>\n",
       "      <td>20994</td>\n",
       "      <td>185</td>\n",
       "      <td>1513</td>\n",
       "      <td>12602</td>\n",
       "      <td>174</td>\n",
       "      <td>3101</td>\n",
       "      <td>17268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185</td>\n",
       "      <td>1673</td>\n",
       "      <td>1818</td>\n",
       "      <td>12858</td>\n",
       "      <td>25632</td>\n",
       "      <td>175</td>\n",
       "      <td>14687</td>\n",
       "      <td>26600</td>\n",
       "      <td>5048</td>\n",
       "      <td>14494</td>\n",
       "      <td>...</td>\n",
       "      <td>5815</td>\n",
       "      <td>14255</td>\n",
       "      <td>18834</td>\n",
       "      <td>1116</td>\n",
       "      <td>16684</td>\n",
       "      <td>1353</td>\n",
       "      <td>185</td>\n",
       "      <td>1513</td>\n",
       "      <td>12602</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1     2      3      4      5      6      7      8      9  ...  \\\n",
       "0    101   1509  2592   8179   2229    185   1161   2495   1204  12754  ...   \n",
       "1   2999   8682  2330    185   1513  12602    174   3101  17268    185  ...   \n",
       "2   5351  10170  1700   7577   2070  21217   9505   8682   2330  17811  ...   \n",
       "3  17811  20994  1286  13093   2259   3566   4311  22631  20673   2129  ...   \n",
       "4    185   1673  1818  12858  25632    175  14687  26600   5048  14494  ...   \n",
       "\n",
       "     118    119    120    121    122    123    124   125    126    127  \n",
       "0   5815  14255  18834   1116   2999  26600    191  2225  21608   5332  \n",
       "1   3161   1197  15342   1548   5531  22172   2229  2070  21217   3836  \n",
       "2  11019   1233   6617  11531   1116   1762   1499  2999   2060   8351  \n",
       "3   1849   2554  17811  20994    185   1513  12602   174   3101  17268  \n",
       "4   5815  14255  18834   1116  16684   1353    185  1513  12602    174  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_trainDataset = pd.read_csv('chunked_trainDatasetAllNoStopWord.csv')\n",
    "chunked_trainDataset = chunked_trainDataset.drop('Unnamed: 0', 1)\n",
    "print(\"Number of chunked samples:    \", chunked_trainDataset.shape[0])\n",
    "print(\"Number of tokens per samples: \", chunked_trainDataset.shape[1])\n",
    "chunked_trainDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2505e0",
   "metadata": {},
   "source": [
    "##### Example in converting 3 samples (first 3 rows) back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eaa494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the list:   154696\n",
      "<class 'list'>\n",
      "\n",
      "\n",
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 175, 1207, 15415, 14375, 1116, 174, 7501, 8974, 5531, 2229, 185, 1161, 11937, 7577, 3839, 9505, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 2620, 4248, 15070, 6719, 3621, 2660, 16418, 2050, 14196, 27316, 2999, 16973, 1933, 1286, 13093, 9046, 1439, 7209, 3077, 1181, 3105, 14701, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 16530, 1286, 3971, 5001, 10346, 2382, 8351, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 1607, 175, 1603, 1757, 2184, 5531, 2229, 185, 1161, 11937, 7577, 9505, 17688, 2394, 2050, 14196, 20844, 5815, 14255, 18834, 1116, 2999, 26600, 191, 2225, 21608, 5332]\n",
      "['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'la', '##t', 'indication', 'f', 'new', 'onset', '##cite', '##s', 'e', '##val', 'infection', 'technique', 'chest', 'p', '##a', 'lateral', 'comparison', 'none', 'findings', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'p', '##ne', '##um', '##oth', '##orax', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', 'likely', 'represent', 'nipple', 'shadows', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'normal', 'clips', 'project', 'left', 'lung', 'potentially', 'within', 'breast', 'image', '##d', 'upper', 'abdomen', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'posterior', 'left', 'sixth', 'seventh', 'ribs', 'noted', 'impression', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]', '[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'la', '##t', 'indication', 'history', 'f', 'short', '##ness', 'breath', 'technique', 'chest', 'p', '##a', 'lateral', 'comparison', 'findings', 'cardiac', 'media', '##st', '##inal', 'hi', '##lar', 'con', '##tour', '##s', 'normal', 'pulmonary', 'v', '##as', '##cula', '##ture']\n",
      "\n",
      "\n",
      "[2999, 8682, 2330, 185, 1513, 12602, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 1675, 2967, 16973, 1562, 20266, 1286, 7209, 6456, 1286, 25984, 23298, 22869, 1116, 1145, 7160, 8351, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 22832, 1785, 102, 101, 1509, 2592, 8179, 2229, 15139, 1643, 12754, 175, 21810, 12104, 1965, 7577, 2229, 2070, 15241, 9505, 1423, 22172, 2458, 2229, 2136, 17811, 20994, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 3621, 2660, 16418, 2050, 14196, 27316, 2999, 1562, 2967, 16973, 20266, 1286, 7209, 6456, 1286, 25984, 23298, 22869, 1116, 1714, 1586, 1268, 23123, 2386, 1465, 7880, 20484, 1306, 1562, 8351, 12104, 4487, 1582, 6533, 6617, 1665, 1965, 102, 101, 1509, 2592, 12754, 1214, 1385, 1590, 172, 3161, 1197, 15342, 1548, 5531, 22172, 2229, 2070, 21217, 3836]\n",
      "['normal', 'lungs', 'clear', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'p', '##ne', '##um', '##oth', '##orax', 'present', 'multiple', 'clips', 'seen', 'projecting', 'left', 'breast', 'remote', 'left', '##sided', 'rib', 'fracture', '##s', 'also', 'demonstrated', 'impression', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'abnormal', '##ity', '[SEP]', '[CLS]', 'final', 'report', 'examination', 'chest', 'portable', '##p', 'indication', 'f', 'cough', 'acute', 'process', 'comparison', 'chest', 'radio', '##graph', 'findings', 'single', 'frontal', 'view', 'chest', 'provided', 'focal', 'consolidation', 'e', '##ff', '##usion', 'p', '##ne', '##um', '##oth', '##orax', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'normal', 'seen', 'multiple', 'clips', 'projecting', 'left', 'breast', 'remote', 'left', '##sided', 'rib', 'fracture', '##s', 'free', 'air', 'right', 'hem', '##id', '##ia', '##ph', '##rag', '##m', 'seen', 'impression', 'acute', '##tra', '##th', '##ora', '##ci', '##c', 'process', '[SEP]', '[CLS]', 'final', 'report', 'indication', 'year', 'old', 'woman', 'c', '##ir', '##r', '##hos', '##is', 'technique', 'frontal', 'chest', 'radio', '##graphs', 'obtained']\n",
      "\n",
      "\n",
      "[5351, 10170, 1700, 7577, 2070, 21217, 9505, 8682, 2330, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 1762, 2060, 2999, 2394, 2050, 14196, 14255, 18834, 1116, 2999, 2967, 13467, 16973, 1933, 1286, 7209, 1385, 1286, 23298, 22869, 1116, 2382, 8351, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 177, 1183, 26466, 1465, 185, 1605, 22631, 7577, 3839, 9505, 185, 1161, 11937, 4696, 2229, 2136, 8682, 26449, 20725, 17811, 20994, 1286, 13093, 2259, 4903, 11937, 23123, 2386, 1465, 7880, 20484, 1306, 10496, 26557, 4035, 18791, 20512, 20557, 15328, 185, 1513, 12602, 3528, 4777, 3621, 2660, 16418, 2050, 14196, 27316, 9495, 12148, 1596, 9072, 11019, 1233, 6617, 11531, 1116, 1762, 1499, 2999, 2060, 8351]\n",
      "['patient', 'upright', 'position', 'comparison', 'radio', '##graphs', 'findings', 'lungs', 'clear', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'p', '##ne', '##um', '##oth', '##orax', 'heart', 'size', 'normal', 'media', '##st', '##inal', 'con', '##tour', '##s', 'normal', 'multiple', 'surgical', 'clips', 'project', 'left', 'breast', 'old', 'left', 'rib', 'fracture', '##s', 'noted', 'impression', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]', '[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'la', '##t', 'indication', 'h', '##y', '##pox', '##ia', 'p', '##na', '##piration', 'comparison', 'none', 'findings', 'p', '##a', 'lateral', 'views', 'chest', 'provided', 'lungs', 'adequately', '##erated', 'focal', 'consolidation', 'left', 'lung', 'base', 'adjacent', 'lateral', 'hem', '##id', '##ia', '##ph', '##rag', '##m', 'mild', 'vascular', 'en', '##gor', '##gement', 'bilateral', '##pical', 'p', '##le', '##ural', 'thick', '##ening', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'remarkable', '##ort', '##ic', 'arch', 'ca', '##l', '##ci', '##fication', '##s', 'heart', 'top', 'normal', 'size', 'impression']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunked_dfList = chunked_trainDataset.values.tolist()   # It is a big list containing 203085 samples (list)\n",
    "print(\"Length of the list:  \", len(chunked_dfList))\n",
    "print(type(chunked_dfList[0]))\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(chunked_dfList[i])\n",
    "    print(tokenizer.convert_ids_to_tokens(chunked_dfList[i]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b3f31",
   "metadata": {},
   "source": [
    "# Pad all the reports \n",
    "### Padding and removing outliers from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0200a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(dataset, df, maxlength = 300):\n",
    "    \n",
    "    paddedTextDf = []\n",
    "    for i in range(len(dataset)):\n",
    "        paddedText = dataset[df.index[i]]        \n",
    "        tokenCount = len(paddedText)\n",
    "        \n",
    "        if tokenCount >= 20 and tokenCount <= 300:       # remove samples less than 20 tokens or more than 300 tokens\n",
    "            while tokenCount < 300:\n",
    "                tokenCount += 1\n",
    "                paddedText.append(0)       # add PAD token (id = 0) until 300 tokens\n",
    "                \n",
    "            paddedTextDf.append(paddedText)\n",
    "    \n",
    "    return paddedTextDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589cbe9",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54a2d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = df_raw_reports.sample(n=3)\n",
    "example_dataset = TrainDataset(example_df, \"raw_report\")\n",
    "padded_text = padding(example_dataset, example_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa7c951c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[CLS] final report examination chest portablep indication hypoxiafiltrates comparison findingsp portable upright view chest overlying ekg leads present left cp angles partially excluded lungs clear cardiomediastinal silhouette normal stable bony structures intact impression pneumonia [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Length of tokens:  300\n",
      "\n",
      "\n",
      "[CLS] final report examination chest portablep indication year old woman fever hx pnacurrent pna technique chest portablep comparison impression heart size mediastinum stable right picc line tip level superiorvc brachiocephalic veinvc junction right heart size mediastinum stable lungs clear except faint opacity right lower lung potentially might represent developing infectious process minimal amount right pleural effusion cannot excluded gastrostomy projecting left upper quadrant [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Length of tokens:  300\n",
      "\n",
      "\n",
      "[CLS] final report pa lateral views chest reason exam status post ercp bleeding fever prior studies available comparison cardiac size top normal bibasilar atelectases otherwise lungs clear pneumothorax pleural effusion pulmonary edema enlargementzygos vein suggests fluidload [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Length of tokens:  300\n"
     ]
    }
   ],
   "source": [
    "for pad in padded_text:\n",
    "    print(\"\\n\")\n",
    "    print(tokenizer.decode(pad))\n",
    "    print()\n",
    "    print(\"Length of tokens: \", len(pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98110f6a",
   "metadata": {},
   "source": [
    "## Padding all Reports (Skip if done!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d34b4952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all training samples:  222337\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of all training samples: \", len(df_raw_reports.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "babfd0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "Number of samples:  222062\n"
     ]
    }
   ],
   "source": [
    "# trainDf = df_raw_reports[0:len(df_raw_reports.index)]\n",
    "# train_dataset = TrainDataset(trainDf, \"raw_report\")\n",
    "# padded_train_dataset = padding(train_dataset, trainDf)\n",
    "# print(\"Number of samples: \", len(padded_train_dataset))\n",
    "\n",
    "# # Save as .csv file:\n",
    "# padded_trainDF = pd.DataFrame(padded_train_dataset)\n",
    "# padded_trainDF.to_csv(\"padded_trainDataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402b990",
   "metadata": {},
   "source": [
    "### Reading the chunked train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d14d2048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:     222062\n",
      "Number of tokens per samples:  300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>1509</td>\n",
       "      <td>2592</td>\n",
       "      <td>8179</td>\n",
       "      <td>2229</td>\n",
       "      <td>185</td>\n",
       "      <td>1161</td>\n",
       "      <td>2495</td>\n",
       "      <td>1204</td>\n",
       "      <td>12754</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>1509</td>\n",
       "      <td>2592</td>\n",
       "      <td>8179</td>\n",
       "      <td>2229</td>\n",
       "      <td>185</td>\n",
       "      <td>1161</td>\n",
       "      <td>2495</td>\n",
       "      <td>1204</td>\n",
       "      <td>12754</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>1509</td>\n",
       "      <td>2592</td>\n",
       "      <td>8179</td>\n",
       "      <td>2229</td>\n",
       "      <td>15139</td>\n",
       "      <td>1643</td>\n",
       "      <td>12754</td>\n",
       "      <td>175</td>\n",
       "      <td>21810</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>1509</td>\n",
       "      <td>2592</td>\n",
       "      <td>12754</td>\n",
       "      <td>1214</td>\n",
       "      <td>1385</td>\n",
       "      <td>1590</td>\n",
       "      <td>172</td>\n",
       "      <td>3161</td>\n",
       "      <td>1197</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>1509</td>\n",
       "      <td>2592</td>\n",
       "      <td>8179</td>\n",
       "      <td>2229</td>\n",
       "      <td>185</td>\n",
       "      <td>1161</td>\n",
       "      <td>2495</td>\n",
       "      <td>1204</td>\n",
       "      <td>12754</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2      3     4      5     6      7     8      9  ...  290  \\\n",
       "0  101  1509  2592   8179  2229    185  1161   2495  1204  12754  ...    0   \n",
       "1  101  1509  2592   8179  2229    185  1161   2495  1204  12754  ...    0   \n",
       "2  101  1509  2592   8179  2229  15139  1643  12754   175  21810  ...    0   \n",
       "3  101  1509  2592  12754  1214   1385  1590    172  3161   1197  ...    0   \n",
       "4  101  1509  2592   8179  2229    185  1161   2495  1204  12754  ...    0   \n",
       "\n",
       "   291  292  293  294  295  296  297  298  299  \n",
       "0    0    0    0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_trainDataset = pd.read_csv(\"padded_trainDataset.csv\")\n",
    "padded_trainDataset = padded_trainDataset.drop('Unnamed: 0', 1)\n",
    "print(\"Number of training samples:    \", padded_trainDataset.shape[0])\n",
    "print(\"Number of tokens per samples: \", padded_trainDataset.shape[1])\n",
    "padded_trainDataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
