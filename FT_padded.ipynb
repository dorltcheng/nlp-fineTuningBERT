{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea99a921",
   "metadata": {},
   "source": [
    "# Fine-tuning ClinicalBERT with MIMIC-CXR data \n",
    "- Version: padded\n",
    "### Access all dataset here: \n",
    "https://imperiallondon-my.sharepoint.com/:f:/g/personal/dlc19_ic_ac_uk/EgobSIgJFitCuMdL0Sg6KmABP7qqtibuOz1R1jIZDEX22Q?e=U5CfiK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f4e8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /media/SharedUsers/dlc19/home/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data manupulation libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "#pandarallel.initialize()\n",
    "\n",
    "#string manupulation libs\n",
    "#string manupulation libs\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#torch libs\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments\n",
    "\n",
    "# data manipulations\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import pydicom\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf751bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(1)) # use gpu 1\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d33cd",
   "metadata": {},
   "source": [
    "### Data Preprocessing - done separately from data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54ac168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_temp = pd.read_csv('train_raw_reports.csv')\n",
    "# df_test_temp = pd.read_csv('test_raw_reports.csv')\n",
    "\n",
    "# #convetring study id to string as it doesn't work as an int\n",
    "# df_train_temp['study_id']=df_train_temp['study_id'].astype(str)\n",
    "# df_test_temp['study_id']=df_test_temp['study_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43015b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display raw reports in a dataframe\n",
    "# df_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c5603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocessing of the train dataset\n",
    "# # stop_words = set(stopwords.words('english'))\n",
    "# def preprocessing(text):\n",
    "#         cleanedReport = re.sub(r'[^\\w\\s]','',text)            # remove punctuation (not word characters and whitespace)\n",
    "#         cleanedReport = re.sub('_', '', cleanedReport)        # remove __ in the report\n",
    "#         cleanedReport = re.sub(r'[\\d-]', '', cleanedReport)   # remove numbers in the report \n",
    "#         cleanedReport = re.sub('\\n', '', cleanedReport)\n",
    "        \n",
    "#         return cleanedReport   \n",
    "    \n",
    "\n",
    "# def preprocessDataframe(df):\n",
    "    \n",
    "#     i = 0\n",
    "    \n",
    "#     for i in range(len(df)):\n",
    "        \n",
    "#         preprocessedText = preprocessing(df.at[i, \"raw_report\"])\n",
    "    \n",
    "#         df.at[i,'raw_report'a] = preprocessedText\n",
    "#         i = i + 1 \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a865167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess the raw reports in the dataframe \n",
    "# df_train_preprocessed = preprocessDataframe(df_train_temp)\n",
    "# df_test_preprocessed = preprocessDataframe(df_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8b2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_preprocessed.head() # check if that worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a398014",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "Don't actually know whether we should be doing this. If we do, need to make our own list (big problem with getting rid of negations and changing meaning).\n",
    "If you want to use, comment out and use \"df_trained_preprocessed_nostopwords\" to make csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaaadb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example. Notice the reports have different meanings because of removal of words like 'no'\n",
    "# print(df_train_preprocessed['raw_report'][0])\n",
    "# words = [word for word in df_train_preprocessed['raw_report'][0].split() if word.lower() not in stop_words ]\n",
    "# new_text = \" \".join(words)\n",
    "\n",
    "# print()\n",
    "# print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab93de",
   "metadata": {},
   "source": [
    "#### Uncomment to remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801e488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_preprocessed_nostopwords = df_train_preprocessed.copy()\n",
    "# l = len(df_train_preprocessed['raw_report'])\n",
    "# for i in range(0,l):\n",
    "#     if i%10000 == 0:\n",
    "#         print(i) # just to check progress - should take 10-15mins, 220,000 reports\n",
    "#     words = [word for word in df_train_preprocessed['raw_report'][i].split() if word.lower() not in stop_words]\n",
    "#     new_report = \" \".join(words)\n",
    "#     df_train_preprocessed_nostopwords['raw_report'][i] = new_report\n",
    "\n",
    "# df_test_preprocessed_nostopwords = df_test_preprocessed.copy()\n",
    "# l = len(df_test_preprocessed['raw_report'])\n",
    "# for i in range(0,l):\n",
    "#     words = [word for word in df_test_preprocessed['raw_report'][i].split() if word.lower() not in stop_words]\n",
    "#     new_report = \" \".join(words)\n",
    "#     df_test_preprocessed_nostopwords['raw_report'][i] = new_report\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eabc34",
   "metadata": {},
   "source": [
    "#### Checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b871fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_train_preprocessed['raw_report'][10030])\n",
    "# print()\n",
    "# print(df_train_preprocessed_nostopwords['raw_report'][10030])\n",
    "# print()\n",
    "# print(df_test_preprocessed['raw_report'][2000])\n",
    "# print()\n",
    "# print(df_test_preprocessed_nostopwords['raw_report'][2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad2761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if removing stopwords, change these to _nostopwords versions.\n",
    "# df_train_preprocessed.to_csv('train_preprocessed.csv', index=False)\n",
    "# df_test_preprocessed.to_csv('test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44219bd3",
   "metadata": {},
   "source": [
    "### Loading data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0045f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cd976364fa85dbba\n",
      "Reusing dataset csv (/media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-cd976364fa85dbba/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16526cd9fedf49338dfd201bee377074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['study_id', 'raw_report'],\n",
       "        num_rows: 222337\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['study_id', 'raw_report'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"train_preprocessed.csv\", \"test\": \"test_preprocessed.csv\"}\n",
    "reports_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "reports_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "937c87ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 FINAL REPORT EXAMINATION  CHEST PA AND LAT  INDICATION  F with new onset ascites   eval for infection  TECHNIQUE  Chest PA and lateral  COMPARISON  None  FINDINGS   There is no focal consolidation pleural effusion or pneumothorax  Bilateral nodular opacities that most likely represent nipple shadows The cardiomediastinal silhouette is normal  Clips project over the left lung potentially within the breast The imaged upper abdomen is unremarkable Chronic deformity of the posterior left sixth and seventh ribs are noted  IMPRESSION   No acute cardiopulmonary process\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(reports_dataset[\"train\"][\"raw_report\"][0])\n",
    "print(type(reports_dataset[\"train\"][\"raw_report\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80685ec",
   "metadata": {},
   "source": [
    "### Data Loader - Padding\n",
    "- Include: loading data, text preprocessing, words frequency check, tokenization, tokens-IDs-conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "padsize = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "739d9a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-cd976364fa85dbba/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-e549f7f84f54e327.arrow\n",
      "Loading cached processed dataset at /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-cd976364fa85dbba/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-eb4036fc2652fffe.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 222337\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function_padding(dataset):\n",
    "    result = tokenizer(dataset[\"raw_report\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = reports_dataset.map(\n",
    "    tokenize_function_padding, batched=True, remove_columns=[\"raw_report\", \"study_id\"])\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7ad7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-cd976364fa85dbba/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-1e107e678d4f9a23.arrow\n",
      "Loading cached processed dataset at /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-cd976364fa85dbba/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-178bf7936c7e7de3.arrow\n"
     ]
    }
   ],
   "source": [
    "def padding(dataset):\n",
    "    \n",
    "    num_items = len(dataset['input_ids']) # to get number of all items in train dataset\n",
    "        \n",
    "    while(len(dataset['input_ids']) < padsize):\n",
    "        dataset['input_ids'].append(0)\n",
    "        dataset['token_type_ids'].append(0)\n",
    "        dataset['attention_mask'].append(0)\n",
    "        dataset['word_ids'].append(0)\n",
    "\n",
    "    dataset['labels'] = dataset['input_ids'].copy()\n",
    "    return dataset\n",
    "\n",
    "padded_dataset = tokenized_datasets.map(padding, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a4a4155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_dataset[\"test\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee135562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-cd976364fa85dbba/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-30ec09aaec7ad9ad.arrow\n",
      "Loading cached processed dataset at /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-cd976364fa85dbba/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-e875dd6ef505ad50.arrow\n"
     ]
    }
   ],
   "source": [
    "smaller_dataset = padded_dataset.filter(lambda example: len(example['input_ids'])<=padsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e2e3540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-cd976364fa85dbba/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-34cb2502896db7e8.arrow\n",
      "Loading cached processed dataset at /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-cd976364fa85dbba/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-6999c1b631150c05.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 221006\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_dataset = smaller_dataset\n",
    "\n",
    "def check_function(dataset):\n",
    "\n",
    "    lenReport = len(dataset['input_ids'])\n",
    "    \n",
    "    if (lenReport > padsize):\n",
    "        print('yes')\n",
    "    return dataset\n",
    "\n",
    "padded_dataset.map(check_function, batched = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "740e220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d52ee2",
   "metadata": {},
   "source": [
    "### If loading from JSON files, use this:\n",
    "- JSON aren't working rn, hence dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51dc5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_files = {\n",
    "#     \"train\" : \"final_dataset_chunkedtrain.jsonl\",\n",
    "#     \"test\" : \"final_dataset_chunkedtest.jsonl\"    \n",
    "# }\n",
    "\n",
    "# chunkedDataset = load_dataset(\"json\", data_files = data_files)\n",
    "# chunkedDataset\n",
    "\n",
    "\n",
    "# #testing\n",
    "# data_files_padded = {\n",
    "#     \"train\" : \"final_dataset_padded_train.jsonl\",\n",
    "#     \"test\" : \"final_dataset_padded_test.jsonl\"    \n",
    "# }\n",
    "# paddedDataset = load_dataset(\"json\", data_files = data_files_padded)\n",
    "# chunkedDataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adeebd",
   "metadata": {},
   "source": [
    "# MASKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bfe734",
   "metadata": {},
   "source": [
    "### Report example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c658ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # same report, padded\n",
    "# example = padded_dataset[\"train\"][\"input_ids\"][0]\n",
    "# print(\"Type: \", type(example))\n",
    "# print(\"Length: \", len(example))\n",
    "# print(example)\n",
    "# tokenizer.decode(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d08ff280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole word masking: want to mask all the tokens that correspond to a single word. \n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "def whole_word_masking_data_collator(features, wwm_prob=0.15):\n",
    "    \n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\") #to fit into default_data_collator\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word: # removing repeat word_ids \n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "            #created a list where each index is a whole word, as a list of the indices of it's tokens\n",
    "            \n",
    "        mask = np.random.binomial(1, wwm_prob, (len(mapping),))#each whole word rather than each token has equal chance of selection for masking\n",
    "\n",
    "        input_ids = feature['input_ids']\n",
    "        #print(f'Length of input_ids is  {len(input_ids)}')\n",
    "        labels = feature['labels']\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id    \n",
    "                \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ad876",
   "metadata": {},
   "source": [
    "## Masking Padded Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94160ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> ['[CLS]', 'final', '[MASK]', '[MASK]', 'chest', 'p', '##a', 'and', 'la', '##t', '[MASK]', 'f', '[MASK]', 'new', 'onset', 'as', '##cite', '##s', 'e', '##val', 'for', 'infection', 'technique', '[MASK]', 'p', '##a', 'and', 'lateral', 'comparison', '[MASK]', '[MASK]', 'there', 'is', 'no', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'or', 'p', '##ne', '##um', '##oth', '##orax', '[MASK]', '[MASK]', '[MASK]', 'op', '##ac', '##ities', 'that', '[MASK]', '[MASK]', 'represent', 'nipple', '[MASK]', 'the', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'is', 'normal', '[MASK]', 'project', 'over', 'the', 'left', 'lung', 'potentially', 'within', '[MASK]', 'breast', '[MASK]', '[MASK]', '[MASK]', 'upper', 'abdomen', 'is', 'un', '##rem', '##ark', '##able', 'chronic', '[MASK]', '[MASK]', '[MASK]', 'of', 'the', 'posterior', 'left', 'sixth', 'and', 'seventh', 'ribs', 'are', 'noted', 'impression', 'no', '[MASK]', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]']'\n"
     ]
    }
   ],
   "source": [
    "example_samples = [padded_dataset[\"train\"][i] for i in range(1)]\n",
    "for sample in example_samples:\n",
    "    # THIS IS A FIX - THE PADDED DATA IS MISSING LABELS\n",
    "    sample[\"labels\"] = sample[\"input_ids\"].copy()\n",
    "example_batch = whole_word_masking_data_collator(example_samples)\n",
    "\n",
    "for chunk in example_batch[\"input_ids\"]:\n",
    "    a = tokenizer.convert_ids_to_tokens(chunk)\n",
    "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bed70",
   "metadata": {},
   "source": [
    "# Model Training (Padding)\n",
    "### 1. Prepare the datasets batches (with whole-word-masking) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4351c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate this source of randomness is to apply the masking once on the whole test set,\n",
    "# and then use the default data collator in 🤗 Transformers to collect the batches during evaluation\n",
    "\n",
    "\n",
    "# replace data_collator here with the whole-word-masking ones\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = whole_word_masking_data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1895a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 221006\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddedDataset = padded_dataset.remove_columns(['word_ids'])\n",
    "paddedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9bbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dceac1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-cd976364fa85dbba/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-567e2d642cf3dd59.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] final report ap chest [MASK] history intubated yearold woman check tube placement [MASK] ap chest compared to [MASK] [MASK] the endotracheal [MASK] at the upper [MASK] of the [MASK] [MASK] [MASK] [MASK] [MASK] less than mm [MASK] [MASK] carina care should be taken that [MASK] tube does [MASK] withdraw [MASK] further lungs [MASK] [MASK] cardiomediastinal and [MASK] [MASK] silhouettes and pleural surfaces are normal [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply this function to our test set and drop the unmasked columns so we can replace them with the masked ones\n",
    "eval_paddedDataset = padded_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=padded_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_paddedDataset = eval_paddedDataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_token_type_ids\": \"token_type_ids\", \n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")\n",
    "tokenizer.decode(eval_paddedDataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08b24649",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d4108",
   "metadata": {},
   "source": [
    "### 2. Set up dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "762abce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    padded_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=whole_word_masking_data_collator, # replace self-defined whole-word-masking-data-collator\n",
    ")\n",
    "\n",
    "# Use the default_data_collator from Transformers for the evaluation set\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_paddedDataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3b786d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, data in enumerate(train_dataloader):\n",
    "#     print(batch_idx)\n",
    "#     print(data[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61daf2b2",
   "metadata": {},
   "source": [
    "### 3. Steps for training with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8155fe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Is it correct lolllll?\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# https://huggingface.co/docs/transformers/model_doc/auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89eb16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer \n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "541e64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(cpu=True)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, \n",
    "    optimizer, \n",
    "    train_dataloader, \n",
    "    eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99ebafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler:\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 10 # change this later\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d463a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ICLbioengNLP/CXR_BioClinicalBERT_SW'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### Saving onto Huggingface hub ###########\n",
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"CXR_BioClinicalBERT_SW\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2651b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/SharedUsers/dlc19/home/codes/nlp-fineTuningBERT/CXR_BioClinicalBERT_SW is already a clone of https://huggingface.co/ICLbioengNLP/CXR_BioClinicalBERT_SW. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "output_dir = model_name\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f082a7",
   "metadata": {},
   "source": [
    "### 5. Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98fbb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea0957008f34a92a656c7781c2cb54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 0: Training Start   #############\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "perplexities = []\n",
    "\n",
    "for epoch in range(num_train_epochs): \n",
    "    # Training\n",
    "    model.train()\n",
    "    \n",
    "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(\"Finishing epoch \", epoch)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_chunkedDataset)]\n",
    "    \n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "    \n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5881c23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1576959602969485, 1.1498089768602706, 1.1442581128031095, 1.1400363561367657, 1.1363503719807606, 1.13433012291388, 1.132400135905862, 1.1313966997247378, 1.1313809281236062, 1.1313809281236062]\n",
      "tensor([0.1343, 0.1343, 0.1343,  ..., 0.1029, 0.1029, 0.1029], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(perplexities)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c7dde",
   "metadata": {},
   "source": [
    "# Comparing pre-trained and fine-tuned models\n",
    "#### Can also do BLEU score evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1044cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler_ft = pipeline(\n",
    "    \"fill-mask\", model=\"ICLbioengNLP/CXR_BioClinicalBERT_SW\")\n",
    "\n",
    "mask_filler_original = pipeline(\n",
    "    \"fill-mask\", model=\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f203755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for text1:\n",
      "1. Pre-trained Bio_ClinicalBERT\n",
      ">>> there are no signs of bleeding.\n",
      ">>> there are no signs of infection.\n",
      ">>> there are no signs of withdrawal.\n",
      ">>> there are no signs of change.\n",
      ">>> there are no signs of distress.\n",
      "\n",
      "2. Fine-tuned CXR_BioClinicalBERT_chunkedv1\n",
      ">>> there are no signs of pneumonia.\n",
      ">>> there are no signs of failure.\n",
      ">>> there are no signs of consolidation.\n",
      ">>> there are no signs of congestion.\n",
      ">>> there are no signs of infection.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"There are no signs of [MASK].\"\n",
    "\n",
    "preds1_ft = mask_filler_ft(text1)\n",
    "preds1_org = mask_filler_original(text1)\n",
    "\n",
    "print(\"Predictions for text1:\")\n",
    "print(\"1. Pre-trained Bio_ClinicalBERT\")\n",
    "for pred in preds1_org:\n",
    "    print(f\">>> {pred['sequence']}\")\n",
    "print()\n",
    "print(\"2. Fine-tuned CXR_BioClinicalBERT_SW\")\n",
    "for pred in preds1_ft:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c023c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for text2:\n",
      "1. Pre-trained Bio_ClinicalBERT\n",
      ">>> the patient suffered from pneumonia.\n",
      ">>> the patient suffered from anxiety.\n",
      ">>> the patient suffered from fatigue.\n",
      ">>> the patient suffered from fall.\n",
      ">>> the patient suffered from this.\n",
      "\n",
      "2. Fine-tuned CXR_BioClinicalBERT_chunkedv1\n",
      ">>> the patient suffered from pneumonia.\n",
      ">>> the patient suffered from trauma.\n",
      ">>> the patient suffered from bleeding.\n",
      ">>> the patient suffered from seizure.\n",
      ">>> the patient suffered from surgery.\n"
     ]
    }
   ],
   "source": [
    "text2 = \"The patient suffered from [MASK]. \"\n",
    "preds2_ft = mask_filler_ft(text2)\n",
    "preds2_org = mask_filler_original(text2)\n",
    "\n",
    "print(\"Predictions for text2:\")\n",
    "print(\"1. Pre-trained Bio_ClinicalBERT\")\n",
    "for pred in preds2_org:\n",
    "    print(f\">>> {pred['sequence']}\")\n",
    "print()\n",
    "print(\"2. Fine-tuned CXR_BioClinicalBERT_SW\")\n",
    "for pred in preds2_ft:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d8833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
