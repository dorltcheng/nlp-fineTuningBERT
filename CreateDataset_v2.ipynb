{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97cda9bc",
   "metadata": {},
   "source": [
    "# Creating `dataset.Dataset` (From huggingface) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6801497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset       # About dataset: https://huggingface.co/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00968c0e",
   "metadata": {},
   "source": [
    "#### Model inputs\n",
    "https://huggingface.co/transformers/v3.2.0/glossary.html#:~:text=Token%20Type%20IDs,-Some%20models'%20purpose&text=They%20are%20represented%20as%20a,%2C%201%2C%201%2C%201%5D\n",
    "\n",
    "1. **Input IDs** - tokens IDs\n",
    "2. **Token type IDs** - (for QA and classification only - keep it all 0) \n",
    "3. **Attention mask** - whether the token needs attention to \n",
    "    - **[PAD] with attention mask = 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b410bdf",
   "metadata": {},
   "source": [
    "### Convert data into `dataset.Dataset` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec61eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatingDatasetPd(tokenDataset):\n",
    "    \n",
    "    # 1. input_ids\n",
    "    input_ids = tokenDataset.values.tolist()\n",
    "    \n",
    "    # 2. make token_type_ids and attention_mask\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for sample in input_ids:\n",
    "        token_type_ids_sample = []\n",
    "        attention_mask_sample = []\n",
    "        \n",
    "        for token in sample:\n",
    "            token_type_ids_sample.append(0)    # we dun care, keep it a list of 0 for token_type_ids\n",
    "            \n",
    "            if token is not 0:                 # if token is not 0 (meaning it is not [PAD])\n",
    "                attention_mask_sample.append(1)\n",
    "            else:\n",
    "                attention_mask_sample.append(0)    # no attention to [PAD]\n",
    "                \n",
    "        token_type_ids.append(token_type_ids_sample)\n",
    "        attention_mask.append(attention_mask_sample)\n",
    "            \n",
    "    dictforDataset = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n",
    "    dataset = Dataset.from_dict(dictforDataset)     # Convert it from dict \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2103f358",
   "metadata": {},
   "source": [
    "#### 1. Try with `chunked_trainDataset.csv`\n",
    "- From `chunked_trainDataset.csv`: with 154696 training samples, each of them is 128 tokens long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8087eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c= pd.read_csv(\"chunked_trainDataset.csv\")   # name of the .csv file you want to convert into Dataset\n",
    "df_c = df_c.drop('Unnamed: 0', 1)\n",
    "trainDataset_c = creatingDatasetPd(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9ed2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 154696\n",
      "})\n",
      "\n",
      "Example input_ids:  [101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 175, 1207, 15415, 14375, 1116, 174, 7501, 8974, 5531, 2229, 185, 1161, 11937, 7577, 3839, 9505, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 2620, 4248, 15070, 6719, 3621, 2660, 16418, 2050, 14196, 27316, 2999, 16973, 1933, 1286, 13093, 9046, 1439, 7209, 3077, 1181, 3105, 14701, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 16530, 1286, 3971, 5001, 10346, 2382, 8351, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 1607, 175, 1603, 1757, 2184, 5531, 2229, 185, 1161, 11937, 7577, 9505, 17688, 2394, 2050, 14196, 20844, 5815, 14255, 18834, 1116, 2999, 26600, 191, 2225, 21608, 5332]\n",
      "Length:  128\n",
      "\n",
      "Example token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length:  128\n",
      "\n",
      "Example attention_mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length:  128\n"
     ]
    }
   ],
   "source": [
    "### EXAMPLE - CHECKING ITS DATATYPE AND LENGTH \n",
    "print(trainDataset_c)\n",
    "print()\n",
    "print('Example input_ids: ', trainDataset_c['input_ids'][0])\n",
    "print('Length: ', len(trainDataset_c['input_ids'][0]))\n",
    "print()\n",
    "print('Example token_type_ids: ', trainDataset_c['token_type_ids'][0])\n",
    "print('Length: ', len(trainDataset_c['token_type_ids'][0]))\n",
    "print()\n",
    "print('Example attention_mask: ', trainDataset_c['attention_mask'][0])\n",
    "print('Length: ', len(trainDataset_c['attention_mask'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4c2738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use add_column to add a test set into the same dictionary but not really necessary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab865e",
   "metadata": {},
   "source": [
    "#### 2. Try with `padded_trainDataset.csv`\n",
    "- From `padded_trainDataset.csv`: with 222062 training samples, each of them is 300 tokens long\n",
    "- No attention to [PAD] tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b756d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_csv(\"padded_trainDataset.csv\")   # name of the .csv file you want to convert into Dataset\n",
    "df_p = df_p.drop('Unnamed: 0', 1)\n",
    "trainDataset_p = creatingDatasetPd(df_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45016013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 222062\n",
      "})\n",
      "\n",
      "Example input_ids:  [101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 175, 1207, 15415, 14375, 1116, 174, 7501, 8974, 5531, 2229, 185, 1161, 11937, 7577, 3839, 9505, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 2620, 4248, 15070, 6719, 3621, 2660, 16418, 2050, 14196, 27316, 2999, 16973, 1933, 1286, 13093, 9046, 1439, 7209, 3077, 1181, 3105, 14701, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 16530, 1286, 3971, 5001, 10346, 2382, 8351, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length:  300\n",
      "\n",
      "Example token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length:  300\n",
      "\n",
      "Example attention_mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length:  300\n"
     ]
    }
   ],
   "source": [
    "### EXAMPLE - CHECKING ITS DATATYPE AND LENGTH \n",
    "print(trainDataset_p)\n",
    "print()\n",
    "print('Example input_ids: ', trainDataset_p['input_ids'][0])\n",
    "print('Length: ', len(trainDataset_p['input_ids'][0]))\n",
    "print()\n",
    "print('Example token_type_ids: ', trainDataset_p['token_type_ids'][0])\n",
    "print('Length: ', len(trainDataset_p['token_type_ids'][0]))\n",
    "print()\n",
    "print('Example attention_mask: ', trainDataset_p['attention_mask'][0])\n",
    "print('Length: ', len(trainDataset_p['attention_mask'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d451f",
   "metadata": {},
   "source": [
    "### Next steps: replace the above files with the masked ones, then can carry on with training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c2dbef",
   "metadata": {},
   "source": [
    "=================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c9864",
   "metadata": {},
   "source": [
    "## Trying the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "632aabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    trainDataset_c,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    #collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0791d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
