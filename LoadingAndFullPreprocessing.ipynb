{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f59fad",
   "metadata": {},
   "source": [
    "# Data Loading and Pre-processing (chunking, padding, masking)\n",
    "### Access all dataset here: \n",
    "https://imperiallondon-my.sharepoint.com/:f:/g/personal/dlc19_ic_ac_uk/EgobSIgJFitCuMdL0Sg6KmABP7qqtibuOz1R1jIZDEX22Q?e=U5CfiK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571348aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /media/SharedUsers/dlc19/home/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data manupulation libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "#pandarallel.initialize()\n",
    "\n",
    "#string manupulation libs\n",
    "#string manupulation libs\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "\n",
    "#torch libs\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "# data manipulations\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import pydicom\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d00afb",
   "metadata": {},
   "source": [
    "### Data Preprocessing - done separately from data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33874f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_temp = pd.read_csv('train_raw_reports.csv')\n",
    "df_test_temp = pd.read_csv('test_raw_reports.csv')\n",
    "\n",
    "#convetring study id to string as it doesn't work as an int\n",
    "df_train_temp['study_id']=df_train_temp['study_id'].astype(str)\n",
    "df_test_temp['study_id']=df_test_temp['study_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ceef69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50414267</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53189527</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53911762</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56699142</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57375967</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id                                         raw_report\n",
       "0  50414267                                   FINAL REPORT\\...\n",
       "1  53189527                                   FINAL REPORT\\...\n",
       "2  53911762                                   FINAL REPORT\\...\n",
       "3  56699142                                   FINAL REPORT\\...\n",
       "4  57375967                                   FINAL REPORT\\..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display raw reports in a dataframe\n",
    "df_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8eb0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing of the train dataset\n",
    "def preprocessing(text):\n",
    "        cleanedReport = re.sub(r'[^\\w\\s]','',text)            # remove punctuation (not word characters and whitespace)\n",
    "        cleanedReport = re.sub('_', '', cleanedReport)        # remove __ in the report\n",
    "        cleanedReport = re.sub(r'[\\d-]', '', cleanedReport)   # remove numbers in the report \n",
    "        cleanedReport = re.sub('\\n', '', cleanedReport)\n",
    "\n",
    "        return cleanedReport   \n",
    "    \n",
    "\n",
    "def preprocessDataframe(df):\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        preprocessedText = preprocessing(df.at[i, \"raw_report\"])\n",
    "    \n",
    "        df.at[i,'raw_report'] = preprocessedText\n",
    "        i = i + 1 \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f68ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the raw reports in the dataframe \n",
    "df_train_preprocessed = preprocessDataframe(df_train_temp)\n",
    "df_test_preprocessed = preprocessDataframe(df_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c92482f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50414267</td>\n",
       "      <td>FINAL REPORT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53189527</td>\n",
       "      <td>FINAL REPORT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53911762</td>\n",
       "      <td>FINAL REPORT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56699142</td>\n",
       "      <td>FINAL REPORT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57375967</td>\n",
       "      <td>FINAL REPORT ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id                                         raw_report\n",
       "0  50414267                                   FINAL REPORT ...\n",
       "1  53189527                                   FINAL REPORT ...\n",
       "2  53911762                                   FINAL REPORT ...\n",
       "3  56699142                                   FINAL REPORT ...\n",
       "4  57375967                                   FINAL REPORT ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_preprocessed.head() # check if that worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c8fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed.to_csv('train_preprocessed.csv', index=False)\n",
    "df_test_preprocessed.to_csv('test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e1819",
   "metadata": {},
   "source": [
    "### Data Loader - Chunking\n",
    "- Include: loading data, text preprocessing, words frequency check, tokenization, tokens-IDs-conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5088de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "782ada75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fc03e2e28cbbeb1b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-fc03e2e28cbbeb1b/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a808c1a6cfc470abc41f8e6206a9dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd7b1692c914c568db81191bac5a593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/csv/default-fc03e2e28cbbeb1b/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab66e7fd83794188a0ee86d23021d843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['study_id', 'raw_report'],\n",
       "        num_rows: 222337\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['study_id', 'raw_report'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"train_preprocessed.csv\", \"test\": \"test_preprocessed.csv\"}\n",
    "reports_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "reports_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140bd49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['study_id', 'raw_report'],\n",
       "    num_rows: 222337\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of how to acess one report as a dataset\n",
    "reports_dataset[\"train\"]\n",
    "#example of how to acess only written report \n",
    "#reports_dataset[\"train\"]['raw_report'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ccd179e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc13a22f1f584f848539619997b02bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bb8e509f944fb79a479da3b8357d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68716377c1647d6934bf61f7d8f4f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91928f064fd4c3a896a7f44f08905c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 206331\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 3345\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(dataset):\n",
    "    result = tokenizer(dataset[\"raw_report\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = reports_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"raw_report\", \"study_id\"]\n",
    ")\n",
    "\n",
    "\n",
    "chunk_size = 128\n",
    "\n",
    "def group_texts(dataset):\n",
    "    # Concatenate all texts\n",
    "    concatenated_text = {k: sum(dataset[k], []) for k in dataset.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_text[list(dataset.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_text.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "chunked_dataset = tokenized_datasets.map(group_texts, batched=True)\n",
    "chunked_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56ddb8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "chunkedDataset = chunked_dataset.copy()\n",
    "print(type(chunked_dataset))\n",
    "print(type(chunkedDataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d570c52f",
   "metadata": {},
   "source": [
    "### Data Loader - Padding\n",
    "- Include: loading data, text preprocessing, words frequency check, tokenization, tokens-IDs-conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "131956d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6430e0adecdb469f91dc46fac1e0ebae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca7acb56c4342499944f5ee4747321c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 222337\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function_padding(dataset):\n",
    "    result = tokenizer(dataset[\"raw_report\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = reports_dataset.map(\n",
    "    tokenize_function_padding, batched=True, remove_columns=[\"raw_report\", \"study_id\"])\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e7e48a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0e3adaee9d43e3864c7ee21e3dcaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d4355a7ea842c99de98f5c18014b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def padding(dataset):\n",
    "    \n",
    "    #for train dataset\n",
    "\n",
    "    \n",
    "    num_items = len(dataset['input_ids']) # to get number of all items in train dataset\n",
    "        \n",
    "        \n",
    "    if (len(dataset['input_ids'])) > 300:\n",
    "        \n",
    "        while(len(dataset['input_ids']) > 300):\n",
    "            dataset['input_ids'].pop()\n",
    "            dataset['token_type_ids'].pop()\n",
    "            dataset['attention_mask'].pop()\n",
    "            dataset['word_ids'].pop()\n",
    "\n",
    "    while(len(dataset['input_ids']) < 301):\n",
    "\n",
    "        dataset['input_ids'].append(0)\n",
    "        dataset['token_type_ids'].append(0)\n",
    "        dataset['attention_mask'].append(0)\n",
    "        dataset['word_ids'].append(0)\n",
    " \n",
    "    dataset['labels'] = dataset['input_ids'].copy()\n",
    "    return dataset\n",
    "\n",
    "padded_dataset = tokenized_datasets.map(padding, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45696363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_dataset[\"test\"][\"input_ids\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50624156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704109afa8b04904b7ae1881c542c1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01595e0635444e89b062d7fcd64dac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 222337\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_function(dataset):\n",
    "\n",
    "    lenReport = len(dataset['input_ids'])\n",
    "    \n",
    "    if (lenReport > 301):\n",
    "        print('yes')\n",
    "    return dataset\n",
    "\n",
    "padded_dataset.map(check_function, batched = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806279e",
   "metadata": {},
   "source": [
    "### If loading from JSON files, use this:\n",
    "- JSON aren't working rn, hence dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daedd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_files = {\n",
    "#     \"train\" : \"final_dataset_chunkedtrain.jsonl\",\n",
    "#     \"test\" : \"final_dataset_chunkedtest.jsonl\"    \n",
    "# }\n",
    "\n",
    "# chunkedDataset = load_dataset(\"json\", data_files = data_files)\n",
    "# chunkedDataset\n",
    "\n",
    "\n",
    "# #testing\n",
    "# data_files_padded = {\n",
    "#     \"train\" : \"final_dataset_padded_train.jsonl\",\n",
    "#     \"test\" : \"final_dataset_padded_test.jsonl\"    \n",
    "# }\n",
    "# paddedDataset = load_dataset(\"json\", data_files = data_files_padded)\n",
    "# chunkedDataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f687a0",
   "metadata": {},
   "source": [
    "# MASKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ba180",
   "metadata": {},
   "source": [
    "### Report example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3098bf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'list'>\n",
      "Length:  128\n",
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 175, 1114, 1207, 15415, 1112, 14375, 1116, 174, 7501, 1111, 8974, 5531, 2229, 185, 1161, 1105, 11937, 7577, 3839, 9505, 1175, 1110, 1185, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 1115, 1211, 2620, 4248, 15070, 6719, 1103, 3621, 2660, 16418, 2050, 14196, 27316, 1110, 2999, 16973, 1933, 1166, 1103, 1286, 13093, 9046, 1439, 1103, 7209, 1103, 3077, 1181, 3105, 14701, 1110, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 1104, 1103, 16530, 1286, 3971, 1105, 5001, 10346, 1132, 2382, 8351, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 1607, 175, 1114]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] final report examination chest pa and lat indication f with new onset ascites eval for infection technique chest pa and lateral comparison none findings there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted impression no acute cardiopulmonary process [SEP] [CLS] final report examination chest pa and lat indication history f with'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first chunked report\n",
    "example = chunked_dataset[\"train\"][\"input_ids\"][0]\n",
    "print(\"Type: \", type(example))\n",
    "print(\"Length: \", len(example))\n",
    "print(example)\n",
    "tokenizer.decode(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c025756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'list'>\n",
      "Length:  301\n",
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 175, 1114, 1207, 15415, 1112, 14375, 1116, 174, 7501, 1111, 8974, 5531, 2229, 185, 1161, 1105, 11937, 7577, 3839, 9505, 1175, 1110, 1185, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 1115, 1211, 2620, 4248, 15070, 6719, 1103, 3621, 2660, 16418, 2050, 14196, 27316, 1110, 2999, 16973, 1933, 1166, 1103, 1286, 13093, 9046, 1439, 1103, 7209, 1103, 3077, 1181, 3105, 14701, 1110, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 1104, 1103, 16530, 1286, 3971, 1105, 5001, 10346, 1132, 2382, 8351, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] final report examination chest pa and lat indication f with new onset ascites eval for infection technique chest pa and lateral comparison none findings there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted impression no acute cardiopulmonary process [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same report, padded\n",
    "example = padded_dataset[\"train\"][\"input_ids\"][0]\n",
    "print(\"Type: \", type(example))\n",
    "print(\"Length: \", len(example))\n",
    "print(example)\n",
    "tokenizer.decode(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2478216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole word masking: want to mask all the tokens that correspond to a single word. \n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "def whole_word_masking_data_collator(features, wwm_prob=0.15):\n",
    "    \n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\") #to fit into default_data_collator\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word: # removing repeat word_ids \n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "            #created a list where each index is a whole word, as a list of the indices of it's tokens\n",
    "            \n",
    "        mask = np.random.binomial(1, wwm_prob, (len(mapping),))#each whole word rather than each token has equal chance of selection for masking\n",
    "\n",
    "        input_ids = feature['input_ids']\n",
    "        #print(f'Length of input_ids is  {len(input_ids)}')\n",
    "        labels = feature['labels']\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id    \n",
    "                \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743e801",
   "metadata": {},
   "source": [
    "## Masking Chunked Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc57f1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "\n",
      "'>>> ['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', '[MASK]', 'la', '##t', '[MASK]', 'f', 'with', 'new', 'onset', 'as', '##cite', '##s', 'e', '##val', 'for', '[MASK]', 'technique', '[MASK]', 'p', '##a', 'and', 'lateral', 'comparison', 'none', 'findings', '[MASK]', 'is', 'no', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'or', 'p', '##ne', '##um', '##oth', '##orax', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', '[MASK]', 'most', '[MASK]', 'represent', 'nipple', 'shadows', 'the', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'is', 'normal', 'clips', 'project', 'over', 'the', 'left', 'lung', 'potentially', '[MASK]', 'the', 'breast', 'the', 'image', '##d', '[MASK]', 'abdomen', 'is', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'of', '[MASK]', 'posterior', 'left', 'sixth', 'and', '[MASK]', 'ribs', 'are', 'noted', 'impression', 'no', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]', '[CLS]', 'final', 'report', 'examination', 'chest', '[MASK]', '[MASK]', 'and', 'la', '##t', '[MASK]', 'history', '[MASK]', 'with']'\n"
     ]
    }
   ],
   "source": [
    "example_samples = [chunked_dataset[\"train\"][i] for i in range(1)]\n",
    "\n",
    "example_batch = whole_word_masking_data_collator(example_samples)\n",
    "print(type(example_batch))\n",
    "\n",
    "for chunk in example_batch[\"input_ids\"]:\n",
    "    a = tokenizer.convert_ids_to_tokens(chunk)\n",
    "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98cd78",
   "metadata": {},
   "source": [
    "## Masking Padded Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "42c47bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> ['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'and', 'la', '##t', 'indication', '[MASK]', 'with', '[MASK]', 'onset', 'as', '##cite', '##s', 'e', '##val', 'for', 'infection', 'technique', 'chest', '[MASK]', '[MASK]', '[MASK]', 'lateral', 'comparison', 'none', '[MASK]', 'there', 'is', 'no', 'focal', '[MASK]', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'or', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', 'that', 'most', '[MASK]', 'represent', 'nipple', 'shadows', '[MASK]', 'card', '##io', '##media', '##st', '##inal', '[MASK]', 'is', 'normal', 'clips', 'project', '[MASK]', '[MASK]', 'left', '[MASK]', 'potentially', 'within', 'the', 'breast', 'the', 'image', '##d', 'upper', 'abdomen', 'is', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'of', 'the', '[MASK]', 'left', '[MASK]', 'and', 'seventh', 'ribs', 'are', 'noted', 'impression', 'no', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']'\n"
     ]
    }
   ],
   "source": [
    "example_samples = [padded_dataset[\"train\"][i] for i in range(1)]\n",
    "for sample in example_samples:\n",
    "    # THIS IS A FIX - THE PADDED DATA IS MISSING LABELS\n",
    "    sample[\"labels\"] = sample[\"input_ids\"].copy()\n",
    "example_batch = whole_word_masking_data_collator(example_samples)\n",
    "\n",
    "for chunk in example_batch[\"input_ids\"]:\n",
    "    a = tokenizer.convert_ids_to_tokens(chunk)\n",
    "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eedc9fe",
   "metadata": {},
   "source": [
    "# Model Training (chunking)\n",
    "### 1. Prepare the datasets batches (with whole-word-masking) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b4e320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate this source of randomness is to apply the masking once on the whole test set,\n",
    "# and then use the default data collator in 🤗 Transformers to collect the batches during evaluation\n",
    "\n",
    "\n",
    "# replace data_collator here with the whole-word-masking ones\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = whole_word_masking_data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3eeefc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 206331\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3345\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkedDataset = chunked_dataset.remove_columns(['word_ids'])\n",
    "chunkedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eddc8817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02a2b3cfeb34f658429f6d4b980767c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] final report ap chest am history [MASK] [MASK] [MASK] [MASK] [MASK] woman check tube placement [MASK] ap chest compared [MASK] tip [MASK] [MASK] endotracheal tube at the upper margin of the clavicles is [MASK] [MASK] than [MASK] from the [MASK] [MASK] care should be taken that the tube does not withdraw any further lungs are clear cardiomediastinal and [MASK] [MASK] silhouettes and pleural [MASK] are [MASK] [SEP] [CLS] final report ap chest [MASK] [MASK] history et tube advanced impression et tube [MASK] standard placement the nasogastric tube ends in the stomach the lungs [MASK] fully expanded and clear the heart size is normal adenopathy at least'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply this function to our test set and drop the unmasked columns so we can replace them with the masked ones\n",
    "eval_chunkedDataset = chunked_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=chunked_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_chunkedDataset = eval_chunkedDataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_token_type_ids\": \"token_type_ids\", \n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")\n",
    "tokenizer.decode(eval_chunkedDataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89777f",
   "metadata": {},
   "source": [
    "### 2. Set up dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a746ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    chunked_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=whole_word_masking_data_collator, # replace self-defined whole-word-masking-data-collator\n",
    ")\n",
    "\n",
    "# Use the default_data_collator from Transformers for the evaluation set\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_chunkedDataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b384ccff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fdf2c4c8d68>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e29b9",
   "metadata": {},
   "source": [
    "### 3. Steps for training with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35617164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Is it correct lolllll?\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# https://huggingface.co/docs/transformers/model_doc/auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "30c8d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer \n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5bdf4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, \n",
    "    optimizer, \n",
    "    train_dataloader, \n",
    "    eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5392e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler:\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 10 # change this later\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "91674250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ICLbioengNLP/CXR_BioClinicalBERT_chunkedv1'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### Saving onto Huggingface hub ###########\n",
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"CXR_BioClinicalBERT_chunkedv1\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "644aecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/ICLbioengNLP/CXR_BioClinicalBERT_chunkedv1 into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "output_dir = model_name\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7e0b6",
   "metadata": {},
   "source": [
    "### 5. Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b85520b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7fdedab419e8>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ec1aa09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b631206afa4c02829b474b2b1be9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  0\n",
      ">>> Epoch 0: Perplexity: 1.1576959602969485\n",
      "Finishing epoch  1\n",
      ">>> Epoch 1: Perplexity: 1.1498089768602706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  2\n",
      ">>> Epoch 2: Perplexity: 1.1442581128031095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  3\n",
      ">>> Epoch 3: Perplexity: 1.1400363561367657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  4\n",
      ">>> Epoch 4: Perplexity: 1.1363503719807606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  5\n",
      ">>> Epoch 5: Perplexity: 1.13433012291388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  6\n",
      ">>> Epoch 6: Perplexity: 1.132400135905862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (4) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  7\n",
      ">>> Epoch 7: Perplexity: 1.1313966997247378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  8\n",
      ">>> Epoch 8: Perplexity: 1.1313809281236062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (4) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  9\n",
      ">>> Epoch 9: Perplexity: 1.1313809281236062\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "perplexities = []\n",
    "\n",
    "for epoch in range(num_train_epochs): # for now try 2 epochs and see what happen\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(\"Finishing epoch \", epoch)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_chunkedDataset)]\n",
    "    \n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "    \n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6a404814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1576959602969485, 1.1498089768602706, 1.1442581128031095, 1.1400363561367657, 1.1363503719807606, 1.13433012291388, 1.132400135905862, 1.1313966997247378, 1.1313809281236062, 1.1313809281236062]\n",
      "tensor([0.1343, 0.1343, 0.1343,  ..., 0.1029, 0.1029, 0.1029], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(perplexities)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2ac36",
   "metadata": {},
   "source": [
    "# Comparing pre-trained and fine-tuned models\n",
    "#### Can also do BLEU score evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9363506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87e86a6ab2942d58e6b3d377a4a18df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/643 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e699274a8d47c88e38ae3219f35830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6456decf06045f9b3a9d2e1dc9243cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/358 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8c7cb143d44f35862f2c380cc00979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584905c695624f0b9c32ff42d4c96c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/653k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f505ecdb287429dad26d444f18e747e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler_ft = pipeline(\n",
    "    \"fill-mask\", model=\"ICLbioengNLP/CXR_BioClinicalBERT_chunkedv1\")\n",
    "\n",
    "mask_filler_original = pipeline(\n",
    "    \"fill-mask\", model=\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "93d99e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for text1:\n",
      "1. Pre-trained Bio_ClinicalBERT\n",
      ">>> there are no signs of bleeding.\n",
      ">>> there are no signs of infection.\n",
      ">>> there are no signs of withdrawal.\n",
      ">>> there are no signs of change.\n",
      ">>> there are no signs of distress.\n",
      "\n",
      "2. Fine-tuned CXR_Bio_ClinicalBERT_v1\n",
      ">>> there are no signs of pneumonia.\n",
      ">>> there are no signs of failure.\n",
      ">>> there are no signs of consolidation.\n",
      ">>> there are no signs of complications.\n",
      ">>> there are no signs of congestion.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"There are no signs of [MASK].\"\n",
    "\n",
    "preds1_ft = mask_filler_ft(text1)\n",
    "preds1_org = mask_filler_original(text1)\n",
    "\n",
    "print(\"Predictions for text1:\")\n",
    "print(\"1. Pre-trained Bio_ClinicalBERT\")\n",
    "for pred in preds1_org:\n",
    "    print(f\">>> {pred['sequence']}\")\n",
    "print()\n",
    "print(\"2. Fine-tuned CXR_Bio_ClinicalBERT_v1\")\n",
    "for pred in preds1_ft:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c45c65a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for text2:\n",
      "1. Pre-trained Bio_ClinicalBERT\n",
      ">>> the patient suffered from pneumonia.\n",
      ">>> the patient suffered from anxiety.\n",
      ">>> the patient suffered from fatigue.\n",
      ">>> the patient suffered from fall.\n",
      ">>> the patient suffered from this.\n",
      "\n",
      "2. Fine-tuned CXR_Bio_ClinicalBERT_v1\n",
      ">>> the patient suffered from trauma.\n",
      ">>> the patient suffered from pneumonia.\n",
      ">>> the patient suffered from bleeding.\n",
      ">>> the patient suffered from seizure.\n",
      ">>> the patient suffered from surgery.\n"
     ]
    }
   ],
   "source": [
    "text2 = \"The patient suffered from [MASK]. \"\n",
    "preds2_ft = mask_filler_ft(text2)\n",
    "preds2_org = mask_filler_original(text2)\n",
    "\n",
    "print(\"Predictions for text2:\")\n",
    "print(\"1. Pre-trained Bio_ClinicalBERT\")\n",
    "for pred in preds2_org:\n",
    "    print(f\">>> {pred['sequence']}\")\n",
    "print()\n",
    "print(\"2. Fine-tuned CXR_Bio_ClinicalBERT_v1\")\n",
    "for pred in preds2_ft:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f613db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
