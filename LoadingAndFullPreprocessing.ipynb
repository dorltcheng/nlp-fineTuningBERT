{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb265747",
   "metadata": {},
   "source": [
    "# Data Loading and Pre-processing (chunking, padding, masking)\n",
    "### Access all dataset here: \n",
    "https://imperiallondon-my.sharepoint.com/:f:/g/personal/dlc19_ic_ac_uk/EgobSIgJFitCuMdL0Sg6KmABP7qqtibuOz1R1jIZDEX22Q?e=U5CfiK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a9ae4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /media/SharedUsers/elh19/home/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data manupulation libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "#pandarallel.initialize()\n",
    "\n",
    "#string manupulation libs\n",
    "#string manupulation libs\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "\n",
    "#torch libs\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "# data manipulations\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import pydicom\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a080a6",
   "metadata": {},
   "source": [
    "### Data Preprocessing - done separately from data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c19c0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_temp = pd.read_csv('train_raw_reports.csv')\n",
    "df_test_temp = pd.read_csv('test_raw_reports.csv')\n",
    "\n",
    "#convetring study id to string as it doesn't work as an int\n",
    "df_train_temp['study_id']=df_train_temp['study_id'].astype(str)\n",
    "df_test_temp['study_id']=df_test_temp['study_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "07df4a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50414267</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53189527</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53911762</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56699142</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57375967</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id                                         raw_report\n",
       "0  50414267                                   FINAL REPORT\\...\n",
       "1  53189527                                   FINAL REPORT\\...\n",
       "2  53911762                                   FINAL REPORT\\...\n",
       "3  56699142                                   FINAL REPORT\\...\n",
       "4  57375967                                   FINAL REPORT\\..."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display raw reports in a dataframe\n",
    "df_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8dc90b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing of the train dataset\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocessing(text):\n",
    "        cleanedReport = re.sub(r'[^\\w\\s]','',text)            # remove punctuation (not word characters and whitespace)\n",
    "        cleanedReport = re.sub('_', '', cleanedReport)        # remove __ in the report\n",
    "        cleanedReport = re.sub(r'[\\d-]', '', cleanedReport)   # remove numbers in the report \n",
    "        cleanedReport = re.sub('\\n', '', cleanedReport)\n",
    "#         cleanedReport = [word for word in cleanedReport if word not in stop_words]\n",
    "        return cleanedReport   \n",
    "    \n",
    "\n",
    "def preprocessDataframe(df):\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        preprocessedText = preprocessing(df.at[i, \"raw_report\"])\n",
    "    \n",
    "        df.at[i,'raw_report'] = preprocessedText\n",
    "        i = i + 1 \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "27e8df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the raw reports in the dataframe \n",
    "df_train_preprocessed = preprocessDataframe(df_train_temp)\n",
    "df_test_preprocessed = preprocessDataframe(df_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6e6eff98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50414267</td>\n",
       "      <td>FINAL REPORT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53189527</td>\n",
       "      <td>FINAL REPORT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53911762</td>\n",
       "      <td>FINAL REPORT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56699142</td>\n",
       "      <td>FINAL REPORT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57375967</td>\n",
       "      <td>FINAL REPORT ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id                                         raw_report\n",
       "0  50414267                                   FINAL REPORT ...\n",
       "1  53189527                                   FINAL REPORT ...\n",
       "2  53911762                                   FINAL REPORT ...\n",
       "3  56699142                                   FINAL REPORT ...\n",
       "4  57375967                                   FINAL REPORT ..."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_preprocessed.head() # check if that worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da1b665",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "Don't actually know whether we should be doing this. If we do, need to make our own list (big problem with getting rid of negations and changing meaning).\n",
    "If you want to use, comment out and use \"df_trained_preprocessed_nostopwords\" to make csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7f88c6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 FINAL REPORT EXAMINATION  CHEST PA AND LAT  INDICATION  F with new onset ascites   eval for infection  TECHNIQUE  Chest PA and lateral  COMPARISON  None  FINDINGS   There is no focal consolidation pleural effusion or pneumothorax  Bilateral nodular opacities that most likely represent nipple shadows The cardiomediastinal silhouette is normal  Clips project over the left lung potentially within the breast The imaged upper abdomen is unremarkable Chronic deformity of the posterior left sixth and seventh ribs are noted  IMPRESSION   No acute cardiopulmonary process\n",
      "\n",
      "FINAL REPORT EXAMINATION CHEST PA LAT INDICATION F new onset ascites eval infection TECHNIQUE Chest PA lateral COMPARISON None FINDINGS focal consolidation pleural effusion pneumothorax Bilateral nodular opacities likely represent nipple shadows cardiomediastinal silhouette normal Clips project left lung potentially within breast imaged upper abdomen unremarkable Chronic deformity posterior left sixth seventh ribs noted IMPRESSION acute cardiopulmonary process\n"
     ]
    }
   ],
   "source": [
    "# Example. Notice the reports have different meanings because of removal of words like 'no'\n",
    "print(df_train_preprocessed['raw_report'][0])\n",
    "words = [word for word in df_train_preprocessed['raw_report'][0].split() if word.lower() not in stop_words ]\n",
    "new_text = \" \".join(words)\n",
    "\n",
    "print()\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e30eb",
   "metadata": {},
   "source": [
    "#### Uncomment to remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b5db7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n"
     ]
    }
   ],
   "source": [
    "# df_train_preprocessed_nostopwords = df_train_preprocessed.copy()\n",
    "# l = len(df_train_preprocessed['raw_report'])\n",
    "# for i in range(0,l):\n",
    "#     if i%10000 == 0:\n",
    "#         print(i) # just to check progress - should take 10-15mins, 220,000 reports\n",
    "#     words = [word for word in df_train_preprocessed['raw_report'][i].split() if word.lower() not in stop_words]\n",
    "#     new_report = \" \".join(words)\n",
    "#     df_train_preprocessed_nostopwords['raw_report'][i] = new_report\n",
    "\n",
    "# df_test_preprocessed_nostopwords = df_test_preprocessed.copy()\n",
    "# l = len(df_test_preprocessed['raw_report'])\n",
    "# for i in range(0,l):\n",
    "#     words = [word for word in df_test_preprocessed['raw_report'][i].split() if word.lower() not in stop_words]\n",
    "#     new_report = \" \".join(words)\n",
    "#     df_test_preprocessed_nostopwords['raw_report'][i] = new_report\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502691da",
   "metadata": {},
   "source": [
    "#### Checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "82392b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 FINAL REPORT EXAMINATION  CHEST PORTABLE AP  INDICATION   year old woman with ptx and bp fistula  interval change ptx on suction  COMPARISON  Portable chest radiograph from earlier on the same day dated  at  hour  FINDINGS   Significant interval reduction in the size of the right pneumothorax after action Interval reexpansion of the right upper and middle lung now with a smalltomoderate sized apical right pneumothorax extending to about the  posterior rib No evidence of tension Associated slight reexpansion of the right lower lung although substantial atelectasis remains No pleural effusion Stable moderate subcutaneous emphysema in the right chest wall and supraclavicular region The right chest tube appears unchanged in position The left lung is clear Stable cardiomediastinal silhouette and hila  IMPRESSION    Smalltomoderate right apical pneumothorax markedly improved after suction with reexpansion of the upper lung but prominent remaining residual right lower lung atelectasis   Stable moderate subcutaneous emphysema in the right chest wall  NOTIFICATION   The findings were discussed by Dr  with Dr  the referring provider on the telephone on  at  PM  minutes after discovery of the findings\n",
      "\n",
      "FINAL REPORT EXAMINATION CHEST PORTABLE AP INDICATION year old woman ptx bp fistula interval change ptx suction COMPARISON Portable chest radiograph earlier day dated hour FINDINGS Significant interval reduction size right pneumothorax action Interval reexpansion right upper middle lung smalltomoderate sized apical right pneumothorax extending posterior rib evidence tension Associated slight reexpansion right lower lung although substantial atelectasis remains pleural effusion Stable moderate subcutaneous emphysema right chest wall supraclavicular region right chest tube appears unchanged position left lung clear Stable cardiomediastinal silhouette hila IMPRESSION Smalltomoderate right apical pneumothorax markedly improved suction reexpansion upper lung prominent remaining residual right lower lung atelectasis Stable moderate subcutaneous emphysema right chest wall NOTIFICATION findings discussed Dr Dr referring provider telephone PM minutes discovery findings\n",
      "\n",
      "                                 FINAL REPORT CHEST RADIOGRAPH  INDICATION  Chronic heart failure status post intubation  COMPARISON    FINDINGS  As compared to the previous radiograph the patient has received an endotracheal tube  The tube projects  cm above the carina  The lung volumes are low  The right internal jugular vein catheter is unchanged  The preexisting parenchymal opacities at the lung bases are minimally improved  No new opacities  No evidence of complications notably no pneumothorax\n",
      "\n",
      "FINAL REPORT CHEST RADIOGRAPH INDICATION Chronic heart failure status post intubation COMPARISON FINDINGS compared previous radiograph patient received endotracheal tube tube projects cm carina lung volumes low right internal jugular vein catheter unchanged preexisting parenchymal opacities lung bases minimally improved new opacities evidence complications notably pneumothorax\n"
     ]
    }
   ],
   "source": [
    "# print(df_train_preprocessed['raw_report'][10030])\n",
    "# print()\n",
    "# print(df_train_preprocessed_nostopwords['raw_report'][10030])\n",
    "# print()\n",
    "# print(df_test_preprocessed['raw_report'][2000])\n",
    "# print()\n",
    "# print(df_test_preprocessed_nostopwords['raw_report'][2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36cc7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if removing stopwords, change these to _nostopwords versions.\n",
    "df_train_preprocessed.to_csv('train_preprocessed.csv', index=False)\n",
    "df_test_preprocessed.to_csv('test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0d726",
   "metadata": {},
   "source": [
    "### Data Loader - Chunking\n",
    "- Include: loading data, text preprocessing, words frequency check, tokenization, tokens-IDs-conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82c81d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dc6b507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-26f909f52d14cf3e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /media/SharedUsers/elh19/home/.cache/huggingface/datasets/csv/default-26f909f52d14cf3e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a1a476310a47c68123895ad9d1411d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad39ee903ee4b8d92bbab32254f5090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /media/SharedUsers/elh19/home/.cache/huggingface/datasets/csv/default-26f909f52d14cf3e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325fe2e770ff417888e8487e0668f27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['study_id', 'raw_report'],\n",
       "        num_rows: 222337\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['study_id', 'raw_report'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"train_preprocessed.csv\", \"test\": \"test_preprocessed.csv\"}\n",
    "reports_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "reports_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06113668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['study_id', 'raw_report'],\n",
       "    num_rows: 222337\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of how to acess one report as a dataset\n",
    "reports_dataset[\"train\"]\n",
    "#example of how to acess only written report \n",
    "#reports_dataset[\"train\"]['raw_report'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ab21465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb6e439717044c8b46b067a2e456d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966108518b5c491ab07e02788e3058d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6762fe5a6b349428cecc29a7030b3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f160d3366e434a2ead993cc82457d71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 206331\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 3345\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(dataset):\n",
    "    result = tokenizer(dataset[\"raw_report\"])\n",
    "#     result = [word for word in result if word not in stopwords.words('english')]\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = reports_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"raw_report\", \"study_id\"]\n",
    ")\n",
    "\n",
    "\n",
    "chunk_size = 128\n",
    "\n",
    "def group_texts(dataset):\n",
    "    # Concatenate all texts\n",
    "    concatenated_text = {k: sum(dataset[k], []) for k in dataset.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_text[list(dataset.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_text.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "chunked_dataset = tokenized_datasets.map(group_texts, batched=True)\n",
    "chunked_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e679cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "chunkedDataset = chunked_dataset.copy()\n",
    "print(type(chunked_dataset))\n",
    "print(type(chunkedDataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae147733",
   "metadata": {},
   "source": [
    "### Data Loader - Padding\n",
    "- Include: loading data, text preprocessing, words frequency check, tokenization, tokens-IDs-conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5013b069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6430e0adecdb469f91dc46fac1e0ebae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca7acb56c4342499944f5ee4747321c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 222337\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function_padding(dataset):\n",
    "    result = tokenizer(dataset[\"raw_report\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = reports_dataset.map(\n",
    "    tokenize_function_padding, batched=True, remove_columns=[\"raw_report\", \"study_id\"])\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eec591f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0e3adaee9d43e3864c7ee21e3dcaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d4355a7ea842c99de98f5c18014b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def padding(dataset):\n",
    "    \n",
    "    #for train dataset\n",
    "\n",
    "    \n",
    "    num_items = len(dataset['input_ids']) # to get number of all items in train dataset\n",
    "        \n",
    "        \n",
    "    if (len(dataset['input_ids'])) > 300:\n",
    "        \n",
    "        while(len(dataset['input_ids']) > 300):\n",
    "            dataset['input_ids'].pop()\n",
    "            dataset['token_type_ids'].pop()\n",
    "            dataset['attention_mask'].pop()\n",
    "            dataset['word_ids'].pop()\n",
    "\n",
    "    while(len(dataset['input_ids']) < 301):\n",
    "\n",
    "        dataset['input_ids'].append(0)\n",
    "        dataset['token_type_ids'].append(0)\n",
    "        dataset['attention_mask'].append(0)\n",
    "        dataset['word_ids'].append(0)\n",
    " \n",
    "    dataset['labels'] = dataset['input_ids'].copy()\n",
    "    return dataset\n",
    "\n",
    "padded_dataset = tokenized_datasets.map(padding, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34eaf459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_dataset[\"test\"][\"input_ids\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ec668f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704109afa8b04904b7ae1881c542c1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01595e0635444e89b062d7fcd64dac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 222337\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_function(dataset):\n",
    "\n",
    "    lenReport = len(dataset['input_ids'])\n",
    "    \n",
    "    if (lenReport > 301):\n",
    "        print('yes')\n",
    "    return dataset\n",
    "\n",
    "padded_dataset.map(check_function, batched = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c8cfe",
   "metadata": {},
   "source": [
    "### If loading from JSON files, use this:\n",
    "- JSON aren't working rn, hence dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "763419a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_files = {\n",
    "#     \"train\" : \"final_dataset_chunkedtrain.jsonl\",\n",
    "#     \"test\" : \"final_dataset_chunkedtest.jsonl\"    \n",
    "# }\n",
    "\n",
    "# chunkedDataset = load_dataset(\"json\", data_files = data_files)\n",
    "# chunkedDataset\n",
    "\n",
    "\n",
    "# #testing\n",
    "# data_files_padded = {\n",
    "#     \"train\" : \"final_dataset_padded_train.jsonl\",\n",
    "#     \"test\" : \"final_dataset_padded_test.jsonl\"    \n",
    "# }\n",
    "# paddedDataset = load_dataset(\"json\", data_files = data_files_padded)\n",
    "# chunkedDataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f0f78",
   "metadata": {},
   "source": [
    "# MASKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b1f2e",
   "metadata": {},
   "source": [
    "### Report example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9f9537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'list'>\n",
      "Length:  128\n",
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 175, 1114, 1207, 15415, 1112, 14375, 1116, 174, 7501, 1111, 8974, 5531, 2229, 185, 1161, 1105, 11937, 7577, 3839, 9505, 1175, 1110, 1185, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 1115, 1211, 2620, 4248, 15070, 6719, 1103, 3621, 2660, 16418, 2050, 14196, 27316, 1110, 2999, 16973, 1933, 1166, 1103, 1286, 13093, 9046, 1439, 1103, 7209, 1103, 3077, 1181, 3105, 14701, 1110, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 1104, 1103, 16530, 1286, 3971, 1105, 5001, 10346, 1132, 2382, 8351, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 1607, 175, 1114]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] final report examination chest pa and lat indication f with new onset ascites eval for infection technique chest pa and lateral comparison none findings there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted impression no acute cardiopulmonary process [SEP] [CLS] final report examination chest pa and lat indication history f with'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first chunked report\n",
    "example = chunked_dataset[\"train\"][\"input_ids\"][0]\n",
    "print(\"Type: \", type(example))\n",
    "print(\"Length: \", len(example))\n",
    "print(example)\n",
    "tokenizer.decode(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdab9dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'list'>\n",
      "Length:  301\n",
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 175, 1114, 1207, 15415, 1112, 14375, 1116, 174, 7501, 1111, 8974, 5531, 2229, 185, 1161, 1105, 11937, 7577, 3839, 9505, 1175, 1110, 1185, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 1115, 1211, 2620, 4248, 15070, 6719, 1103, 3621, 2660, 16418, 2050, 14196, 27316, 1110, 2999, 16973, 1933, 1166, 1103, 1286, 13093, 9046, 1439, 1103, 7209, 1103, 3077, 1181, 3105, 14701, 1110, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 1104, 1103, 16530, 1286, 3971, 1105, 5001, 10346, 1132, 2382, 8351, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] final report examination chest pa and lat indication f with new onset ascites eval for infection technique chest pa and lateral comparison none findings there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted impression no acute cardiopulmonary process [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same report, padded\n",
    "example = padded_dataset[\"train\"][\"input_ids\"][0]\n",
    "print(\"Type: \", type(example))\n",
    "print(\"Length: \", len(example))\n",
    "print(example)\n",
    "tokenizer.decode(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7225e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole word masking: want to mask all the tokens that correspond to a single word. \n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "def whole_word_masking_data_collator(features, wwm_prob=0.15):\n",
    "    \n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\") #to fit into default_data_collator\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word: # removing repeat word_ids \n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "            #created a list where each index is a whole word, as a list of the indices of it's tokens\n",
    "            \n",
    "        mask = np.random.binomial(1, wwm_prob, (len(mapping),))#each whole word rather than each token has equal chance of selection for masking\n",
    "\n",
    "        input_ids = feature['input_ids']\n",
    "        #print(f'Length of input_ids is  {len(input_ids)}')\n",
    "        labels = feature['labels']\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id    \n",
    "                \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e88bc2c",
   "metadata": {},
   "source": [
    "## Masking Chunked Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82bdf46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "\n",
      "'>>> ['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', '[MASK]', 'la', '##t', '[MASK]', 'f', 'with', 'new', 'onset', 'as', '##cite', '##s', 'e', '##val', 'for', '[MASK]', 'technique', '[MASK]', 'p', '##a', 'and', 'lateral', 'comparison', 'none', 'findings', '[MASK]', 'is', 'no', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'or', 'p', '##ne', '##um', '##oth', '##orax', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', '[MASK]', 'most', '[MASK]', 'represent', 'nipple', 'shadows', 'the', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'is', 'normal', 'clips', 'project', 'over', 'the', 'left', 'lung', 'potentially', '[MASK]', 'the', 'breast', 'the', 'image', '##d', '[MASK]', 'abdomen', 'is', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'of', '[MASK]', 'posterior', 'left', 'sixth', 'and', '[MASK]', 'ribs', 'are', 'noted', 'impression', 'no', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]', '[CLS]', 'final', 'report', 'examination', 'chest', '[MASK]', '[MASK]', 'and', 'la', '##t', '[MASK]', 'history', '[MASK]', 'with']'\n"
     ]
    }
   ],
   "source": [
    "example_samples = [chunked_dataset[\"train\"][i] for i in range(1)]\n",
    "\n",
    "example_batch = whole_word_masking_data_collator(example_samples)\n",
    "print(type(example_batch))\n",
    "\n",
    "for chunk in example_batch[\"input_ids\"]:\n",
    "    a = tokenizer.convert_ids_to_tokens(chunk)\n",
    "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5499c4",
   "metadata": {},
   "source": [
    "## Masking Padded Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ab48f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> ['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'and', 'la', '##t', 'indication', '[MASK]', 'with', '[MASK]', 'onset', 'as', '##cite', '##s', 'e', '##val', 'for', 'infection', 'technique', 'chest', '[MASK]', '[MASK]', '[MASK]', 'lateral', 'comparison', 'none', '[MASK]', 'there', 'is', 'no', 'focal', '[MASK]', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'or', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', 'that', 'most', '[MASK]', 'represent', 'nipple', 'shadows', '[MASK]', 'card', '##io', '##media', '##st', '##inal', '[MASK]', 'is', 'normal', 'clips', 'project', '[MASK]', '[MASK]', 'left', '[MASK]', 'potentially', 'within', 'the', 'breast', 'the', 'image', '##d', 'upper', 'abdomen', 'is', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'of', 'the', '[MASK]', 'left', '[MASK]', 'and', 'seventh', 'ribs', 'are', 'noted', 'impression', 'no', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']'\n"
     ]
    }
   ],
   "source": [
    "example_samples = [padded_dataset[\"train\"][i] for i in range(1)]\n",
    "for sample in example_samples:\n",
    "    # THIS IS A FIX - THE PADDED DATA IS MISSING LABELS\n",
    "    sample[\"labels\"] = sample[\"input_ids\"].copy()\n",
    "example_batch = whole_word_masking_data_collator(example_samples)\n",
    "\n",
    "for chunk in example_batch[\"input_ids\"]:\n",
    "    a = tokenizer.convert_ids_to_tokens(chunk)\n",
    "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cb19d",
   "metadata": {},
   "source": [
    "# Model Training (chunking)\n",
    "### 1. Prepare the datasets batches (with whole-word-masking) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6dc58b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate this source of randomness is to apply the masking once on the whole test set,\n",
    "# and then use the default data collator in ðŸ¤— Transformers to collect the batches during evaluation\n",
    "\n",
    "\n",
    "# replace data_collator here with the whole-word-masking ones\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = whole_word_masking_data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "317ec625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 206331\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3345\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkedDataset = chunked_dataset.remove_columns(['word_ids'])\n",
    "chunkedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9c20b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02a2b3cfeb34f658429f6d4b980767c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] final report ap chest am history [MASK] [MASK] [MASK] [MASK] [MASK] woman check tube placement [MASK] ap chest compared [MASK] tip [MASK] [MASK] endotracheal tube at the upper margin of the clavicles is [MASK] [MASK] than [MASK] from the [MASK] [MASK] care should be taken that the tube does not withdraw any further lungs are clear cardiomediastinal and [MASK] [MASK] silhouettes and pleural [MASK] are [MASK] [SEP] [CLS] final report ap chest [MASK] [MASK] history et tube advanced impression et tube [MASK] standard placement the nasogastric tube ends in the stomach the lungs [MASK] fully expanded and clear the heart size is normal adenopathy at least'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply this function to our test set and drop the unmasked columns so we can replace them with the masked ones\n",
    "eval_chunkedDataset = chunked_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=chunked_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_chunkedDataset = eval_chunkedDataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_token_type_ids\": \"token_type_ids\", \n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")\n",
    "tokenizer.decode(eval_chunkedDataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d487f62",
   "metadata": {},
   "source": [
    "### 2. Set up dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d544bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    chunked_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=whole_word_masking_data_collator, # replace self-defined whole-word-masking-data-collator\n",
    ")\n",
    "\n",
    "# Use the default_data_collator from Transformers for the evaluation set\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_chunkedDataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e03c5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fdf2c4c8d68>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c6a18",
   "metadata": {},
   "source": [
    "### 3. Steps for training with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a48f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Is it correct lolllll?\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# https://huggingface.co/docs/transformers/model_doc/auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cfc3d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer \n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7358344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, \n",
    "    optimizer, \n",
    "    train_dataloader, \n",
    "    eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0d47234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler:\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 10 # change this later\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ba54047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ICLbioengNLP/CXR_BioClinicalBERT_chunkedv1'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### Saving onto Huggingface hub ###########\n",
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"CXR_BioClinicalBERT_chunkedv1\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7326fe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/ICLbioengNLP/CXR_BioClinicalBERT_chunkedv1 into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "output_dir = model_name\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f958398",
   "metadata": {},
   "source": [
    "### 5. Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0d04449b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7fdedab419e8>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87a15232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b631206afa4c02829b474b2b1be9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  0\n",
      ">>> Epoch 0: Perplexity: 1.1576959602969485\n",
      "Finishing epoch  1\n",
      ">>> Epoch 1: Perplexity: 1.1498089768602706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  2\n",
      ">>> Epoch 2: Perplexity: 1.1442581128031095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  3\n",
      ">>> Epoch 3: Perplexity: 1.1400363561367657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  4\n",
      ">>> Epoch 4: Perplexity: 1.1363503719807606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  5\n",
      ">>> Epoch 5: Perplexity: 1.13433012291388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  6\n",
      ">>> Epoch 6: Perplexity: 1.132400135905862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (4) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  7\n",
      ">>> Epoch 7: Perplexity: 1.1313966997247378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  8\n",
      ">>> Epoch 8: Perplexity: 1.1313809281236062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (4) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing epoch  9\n",
      ">>> Epoch 9: Perplexity: 1.1313809281236062\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "perplexities = []\n",
    "\n",
    "for epoch in range(num_train_epochs): # for now try 2 epochs and see what happen\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(\"Finishing epoch \", epoch)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_chunkedDataset)]\n",
    "    \n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "    \n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "606ac8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1576959602969485, 1.1498089768602706, 1.1442581128031095, 1.1400363561367657, 1.1363503719807606, 1.13433012291388, 1.132400135905862, 1.1313966997247378, 1.1313809281236062, 1.1313809281236062]\n",
      "tensor([0.1343, 0.1343, 0.1343,  ..., 0.1029, 0.1029, 0.1029], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(perplexities)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6831c8",
   "metadata": {},
   "source": [
    "# Comparing pre-trained and fine-tuned models\n",
    "#### Can also do BLEU score evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c3919dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler_ft = pipeline(\n",
    "    \"fill-mask\", model=\"ICLbioengNLP/CXR_BioClinicalBERT_chunkedv1\")\n",
    "\n",
    "mask_filler_original = pipeline(\n",
    "    \"fill-mask\", model=\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b545bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for text1:\n",
      "1. Pre-trained Bio_ClinicalBERT\n",
      ">>> there are no signs of bleeding.\n",
      ">>> there are no signs of infection.\n",
      ">>> there are no signs of withdrawal.\n",
      ">>> there are no signs of change.\n",
      ">>> there are no signs of distress.\n",
      "\n",
      "2. Fine-tuned CXR_BioClinicalBERT_chunkedv1\n",
      ">>> there are no signs of pneumonia.\n",
      ">>> there are no signs of failure.\n",
      ">>> there are no signs of consolidation.\n",
      ">>> there are no signs of congestion.\n",
      ">>> there are no signs of infection.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"There are no signs of [MASK].\"\n",
    "\n",
    "preds1_ft = mask_filler_ft(text1)\n",
    "preds1_org = mask_filler_original(text1)\n",
    "\n",
    "print(\"Predictions for text1:\")\n",
    "print(\"1. Pre-trained Bio_ClinicalBERT\")\n",
    "for pred in preds1_org:\n",
    "    print(f\">>> {pred['sequence']}\")\n",
    "print()\n",
    "print(\"2. Fine-tuned CXR_BioClinicalBERT_chunkedv1\")\n",
    "for pred in preds1_ft:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf75b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for text2:\n",
      "1. Pre-trained Bio_ClinicalBERT\n",
      ">>> the patient suffered from pneumonia.\n",
      ">>> the patient suffered from anxiety.\n",
      ">>> the patient suffered from fatigue.\n",
      ">>> the patient suffered from fall.\n",
      ">>> the patient suffered from this.\n",
      "\n",
      "2. Fine-tuned CXR_BioClinicalBERT_chunkedv1\n",
      ">>> the patient suffered from pneumonia.\n",
      ">>> the patient suffered from trauma.\n",
      ">>> the patient suffered from bleeding.\n",
      ">>> the patient suffered from seizure.\n",
      ">>> the patient suffered from surgery.\n"
     ]
    }
   ],
   "source": [
    "text2 = \"The patient suffered from [MASK]. \"\n",
    "preds2_ft = mask_filler_ft(text2)\n",
    "preds2_org = mask_filler_original(text2)\n",
    "\n",
    "print(\"Predictions for text2:\")\n",
    "print(\"1. Pre-trained Bio_ClinicalBERT\")\n",
    "for pred in preds2_org:\n",
    "    print(f\">>> {pred['sequence']}\")\n",
    "print()\n",
    "print(\"2. Fine-tuned CXR_BioClinicalBERT_chunkedv1\")\n",
    "for pred in preds2_ft:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb4a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
