{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "057ac30b",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d496846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# All imports \n",
    "##################\n",
    "\n",
    "#sys libs\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data manupulation libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "#pandarallel.initialize()\n",
    "\n",
    "#string manupulation libs\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import spacy\n",
    "\n",
    "#torch libs\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "# data manipulations\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import pydicom\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c98dd9",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5a7976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>50414267</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>53189527</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>53911762</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>56699142</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s56699142/ea030e7a-2e3b134...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>57375967</td>\n",
       "      <td>10000764</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000764/s57375967/096052b7-d256dc4...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  study_id  subject_id  split  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n",
       "1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n",
       "2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n",
       "3  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  56699142    10000032  train   \n",
       "4  096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4  57375967    10000764  train   \n",
       "\n",
       "                                                path  \\\n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...   \n",
       "1  files/p10/p10000032/s53189527/2a2277a9-b0ded15...   \n",
       "2  files/p10/p10000032/s53911762/68b5c4b1-227d048...   \n",
       "3  files/p10/p10000032/s56699142/ea030e7a-2e3b134...   \n",
       "4  files/p10/p10000764/s57375967/096052b7-d256dc4...   \n",
       "\n",
       "                                          raw_report  \n",
       "0                                   FINAL REPORT\\...  \n",
       "1                                   FINAL REPORT\\...  \n",
       "2                                   FINAL REPORT\\...  \n",
       "3                                   FINAL REPORT\\...  \n",
       "4                                   FINAL REPORT\\...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used to make the csv with raw reports, not really important anymore once we have the reports in a csv\n",
    "cxr_root_path = \"/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/\"\n",
    "df_record = pd.read_csv('{}/cxr-record-list.csv.gz'.format(cxr_root_path), sep=',')\n",
    "df_split = pd.read_csv('{}/mimic-cxr-2.0.0-split.csv.gz'.format(cxr_root_path))\n",
    "\n",
    "df_temp = df_split.merge(df_record, on=['subject_id', 'study_id', 'dicom_id'], how='left')\n",
    "\n",
    "df_sections = pd.read_csv('{}/mimic-cxr-sections/mimic_cxr_sectioned.csv'.format(cxr_root_path))\n",
    "\n",
    "# if you already have the df_raw_reports.csv just uncomment the following line and skipp to the vocabulary class\n",
    "\n",
    "# extract the csv into a dataframe\n",
    "df_raw_reports = pd.read_csv('df_raw_reports.csv')\n",
    "# discard duplicates of the reports as there were originally more rows than reports (as there is sometimes more images per study)\n",
    "df_raw_reports= df_raw_reports.drop_duplicates(subset = [\"raw_report\"])\n",
    "#restart the index column as it was filtered from larger data and the indices were messed up\n",
    "df_raw_reports.reset_index(inplace=True)\n",
    "#discard some unimportant columns\n",
    "df_raw_reports = df_raw_reports.drop('index', 1)\n",
    "df_raw_reports = df_raw_reports.drop('Unnamed: 0', 1)\n",
    "# print the head to check if looks like intended\n",
    "df_raw_reports.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5deae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377110, 4)\n",
      "5159\n"
     ]
    }
   ],
   "source": [
    "# Checking test dataset number\n",
    "df_split.head()\n",
    "print(df_split.shape)\n",
    "\n",
    "split = df_split[\"split\"].tolist()\n",
    "idx = 0\n",
    "for elem in split:\n",
    "    if elem == \"test\":\n",
    "        idx+=1\n",
    "\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32223c1",
   "metadata": {},
   "source": [
    "### Creating raw reports dataframe (Skip if done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a079f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_sections = df_sections.rename(columns={'study': 'study_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cefdd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the raw report to the sections df\n",
    "\n",
    "\n",
    "paths = df_record[\"path\"].tolist()\n",
    "study_ids = df_record[\"study_id\"].tolist()\n",
    "\n",
    "study_ids_new=[]\n",
    "\n",
    "for study in study_ids:\n",
    "    \n",
    "    new_id = 's' + str(study)\n",
    "    study_ids_new.append(new_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d422556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p10\n",
      "p11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9ce47c1e5491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mdf_raw_reports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_raw_reports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9ce47c1e5491>\u001b[0m in \u001b[0;36msearchDirectory\u001b[0;34m(mainDirectory, df_raw_reports)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubDirectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch171/lib/python3.6/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creates dataframe with study id and raw reports, prints the name of the main directory on enter so you can see the progress\n",
    "# overall there is p10-p19 directories, should take around 25-30 min to run as far as I remember\n",
    "\n",
    "import ntpath\n",
    "\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def searchDirectory(mainDirectory,df_raw_reports):\n",
    "\n",
    "\n",
    "    directory = '/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/files/' + mainDirectory\n",
    "    report_ext = '.txt'\n",
    "    image_ext = '.jpg'\n",
    "    \n",
    "    print(mainDirectory)\n",
    "    for subDirectory in os.listdir(directory):\n",
    "        full_path = directory + '/' + subDirectory\n",
    "       \n",
    "        for root, dirs, files in os.walk(full_path):\n",
    "            \n",
    "            for filename in files:\n",
    "\n",
    "                if filename.endswith(report_ext):\n",
    "                    with open(os.path.join(root, filename), 'r') as report:\n",
    "                        contents = report.read()\n",
    "                    \n",
    "                    df_temp = {\"study_id\": path_leaf(filename)[1:9], \"raw_report\": contents}\n",
    "                    df_raw_reports = df_raw_reports.append(df_temp, ignore_index = True)\n",
    "        \n",
    "    return df_raw_reports\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "directories = ['p10','p11','p12','p13','p14','p15','p16','p17', 'p18', 'p19' ]\n",
    "\n",
    "\n",
    "df_raw_reports = pd.DataFrame([], columns = ['study_id', 'raw_report'])\n",
    "\n",
    "for directory in directories:\n",
    "    df_raw_reports = searchDirectory(directory, df_raw_reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e087a3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>50414267</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>53189527</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>53911762</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>56699142</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s56699142/ea030e7a-2e3b134...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>57375967</td>\n",
       "      <td>10000764</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000764/s57375967/096052b7-d256dc4...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  study_id  subject_id  split  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n",
       "1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n",
       "2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n",
       "3  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  56699142    10000032  train   \n",
       "4  096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4  57375967    10000764  train   \n",
       "\n",
       "                                                path  \\\n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...   \n",
       "1  files/p10/p10000032/s53189527/2a2277a9-b0ded15...   \n",
       "2  files/p10/p10000032/s53911762/68b5c4b1-227d048...   \n",
       "3  files/p10/p10000032/s56699142/ea030e7a-2e3b134...   \n",
       "4  files/p10/p10000764/s57375967/096052b7-d256dc4...   \n",
       "\n",
       "                                          raw_report  \n",
       "0                                   FINAL REPORT\\...  \n",
       "1                                   FINAL REPORT\\...  \n",
       "2                                   FINAL REPORT\\...  \n",
       "3                                   FINAL REPORT\\...  \n",
       "4                                   FINAL REPORT\\...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_reports.head()\n",
    "df_raw_reports.to_csv('raw_reports.csv', index=False)\n",
    "\n",
    "\n",
    "df_raw_reports = pd.read_csv('df_raw_reports.csv')\n",
    "\n",
    "df_raw_reports= df_raw_reports.drop_duplicates(subset = [\"raw_report\"])\n",
    "df_raw_reports.reset_index(inplace=True)\n",
    "df_raw_reports = df_raw_reports.drop('index', 1)\n",
    "df_raw_reports = df_raw_reports.drop('Unnamed: 0', 1)\n",
    "df_raw_reports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87643490",
   "metadata": {},
   "source": [
    "### Classes for the Data Loader\n",
    "- Include: loading data, text preprocessing, words frequency check, tokenization, tokens-IDs-conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d08fa9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461ab424",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Vocabulary class\n",
    "##############################\n",
    "\n",
    "class Vocabulary:\n",
    "  \n",
    "    '''\n",
    "    __init__ method is called by default as soon as an object of this class is initiated\n",
    "    we use this method to initiate our vocab dictionaries\n",
    "    '''\n",
    "    def __init__(self, freq_threshold, max_size):\n",
    "        '''\n",
    "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
    "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
    "        '''\n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "    \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        \n",
    "    '''\n",
    "    a simple tokenizer to split on space and converts the sentence to list of words\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def preprocessing(text):\n",
    "        cleanedReport = re.sub(r'[^\\w\\s]','',text)            # remove punctuation (not word characters and whitespace)\n",
    "        cleanedReport = re.sub('_', '', cleanedReport)        # remove __ in the report\n",
    "        cleanedReport = re.sub(r'[\\d-]', '', cleanedReport)   # remove numbers in the report \n",
    "        cleanedReport = re.sub('\\n', '', cleanedReport)\n",
    "        # need further Lemmatization?\n",
    "        return cleanedReport                                  # should be just a string\n",
    "    \n",
    "    '''\n",
    "    vocabulary frequency check:\n",
    "    '''\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        # calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
    "        frequencies = {}  #init the freq dict\n",
    "        #idx = 4 #index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk\n",
    "        idx = 4\n",
    "        \n",
    "        i=0\n",
    "        for sentence in sentence_list:\n",
    "            i+=1\n",
    "            if (i % 10000 == 0):\n",
    "                print(i) \n",
    "                \n",
    "            preprocessed = self.preprocessing(sentence)\n",
    "            preprocessedList = list(preprocessed.split(\" \"))\n",
    "            for word in preprocessedList:\n",
    "                if word not in frequencies.keys():\n",
    "                    frequencies[word]=1\n",
    "                else:\n",
    "                    frequencies[word]+=1\n",
    "              \n",
    "                    \n",
    "        #limit vocab by removing low freq words\n",
    "        frequencies = {k:v for k,v in frequencies.items() if v > self.freq_threshold} \n",
    "        \n",
    "        #limit vocab to the max_size specified\n",
    "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
    "            \n",
    "        return frequencies\n",
    "\n",
    "            \n",
    "    '''\n",
    "    tokenization and converting tokens to IDs\n",
    "    '''    \n",
    "    def tokenization(self, text):\n",
    "        \n",
    "        # change here to use different tokenizer models:\n",
    "\n",
    "        # add tokenizer\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        tokenized_report = [\"[CLS]\"]\n",
    "        tokenized_report += tokenized_text\n",
    "        tokenized_report.append(\"[SEP]\")\n",
    "        numericalized_report = tokenizer.convert_tokens_to_ids(tokenized_report)\n",
    "        \n",
    "#         # Another encoding method which works on bert-base-uncased tokenizer but not working well with not the others...\n",
    "#         text_str = ' '.join([str(elem) for elem in text])\n",
    "#         encoded = tokenizer.encode_plus(text=text_str,  # the sentence to be encoded\n",
    "#                                         add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "#                                         return_attention_mask = True,  # Generate the attention mask\n",
    "#                                         #return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "#                                         )\n",
    "        \n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b11816a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Train dataset\n",
    "###############\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_train, target_column, transform=None, freq_threshold = 0,\n",
    "                vocab_max_size = 5000):\n",
    "       \n",
    "        self.df_train = df_train\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.target_text = df_train[target_column]\n",
    "\n",
    "        self.report_vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
    "        self.frequencyDict = self.report_vocab.build_vocabulary(self.target_text.tolist())  # build vocab for whole thing (list of whole thing)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_train)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "  \n",
    "        target_text = self.target_text[index]                             # string\n",
    "        target_text = self.report_vocab.preprocessing(target_text)\n",
    "        \n",
    "        for word in target_text.split():\n",
    "            if word not in self.frequencyDict.keys():\n",
    "                target_text = target_text.replace(word, \"\")\n",
    "            \n",
    "        numericalized_report = self.report_vocab.tokenization(target_text)\n",
    "        \n",
    "        # uncomment the following to convert the list to tensor and return (apparently BERT works without tensor)\n",
    "        # return torch.tensor(numericalized_report)\n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba194e",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97634e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a little example on just 10 reports\n",
    "#df_train_test = df_raw_reports[0:10]\n",
    "df_train_test = df_raw_reports[0:10]\n",
    "\n",
    "train_dataset = TrainDataset(df_train_test, \"raw_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae69bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 175, 1114, 1207, 15415, 1112, 14375, 1116, 174, 7501, 1111, 8974, 5531, 2229, 185, 1161, 1105, 11937, 7577, 3839, 9505, 1175, 1110, 1185, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 1115, 1211, 2620, 4248, 15070, 6719, 1103, 3621, 2660, 16418, 2050, 14196, 27316, 1110, 2999, 16973, 1933, 1166, 1103, 1286, 13093, 9046, 1439, 1103, 7209, 1103, 3077, 1181, 3105, 14701, 1110, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 1104, 1103, 16530, 1286, 3971, 1105, 5001, 10346, 1132, 2382, 8351, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102]\n",
      "\n",
      "\n",
      "Number of tokens:  114\n"
     ]
    }
   ],
   "source": [
    "# we can see the results, e.g.\n",
    "example = train_dataset[0]\n",
    "print(example)\n",
    "print(\"\\n\")\n",
    "print(\"Number of tokens: \", len(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a97367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens conversion:\n",
      "['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'and', 'la', '##t', 'indication', 'f', 'with', 'new', 'onset', 'as', '##cite', '##s', 'e', '##val', 'for', 'infection', 'technique', 'chest', 'p', '##a', 'and', 'lateral', 'comparison', 'none', 'findings', 'there', 'is', 'no', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'or', 'p', '##ne', '##um', '##oth', '##orax', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', 'that', 'most', 'likely', 'represent', 'nipple', 'shadows', 'the', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'is', 'normal', 'clips', 'project', 'over', 'the', 'left', 'lung', 'potentially', 'within', 'the', 'breast', 'the', 'image', '##d', 'upper', 'abdomen', 'is', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'of', 'the', 'posterior', 'left', 'sixth', 'and', 'seventh', 'ribs', 'are', 'noted', 'impression', 'no', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]']\n",
      "\n",
      "\n",
      "Decoding:\n",
      "[CLS] final report examination chest pa and lat indication f with new onset ascites eval for infection technique chest pa and lateral comparison none findings there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted impression no acute cardiopulmonary process [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Conversion back to tokens:\n",
    "print(\"Tokens conversion:\")\n",
    "print(tokenizer.convert_ids_to_tokens(example))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Decode it back into sentences (tokens of subwords will be merged together)\n",
    "print(\"Decoding:\")\n",
    "print(tokenizer.decode(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285bca7d",
   "metadata": {},
   "source": [
    "### Chunking to unify token lengths of each sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30839356",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Chunking Function \n",
    "####################\n",
    "\n",
    "def chunking(dataset, df, chunk_size = 128):\n",
    "    \n",
    "    concat = []\n",
    "    for i in range(len(dataset)):     # Add all tokens together in one list\n",
    "        concat.extend(dataset[df.index[i]])\n",
    "        \n",
    "    total_length = len(concat)\n",
    "    print(\"Total concatenated length: \", total_length)\n",
    "    \n",
    "    chunks = lambda concat, chunk_size: [concat[i:i+chunk_size] for i in range(0, total_length, chunk_size)]  \n",
    "    chunked_text = chunks(concat, chunk_size)\n",
    "    \n",
    "    if len(chunked_text[-1]) < chunk_size:\n",
    "        print(\"Removed last chunk: \", len(chunked_text[-1]), \" tokens\")\n",
    "        chunked_text.pop()     # remove last element \n",
    "    \n",
    "    return chunked_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f0858a",
   "metadata": {},
   "source": [
    "##### (Example - for 3 reports):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fd70e1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total concatenated length:  311\n",
      "Removed last chunk:  55  tokens\n",
      "\n",
      "\n",
      "[CLS] final report examination chest portable ap indication year old man with toxic ingestion interval changes technique single frontal view of the chest comparison impression the patient is rotated allowing this limitation cardiomegaly is stable a right ij catheter tip is in the mid svc there is no pneumothorax or pleural effusion left lower lobe opacities are consistent with minimal atelectasis pulmonary edema has almost completely resolved [SEP] [CLS] final report history yearold female with respiratory distress and severe copd evaluate for interval change comparison prior radiographs of the chest dated findings portable semiupright radiograph of the\n",
      "\n",
      "\n",
      "chest demonstrates persistent retrocardiac patchy opacity likely atelectasis there is mild interstitial pulmonary edema most significant at the bases which is slightly improved there are tiny bilateral pleural effusions the cardiomediastinal and hilar contours are unchanged there is no pneumothorax impression persistent retrocardiac opacity likely represents atelectasis persistent although somewhat improved mild interstitial pulmonary edema worst at the bases [SEP] [CLS] final report pa and lateral chest history a yearold man with biventricular icd rule out pneumothorax check lead positions impression\n"
     ]
    }
   ],
   "source": [
    "example_df = df_raw_reports.sample(n=3)\n",
    "example_dataset = TrainDataset(example_df, \"raw_report\")\n",
    "chunked_text = chunking(example_dataset, example_df)\n",
    "\n",
    "for chunk in chunked_text:\n",
    "    print(\"\\n\")\n",
    "    print(tokenizer.decode(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01ec4c",
   "metadata": {},
   "source": [
    "### Chunking all training reports (Skip if done!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "083dd9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "Total concatenated length:  25994905\n",
      "Removed last chunk:  25  tokens\n"
     ]
    }
   ],
   "source": [
    "# # Uncomment if you want to do it yourself!\n",
    "# # For all training reports:\n",
    "# trainDf = df_raw_reports[0:len(df_raw_reports.index)]\n",
    "# train_dataset = TrainDataset(trainDf, \"raw_report\")\n",
    "# chunked_train_dataset = chunking(train_dataset, trainDf)    # chunk size = 128 \n",
    "\n",
    "# # Save as .csv file:\n",
    "# chunked_trainDF = pd.DataFrame(chunked_train_dataset)\n",
    "# chunked_trainDF.to_csv(\"chunked_trainDatasetAll.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ccd91",
   "metadata": {},
   "source": [
    "Each row contains one training sample with 128 tokens, in total there are 204085 training samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fb78d",
   "metadata": {},
   "source": [
    "### Reading the chunked train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "225971bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunked samples:     203085\n",
      "Number of tokens per samples:  128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>1509</td>\n",
       "      <td>2592</td>\n",
       "      <td>8179</td>\n",
       "      <td>2229</td>\n",
       "      <td>185</td>\n",
       "      <td>1161</td>\n",
       "      <td>1105</td>\n",
       "      <td>2495</td>\n",
       "      <td>1204</td>\n",
       "      <td>...</td>\n",
       "      <td>2229</td>\n",
       "      <td>185</td>\n",
       "      <td>1161</td>\n",
       "      <td>1105</td>\n",
       "      <td>2495</td>\n",
       "      <td>1204</td>\n",
       "      <td>12754</td>\n",
       "      <td>1607</td>\n",
       "      <td>175</td>\n",
       "      <td>1114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1603</td>\n",
       "      <td>1757</td>\n",
       "      <td>1104</td>\n",
       "      <td>2184</td>\n",
       "      <td>5531</td>\n",
       "      <td>2229</td>\n",
       "      <td>185</td>\n",
       "      <td>1161</td>\n",
       "      <td>1105</td>\n",
       "      <td>11937</td>\n",
       "      <td>...</td>\n",
       "      <td>1673</td>\n",
       "      <td>1818</td>\n",
       "      <td>12858</td>\n",
       "      <td>25632</td>\n",
       "      <td>1103</td>\n",
       "      <td>3621</td>\n",
       "      <td>2660</td>\n",
       "      <td>16418</td>\n",
       "      <td>2050</td>\n",
       "      <td>14196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27316</td>\n",
       "      <td>1110</td>\n",
       "      <td>2999</td>\n",
       "      <td>1254</td>\n",
       "      <td>1562</td>\n",
       "      <td>1132</td>\n",
       "      <td>2967</td>\n",
       "      <td>16973</td>\n",
       "      <td>20266</td>\n",
       "      <td>1166</td>\n",
       "      <td>...</td>\n",
       "      <td>7209</td>\n",
       "      <td>1105</td>\n",
       "      <td>1385</td>\n",
       "      <td>1286</td>\n",
       "      <td>23298</td>\n",
       "      <td>22869</td>\n",
       "      <td>1116</td>\n",
       "      <td>1132</td>\n",
       "      <td>2382</td>\n",
       "      <td>8351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1185</td>\n",
       "      <td>12104</td>\n",
       "      <td>3621</td>\n",
       "      <td>2660</td>\n",
       "      <td>16091</td>\n",
       "      <td>13505</td>\n",
       "      <td>7637</td>\n",
       "      <td>1616</td>\n",
       "      <td>1965</td>\n",
       "      <td>102</td>\n",
       "      <td>...</td>\n",
       "      <td>13093</td>\n",
       "      <td>2259</td>\n",
       "      <td>3566</td>\n",
       "      <td>4311</td>\n",
       "      <td>1112</td>\n",
       "      <td>22631</td>\n",
       "      <td>1137</td>\n",
       "      <td>20673</td>\n",
       "      <td>2129</td>\n",
       "      <td>26557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4035</td>\n",
       "      <td>18791</td>\n",
       "      <td>20512</td>\n",
       "      <td>102</td>\n",
       "      <td>101</td>\n",
       "      <td>1509</td>\n",
       "      <td>2592</td>\n",
       "      <td>8179</td>\n",
       "      <td>2229</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>7577</td>\n",
       "      <td>1110</td>\n",
       "      <td>1189</td>\n",
       "      <td>1106</td>\n",
       "      <td>2229</td>\n",
       "      <td>2070</td>\n",
       "      <td>21217</td>\n",
       "      <td>5422</td>\n",
       "      <td>9505</td>\n",
       "      <td>1112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2     3      4      5     6      7      8      9  ...  \\\n",
       "0    101   1509   2592  8179   2229    185  1161   1105   2495   1204  ...   \n",
       "1   1603   1757   1104  2184   5531   2229   185   1161   1105  11937  ...   \n",
       "2  27316   1110   2999  1254   1562   1132  2967  16973  20266   1166  ...   \n",
       "3   1185  12104   3621  2660  16091  13505  7637   1616   1965    102  ...   \n",
       "4   4035  18791  20512   102    101   1509  2592   8179   2229    185  ...   \n",
       "\n",
       "     118   119    120    121    122    123    124    125   126    127  \n",
       "0   2229   185   1161   1105   2495   1204  12754   1607   175   1114  \n",
       "1   1673  1818  12858  25632   1103   3621   2660  16418  2050  14196  \n",
       "2   7209  1105   1385   1286  23298  22869   1116   1132  2382   8351  \n",
       "3  13093  2259   3566   4311   1112  22631   1137  20673  2129  26557  \n",
       "4   7577  1110   1189   1106   2229   2070  21217   5422  9505   1112  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_trainDataset = pd.read_csv('chunked_trainDatasetAll.csv')\n",
    "chunked_trainDataset = chunked_trainDataset.drop('Unnamed: 0', 1)\n",
    "print(\"Number of chunked samples:    \", chunked_trainDataset.shape[0])\n",
    "print(\"Number of tokens per samples: \", chunked_trainDataset.shape[1])\n",
    "chunked_trainDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a5d7e5",
   "metadata": {},
   "source": [
    "##### Example in converting 3 samples (first 3 rows) back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2e833bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the list:   203085\n",
      "<class 'list'>\n",
      "\n",
      "\n",
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 175, 1114, 1207, 15415, 1112, 14375, 1116, 174, 7501, 1111, 8974, 5531, 2229, 185, 1161, 1105, 11937, 7577, 3839, 9505, 1175, 1110, 1185, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 1115, 1211, 2620, 4248, 15070, 6719, 1103, 3621, 2660, 16418, 2050, 14196, 27316, 1110, 2999, 16973, 1933, 1166, 1103, 1286, 13093, 9046, 1439, 1103, 7209, 1103, 3077, 1181, 3105, 14701, 1110, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 1104, 1103, 16530, 1286, 3971, 1105, 5001, 10346, 1132, 2382, 8351, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 1607, 175, 1114]\n",
      "['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'and', 'la', '##t', 'indication', 'f', 'with', 'new', 'onset', 'as', '##cite', '##s', 'e', '##val', 'for', 'infection', 'technique', 'chest', 'p', '##a', 'and', 'lateral', 'comparison', 'none', 'findings', 'there', 'is', 'no', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'or', 'p', '##ne', '##um', '##oth', '##orax', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', 'that', 'most', 'likely', 'represent', 'nipple', 'shadows', 'the', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'is', 'normal', 'clips', 'project', 'over', 'the', 'left', 'lung', 'potentially', 'within', 'the', 'breast', 'the', 'image', '##d', 'upper', 'abdomen', 'is', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'of', 'the', 'posterior', 'left', 'sixth', 'and', 'seventh', 'ribs', 'are', 'noted', 'impression', 'no', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]', '[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'and', 'la', '##t', 'indication', 'history', 'f', 'with']\n",
      "\n",
      "\n",
      "[1603, 1757, 1104, 2184, 5531, 2229, 185, 1161, 1105, 11937, 7577, 9505, 1103, 17688, 2394, 2050, 14196, 1105, 20844, 5815, 14255, 18834, 1116, 1132, 2999, 26600, 191, 2225, 21608, 5332, 1110, 2999, 8682, 1132, 2330, 1185, 185, 1513, 12602, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 1110, 1675, 2967, 16973, 1132, 1254, 1562, 20266, 1166, 1103, 1286, 7209, 6456, 1286, 25984, 23298, 22869, 1116, 1132, 1145, 1231, 7160, 8351, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 22832, 1785, 102, 101, 1509, 2592, 8179, 2229, 15139, 170, 1643, 12754, 175, 1114, 21810, 12104, 1965, 7577, 2229, 2070, 15241, 9505, 1423, 22172, 2458, 1104, 1103, 2229, 2136, 1175, 1110, 1185, 17811, 20994, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 1103, 3621, 2660, 16418, 2050, 14196]\n",
      "['short', '##ness', 'of', 'breath', 'technique', 'chest', 'p', '##a', 'and', 'lateral', 'comparison', 'findings', 'the', 'cardiac', 'media', '##st', '##inal', 'and', 'hi', '##lar', 'con', '##tour', '##s', 'are', 'normal', 'pulmonary', 'v', '##as', '##cula', '##ture', 'is', 'normal', 'lungs', 'are', 'clear', 'no', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'or', 'p', '##ne', '##um', '##oth', '##orax', 'is', 'present', 'multiple', 'clips', 'are', 'again', 'seen', 'projecting', 'over', 'the', 'left', 'breast', 'remote', 'left', '##sided', 'rib', 'fracture', '##s', 'are', 'also', 're', 'demonstrated', 'impression', 'no', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'abnormal', '##ity', '[SEP]', '[CLS]', 'final', 'report', 'examination', 'chest', 'portable', 'a', '##p', 'indication', 'f', 'with', 'cough', 'acute', 'process', 'comparison', 'chest', 'radio', '##graph', 'findings', 'single', 'frontal', 'view', 'of', 'the', 'chest', 'provided', 'there', 'is', 'no', 'focal', 'consolidation', 'e', '##ff', '##usion', 'or', 'p', '##ne', '##um', '##oth', '##orax', 'the', 'card', '##io', '##media', '##st', '##inal']\n",
      "\n",
      "\n",
      "[27316, 1110, 2999, 1254, 1562, 1132, 2967, 16973, 20266, 1166, 1103, 1286, 7209, 1105, 6456, 1286, 25984, 23298, 22869, 1116, 1185, 1714, 1586, 2071, 1103, 1268, 23123, 2386, 1465, 7880, 20484, 1306, 1110, 1562, 8351, 1185, 12104, 1107, 4487, 1582, 6533, 6617, 1665, 1965, 102, 101, 1509, 2592, 12754, 1214, 1385, 1590, 1114, 172, 3161, 1197, 15342, 1548, 5531, 22172, 2229, 2070, 21217, 1127, 3836, 1114, 1103, 5351, 1107, 1103, 10170, 1700, 7577, 2070, 21217, 1121, 1105, 9505, 1103, 8682, 1132, 2330, 1104, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 1103, 1762, 2060, 1110, 2999, 1103, 2394, 2050, 14196, 14255, 18834, 1116, 1132, 2999, 2967, 13467, 16973, 1933, 1166, 1103, 1286, 7209, 1105, 1385, 1286, 23298, 22869, 1116, 1132, 2382, 8351]\n",
      "['silhouette', 'is', 'normal', 'again', 'seen', 'are', 'multiple', 'clips', 'projecting', 'over', 'the', 'left', 'breast', 'and', 'remote', 'left', '##sided', 'rib', 'fracture', '##s', 'no', 'free', 'air', 'below', 'the', 'right', 'hem', '##id', '##ia', '##ph', '##rag', '##m', 'is', 'seen', 'impression', 'no', 'acute', 'in', '##tra', '##th', '##ora', '##ci', '##c', 'process', '[SEP]', '[CLS]', 'final', 'report', 'indication', 'year', 'old', 'woman', 'with', 'c', '##ir', '##r', '##hos', '##is', 'technique', 'frontal', 'chest', 'radio', '##graphs', 'were', 'obtained', 'with', 'the', 'patient', 'in', 'the', 'upright', 'position', 'comparison', 'radio', '##graphs', 'from', 'and', 'findings', 'the', 'lungs', 'are', 'clear', 'of', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'or', 'p', '##ne', '##um', '##oth', '##orax', 'the', 'heart', 'size', 'is', 'normal', 'the', 'media', '##st', '##inal', 'con', '##tour', '##s', 'are', 'normal', 'multiple', 'surgical', 'clips', 'project', 'over', 'the', 'left', 'breast', 'and', 'old', 'left', 'rib', 'fracture', '##s', 'are', 'noted', 'impression']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunked_dfList = chunked_trainDataset.values.tolist()   # It is a big list containing 203085 samples (list)\n",
    "print(\"Length of the list:  \", len(chunked_dfList))\n",
    "print(type(chunked_dfList[0]))\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(chunked_dfList[i])\n",
    "    print(tokenizer.convert_ids_to_tokens(chunked_dfList[i]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46293cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
