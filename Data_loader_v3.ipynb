{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cbcccb6",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ac4754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /media/SharedUsers/dlc19/home/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# All imports \n",
    "##################\n",
    "\n",
    "#sys libs\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data manupulation libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "#pandarallel.initialize()\n",
    "\n",
    "#string manupulation libs\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "\n",
    "#torch libs\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "# data manipulations\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import pydicom\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd071ae7",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4ba336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>50414267</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>53189527</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>53911762</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>56699142</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s56699142/ea030e7a-2e3b134...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>57375967</td>\n",
       "      <td>10000764</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000764/s57375967/096052b7-d256dc4...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  study_id  subject_id  split  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n",
       "1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n",
       "2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n",
       "3  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  56699142    10000032  train   \n",
       "4  096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4  57375967    10000764  train   \n",
       "\n",
       "                                                path  \\\n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...   \n",
       "1  files/p10/p10000032/s53189527/2a2277a9-b0ded15...   \n",
       "2  files/p10/p10000032/s53911762/68b5c4b1-227d048...   \n",
       "3  files/p10/p10000032/s56699142/ea030e7a-2e3b134...   \n",
       "4  files/p10/p10000764/s57375967/096052b7-d256dc4...   \n",
       "\n",
       "                                          raw_report  \n",
       "0                                   FINAL REPORT\\...  \n",
       "1                                   FINAL REPORT\\...  \n",
       "2                                   FINAL REPORT\\...  \n",
       "3                                   FINAL REPORT\\...  \n",
       "4                                   FINAL REPORT\\...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used to make the csv with raw reports, not really important anymore once we have the reports in a csv\n",
    "cxr_root_path = \"/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/\"\n",
    "df_record = pd.read_csv('{}/cxr-record-list.csv.gz'.format(cxr_root_path), sep=',')\n",
    "df_split = pd.read_csv('{}/mimic-cxr-2.0.0-split.csv.gz'.format(cxr_root_path))\n",
    "\n",
    "df_temp = df_split.merge(df_record, on=['subject_id', 'study_id', 'dicom_id'], how='left')\n",
    "\n",
    "df_sections = pd.read_csv('{}/mimic-cxr-sections/mimic_cxr_sectioned.csv'.format(cxr_root_path))\n",
    "\n",
    "# if you already have the df_raw_reports.csv just uncomment the following line and skipp to the vocabulary class\n",
    "\n",
    "# extract the csv into a dataframe\n",
    "df_raw_reports = pd.read_csv('df_raw_reports.csv')\n",
    "# discard duplicates of the reports as there were originally more rows than reports (as there is sometimes more images per study)\n",
    "df_raw_reports= df_raw_reports.drop_duplicates(subset = [\"raw_report\"])\n",
    "#restart the index column as it was filtered from larger data and the indices were messed up\n",
    "df_raw_reports.reset_index(inplace=True)\n",
    "#discard some unimportant columns\n",
    "df_raw_reports = df_raw_reports.drop('index', 1)\n",
    "df_raw_reports = df_raw_reports.drop('Unnamed: 0', 1)\n",
    "# print the head to check if looks like intended\n",
    "df_raw_reports.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53429f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377110, 4)\n",
      "5159\n"
     ]
    }
   ],
   "source": [
    "# Checking test dataset number\n",
    "df_split.head()\n",
    "print(df_split.shape)\n",
    "\n",
    "split = df_split[\"split\"].tolist()\n",
    "idx = 0\n",
    "for elem in split:\n",
    "    if elem == \"test\":\n",
    "        idx+=1\n",
    "\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ca427",
   "metadata": {},
   "source": [
    "### Creating raw reports dataframe (Skip if done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9962bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_sections = df_sections.rename(columns={'study': 'study_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa1aadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the raw report to the sections df\n",
    "\n",
    "\n",
    "paths = df_record[\"path\"].tolist()\n",
    "study_ids = df_record[\"study_id\"].tolist()\n",
    "\n",
    "study_ids_new=[]\n",
    "\n",
    "for study in study_ids:\n",
    "    \n",
    "    new_id = 's' + str(study)\n",
    "    study_ids_new.append(new_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802a8780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p10\n",
      "p11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9ce47c1e5491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mdf_raw_reports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_raw_reports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9ce47c1e5491>\u001b[0m in \u001b[0;36msearchDirectory\u001b[0;34m(mainDirectory, df_raw_reports)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubDirectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch171/lib/python3.6/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creates dataframe with study id and raw reports, prints the name of the main directory on enter so you can see the progress\n",
    "# overall there is p10-p19 directories, should take around 25-30 min to run as far as I remember\n",
    "\n",
    "import ntpath\n",
    "\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def searchDirectory(mainDirectory,df_raw_reports):\n",
    "\n",
    "\n",
    "    directory = '/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/files/' + mainDirectory\n",
    "    report_ext = '.txt'\n",
    "    image_ext = '.jpg'\n",
    "    \n",
    "    print(mainDirectory)\n",
    "    for subDirectory in os.listdir(directory):\n",
    "        full_path = directory + '/' + subDirectory\n",
    "       \n",
    "        for root, dirs, files in os.walk(full_path):\n",
    "            \n",
    "            for filename in files:\n",
    "\n",
    "                if filename.endswith(report_ext):\n",
    "                    with open(os.path.join(root, filename), 'r') as report:\n",
    "                        contents = report.read()\n",
    "                    \n",
    "                    df_temp = {\"study_id\": path_leaf(filename)[1:9], \"raw_report\": contents}\n",
    "                    df_raw_reports = df_raw_reports.append(df_temp, ignore_index = True)\n",
    "        \n",
    "    return df_raw_reports\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "directories = ['p10','p11','p12','p13','p14','p15','p16','p17', 'p18', 'p19' ]\n",
    "\n",
    "\n",
    "df_raw_reports = pd.DataFrame([], columns = ['study_id', 'raw_report'])\n",
    "\n",
    "for directory in directories:\n",
    "    df_raw_reports = searchDirectory(directory, df_raw_reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b03895e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>50414267</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>53189527</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>53911762</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>56699142</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s56699142/ea030e7a-2e3b134...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>57375967</td>\n",
       "      <td>10000764</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000764/s57375967/096052b7-d256dc4...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  study_id  subject_id  split  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n",
       "1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n",
       "2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n",
       "3  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  56699142    10000032  train   \n",
       "4  096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4  57375967    10000764  train   \n",
       "\n",
       "                                                path  \\\n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...   \n",
       "1  files/p10/p10000032/s53189527/2a2277a9-b0ded15...   \n",
       "2  files/p10/p10000032/s53911762/68b5c4b1-227d048...   \n",
       "3  files/p10/p10000032/s56699142/ea030e7a-2e3b134...   \n",
       "4  files/p10/p10000764/s57375967/096052b7-d256dc4...   \n",
       "\n",
       "                                          raw_report  \n",
       "0                                   FINAL REPORT\\...  \n",
       "1                                   FINAL REPORT\\...  \n",
       "2                                   FINAL REPORT\\...  \n",
       "3                                   FINAL REPORT\\...  \n",
       "4                                   FINAL REPORT\\...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_reports.head()\n",
    "df_raw_reports.to_csv('raw_reports.csv', index=False)\n",
    "\n",
    "\n",
    "df_raw_reports = pd.read_csv('df_raw_reports.csv')\n",
    "\n",
    "df_raw_reports= df_raw_reports.drop_duplicates(subset = [\"raw_report\"])\n",
    "df_raw_reports.reset_index(inplace=True)\n",
    "df_raw_reports = df_raw_reports.drop('index', 1)\n",
    "df_raw_reports = df_raw_reports.drop('Unnamed: 0', 1)\n",
    "df_raw_reports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d40e7d",
   "metadata": {},
   "source": [
    "### Classes for the Data Loader\n",
    "- Include: loading data, text preprocessing, words frequency check, tokenization, tokens-IDs-conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04fcc22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9f0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Vocabulary class\n",
    "##############################\n",
    "\n",
    "class Vocabulary:\n",
    "  \n",
    "    '''\n",
    "    __init__ method is called by default as soon as an object of this class is initiated\n",
    "    we use this method to initiate our vocab dictionaries\n",
    "    '''\n",
    "    def __init__(self, freq_threshold, max_size):\n",
    "        '''\n",
    "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
    "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
    "        '''\n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        \n",
    "    '''\n",
    "    a simple tokenizer to split on space and converts the sentence to list of words\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def preprocessing(text):\n",
    "        cleanedReport = re.sub(r'[^\\w\\s]','',text)            # remove punctuation (not word characters and whitespace)\n",
    "        cleanedReport = re.sub('_', '', cleanedReport)        # remove __ in the report\n",
    "        cleanedReport = re.sub(r'[\\d-]', '', cleanedReport)   # remove numbers in the report \n",
    "        cleanedReport = re.sub('\\n', '', cleanedReport)\n",
    "\n",
    "        return cleanedReport                                  # should be just a string\n",
    "    \n",
    "    '''\n",
    "    vocabulary frequency check:\n",
    "    '''\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        # calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
    "        frequencies = {}  #init the freq dict\n",
    "        #idx = 4 #index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk\n",
    "        idx = 4\n",
    "        \n",
    "        i=0\n",
    "        for sentence in sentence_list:\n",
    "            i+=1\n",
    "            if (i % 10000 == 0):\n",
    "                print(i) \n",
    "                \n",
    "            preprocessed = self.preprocessing(sentence)\n",
    "            preprocessedList = list(preprocessed.split(\" \"))\n",
    "            for word in preprocessedList:\n",
    "                if word not in frequencies.keys():\n",
    "                    frequencies[word]=1\n",
    "                else:\n",
    "                    frequencies[word]+=1\n",
    "              \n",
    "                    \n",
    "        #limit vocab by removing low freq words\n",
    "        frequencies = {k:v for k,v in frequencies.items() if v > self.freq_threshold} \n",
    "        \n",
    "        #limit vocab to the max_size specified\n",
    "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
    "            \n",
    "        return frequencies\n",
    "\n",
    "            \n",
    "    '''\n",
    "    tokenization and converting tokens to IDs\n",
    "    '''    \n",
    "    def tokenization(self, text):\n",
    "        \n",
    "        # change here to use different tokenizer models:\n",
    "\n",
    "        # add tokenizer\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        tokenized_text = [word for word in tokenized_text if word not in self.stop_words]    # Stopword removal\n",
    "        \n",
    "        tokenized_report = [\"[CLS]\"]\n",
    "        tokenized_report += tokenized_text\n",
    "        tokenized_report.append(\"[SEP]\")\n",
    "        numericalized_report = tokenizer.convert_tokens_to_ids(tokenized_report)\n",
    "        \n",
    "#         # Another encoding method which works on bert-base-uncased tokenizer but not working well with not the others...\n",
    "#         text_str = ' '.join([str(elem) for elem in text])\n",
    "#         encoded = tokenizer.encode_plus(text=text_str,  # the sentence to be encoded\n",
    "#                                         add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "#                                         return_attention_mask = True,  # Generate the attention mask\n",
    "#                                         #return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "#                                         )\n",
    "        \n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae1bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Train dataset\n",
    "###############\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_train, target_column, transform=None, freq_threshold = 0,\n",
    "                vocab_max_size = 5000):\n",
    "       \n",
    "        self.df_train = df_train\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.target_text = df_train[target_column]\n",
    "\n",
    "        self.report_vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
    "        self.frequencyDict = self.report_vocab.build_vocabulary(self.target_text.tolist())  # build vocab for whole thing (list of whole thing)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_train)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "  \n",
    "        target_text = self.target_text[index]                             # string\n",
    "        target_text = self.report_vocab.preprocessing(target_text)\n",
    "        \n",
    "        for word in target_text.split():\n",
    "            if word not in self.frequencyDict.keys():\n",
    "                target_text = target_text.replace(word, \"\")\n",
    "            \n",
    "        numericalized_report = self.report_vocab.tokenization(target_text)\n",
    "        \n",
    "        # uncomment the following to convert the list to tensor and return (apparently BERT works without tensor)\n",
    "        # return torch.tensor(numericalized_report)\n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f2882",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3bfde55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a little example on just 10 reports\n",
    "#df_train_test = df_raw_reports[0:10]\n",
    "df_train_test = df_raw_reports[0:10]\n",
    "\n",
    "train_dataset = TrainDataset(df_train_test, \"raw_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c995927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 175, 1207, 15415, 14375, 1116, 174, 7501, 8974, 5531, 2229, 185, 1161, 11937, 7577, 3839, 9505, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 2620, 4248, 15070, 6719, 3621, 2660, 16418, 2050, 14196, 27316, 2999, 16973, 1933, 1286, 13093, 9046, 1439, 7209, 3077, 1181, 3105, 14701, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 16530, 1286, 3971, 5001, 10346, 2382, 8351, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102]\n",
      "\n",
      "\n",
      "Number of tokens:  91\n"
     ]
    }
   ],
   "source": [
    "# we can see the results, e.g.\n",
    "example = train_dataset[0]\n",
    "print(example)\n",
    "print(\"\\n\")\n",
    "print(\"Number of tokens: \", len(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fad264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens conversion:\n",
      "['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'la', '##t', 'indication', 'f', 'new', 'onset', '##cite', '##s', 'e', '##val', 'infection', 'technique', 'chest', 'p', '##a', 'lateral', 'comparison', 'none', 'findings', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'p', '##ne', '##um', '##oth', '##orax', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', 'likely', 'represent', 'nipple', 'shadows', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'normal', 'clips', 'project', 'left', 'lung', 'potentially', 'within', 'breast', 'image', '##d', 'upper', 'abdomen', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'posterior', 'left', 'sixth', 'seventh', 'ribs', 'noted', 'impression', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]']\n",
      "\n",
      "\n",
      "Decoding:\n",
      "[CLS] final report examination chest pa lat indication f new onsetcites eval infection technique chest pa lateral comparison none findings focal consolidation pleural effusion pneumothorax bilateral nodular opacities likely represent nipple shadows cardiomediastinal silhouette normal clips project left lung potentially within breast imaged upper abdomen unremarkable chronic deformity posterior left sixth seventh ribs noted impression acute cardiopulmonary process [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Conversion back to tokens:\n",
    "print(\"Tokens conversion:\")\n",
    "print(tokenizer.convert_ids_to_tokens(example))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Decode it back into sentences (tokens of subwords will be merged together)\n",
    "print(\"Decoding:\")\n",
    "print(tokenizer.decode(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f84458",
   "metadata": {},
   "source": [
    "### Chunking to unify token lengths of each sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcd4fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Chunking Function \n",
    "####################\n",
    "\n",
    "def chunking(dataset, df, chunk_size = 128):\n",
    "    \n",
    "    concat = []\n",
    "    for i in range(len(dataset)):     # Add all tokens together in one list\n",
    "        concat.extend(dataset[df.index[i]])\n",
    "        \n",
    "    total_length = len(concat)\n",
    "    print(\"Total concatenated length: \", total_length)\n",
    "    \n",
    "    chunks = lambda concat, chunk_size: [concat[i:i+chunk_size] for i in range(0, total_length, chunk_size)]  \n",
    "    chunked_text = chunks(concat, chunk_size)\n",
    "    \n",
    "    if len(chunked_text[-1]) < chunk_size:\n",
    "        print(\"Removed last chunk: \", len(chunked_text[-1]), \" tokens\")\n",
    "        chunked_text.pop()     # remove last element \n",
    "    \n",
    "    return chunked_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc68cd",
   "metadata": {},
   "source": [
    "##### (Example - for 3 reports):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57747fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total concatenated length:  323\n",
      "Removed last chunk:  67  tokens\n",
      "\n",
      "\n",
      "[CLS] final report indication year old man bradycardiap temporary wire placement temporary wire location pulmonary edema technique portable chest radiograph comparison chest radiograph dated findings ett cm carina leftj centralnous catheter removed enteric tube extends stomach terminates beyond view radiograph right femoral temporary wire terminates likely right ventricle lateral view could provide definitive information round opacity irregular border right upper lobe concerning pneumonia persistent left lower lobe atelectasis hila pulmonary vasculatures normal cardiomediastinal silhouette normal pleural effusion pneumothorax fractures impression right upper lobe opacity concerning pneumonia\n",
      "\n",
      "\n",
      "pulmonary edema temporary wire positioned tip right ventricleification findings discussedrd telephone pm minutes discovery findings [SEP] [CLS] final report history yearold male chf history possible gout presents chest pain days comparison none findingsp lateral views chest exam limited secondary patient body habitus lungs clear consolidation effusion mildly increased intertial markings seen without frank edema cardiac silhouette appears enlarged could partially associated plate technique positioning acutesseous abnormality detected impression mild vascular congestion without definite acute cardiopulmonary process [SEP] [CLS] final report examination chest portablep indication year old womanpct cb gvhd new hypoxia currently diures\n"
     ]
    }
   ],
   "source": [
    "example_df = df_raw_reports.sample(n=3)\n",
    "example_dataset = TrainDataset(example_df, \"raw_report\")\n",
    "chunked_text = chunking(example_dataset, example_df)\n",
    "\n",
    "for chunk in chunked_text:\n",
    "    print(\"\\n\")\n",
    "    print(tokenizer.decode(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ff809",
   "metadata": {},
   "source": [
    "### Chunking all training reports (Skip if done!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "efb7e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "Total concatenated length:  25994905\n",
      "Removed last chunk:  25  tokens\n"
     ]
    }
   ],
   "source": [
    "# # Uncomment if you want to do it yourself!\n",
    "# # For all training reports:\n",
    "# trainDf = df_raw_reports[0:len(df_raw_reports.index)]\n",
    "# train_dataset = TrainDataset(trainDf, \"raw_report\")\n",
    "# chunked_train_dataset = chunking(train_dataset, trainDf)    # chunk size = 128 \n",
    "\n",
    "# # Save as .csv file:\n",
    "# chunked_trainDF = pd.DataFrame(chunked_train_dataset)\n",
    "# chunked_trainDF.to_csv(\"chunked_trainDatasetAll.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61bedc4",
   "metadata": {},
   "source": [
    "Each row contains one training sample with 128 tokens, in total there are 204085 training samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84c8e3",
   "metadata": {},
   "source": [
    "### Reading the chunked train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29486446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunked samples:     154696\n",
      "Number of tokens per samples:  128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>1509</td>\n",
       "      <td>2592</td>\n",
       "      <td>8179</td>\n",
       "      <td>2229</td>\n",
       "      <td>185</td>\n",
       "      <td>1161</td>\n",
       "      <td>2495</td>\n",
       "      <td>1204</td>\n",
       "      <td>12754</td>\n",
       "      <td>...</td>\n",
       "      <td>5815</td>\n",
       "      <td>14255</td>\n",
       "      <td>18834</td>\n",
       "      <td>1116</td>\n",
       "      <td>2999</td>\n",
       "      <td>26600</td>\n",
       "      <td>191</td>\n",
       "      <td>2225</td>\n",
       "      <td>21608</td>\n",
       "      <td>5332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2999</td>\n",
       "      <td>8682</td>\n",
       "      <td>2330</td>\n",
       "      <td>185</td>\n",
       "      <td>1513</td>\n",
       "      <td>12602</td>\n",
       "      <td>174</td>\n",
       "      <td>3101</td>\n",
       "      <td>17268</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>3161</td>\n",
       "      <td>1197</td>\n",
       "      <td>15342</td>\n",
       "      <td>1548</td>\n",
       "      <td>5531</td>\n",
       "      <td>22172</td>\n",
       "      <td>2229</td>\n",
       "      <td>2070</td>\n",
       "      <td>21217</td>\n",
       "      <td>3836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5351</td>\n",
       "      <td>10170</td>\n",
       "      <td>1700</td>\n",
       "      <td>7577</td>\n",
       "      <td>2070</td>\n",
       "      <td>21217</td>\n",
       "      <td>9505</td>\n",
       "      <td>8682</td>\n",
       "      <td>2330</td>\n",
       "      <td>17811</td>\n",
       "      <td>...</td>\n",
       "      <td>11019</td>\n",
       "      <td>1233</td>\n",
       "      <td>6617</td>\n",
       "      <td>11531</td>\n",
       "      <td>1116</td>\n",
       "      <td>1762</td>\n",
       "      <td>1499</td>\n",
       "      <td>2999</td>\n",
       "      <td>2060</td>\n",
       "      <td>8351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17811</td>\n",
       "      <td>20994</td>\n",
       "      <td>1286</td>\n",
       "      <td>13093</td>\n",
       "      <td>2259</td>\n",
       "      <td>3566</td>\n",
       "      <td>4311</td>\n",
       "      <td>22631</td>\n",
       "      <td>20673</td>\n",
       "      <td>2129</td>\n",
       "      <td>...</td>\n",
       "      <td>1849</td>\n",
       "      <td>2554</td>\n",
       "      <td>17811</td>\n",
       "      <td>20994</td>\n",
       "      <td>185</td>\n",
       "      <td>1513</td>\n",
       "      <td>12602</td>\n",
       "      <td>174</td>\n",
       "      <td>3101</td>\n",
       "      <td>17268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185</td>\n",
       "      <td>1673</td>\n",
       "      <td>1818</td>\n",
       "      <td>12858</td>\n",
       "      <td>25632</td>\n",
       "      <td>175</td>\n",
       "      <td>14687</td>\n",
       "      <td>26600</td>\n",
       "      <td>5048</td>\n",
       "      <td>14494</td>\n",
       "      <td>...</td>\n",
       "      <td>5815</td>\n",
       "      <td>14255</td>\n",
       "      <td>18834</td>\n",
       "      <td>1116</td>\n",
       "      <td>16684</td>\n",
       "      <td>1353</td>\n",
       "      <td>185</td>\n",
       "      <td>1513</td>\n",
       "      <td>12602</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1     2      3      4      5      6      7      8      9  ...  \\\n",
       "0    101   1509  2592   8179   2229    185   1161   2495   1204  12754  ...   \n",
       "1   2999   8682  2330    185   1513  12602    174   3101  17268    185  ...   \n",
       "2   5351  10170  1700   7577   2070  21217   9505   8682   2330  17811  ...   \n",
       "3  17811  20994  1286  13093   2259   3566   4311  22631  20673   2129  ...   \n",
       "4    185   1673  1818  12858  25632    175  14687  26600   5048  14494  ...   \n",
       "\n",
       "     118    119    120    121    122    123    124   125    126    127  \n",
       "0   5815  14255  18834   1116   2999  26600    191  2225  21608   5332  \n",
       "1   3161   1197  15342   1548   5531  22172   2229  2070  21217   3836  \n",
       "2  11019   1233   6617  11531   1116   1762   1499  2999   2060   8351  \n",
       "3   1849   2554  17811  20994    185   1513  12602   174   3101  17268  \n",
       "4   5815  14255  18834   1116  16684   1353    185  1513  12602    174  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_trainDataset = pd.read_csv('chunked_trainDatasetAllNoStopWord.csv')\n",
    "chunked_trainDataset = chunked_trainDataset.drop('Unnamed: 0', 1)\n",
    "print(\"Number of chunked samples:    \", chunked_trainDataset.shape[0])\n",
    "print(\"Number of tokens per samples: \", chunked_trainDataset.shape[1])\n",
    "chunked_trainDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a415a",
   "metadata": {},
   "source": [
    "##### Example in converting 3 samples (first 3 rows) back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6f32298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the list:   154696\n",
      "<class 'list'>\n",
      "\n",
      "\n",
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 175, 1207, 15415, 14375, 1116, 174, 7501, 8974, 5531, 2229, 185, 1161, 11937, 7577, 3839, 9505, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 2620, 4248, 15070, 6719, 3621, 2660, 16418, 2050, 14196, 27316, 2999, 16973, 1933, 1286, 13093, 9046, 1439, 7209, 3077, 1181, 3105, 14701, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 16530, 1286, 3971, 5001, 10346, 2382, 8351, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 1607, 175, 1603, 1757, 2184, 5531, 2229, 185, 1161, 11937, 7577, 9505, 17688, 2394, 2050, 14196, 20844, 5815, 14255, 18834, 1116, 2999, 26600, 191, 2225, 21608, 5332]\n",
      "['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'la', '##t', 'indication', 'f', 'new', 'onset', '##cite', '##s', 'e', '##val', 'infection', 'technique', 'chest', 'p', '##a', 'lateral', 'comparison', 'none', 'findings', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'p', '##ne', '##um', '##oth', '##orax', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', 'likely', 'represent', 'nipple', 'shadows', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'normal', 'clips', 'project', 'left', 'lung', 'potentially', 'within', 'breast', 'image', '##d', 'upper', 'abdomen', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'posterior', 'left', 'sixth', 'seventh', 'ribs', 'noted', 'impression', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]', '[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'la', '##t', 'indication', 'history', 'f', 'short', '##ness', 'breath', 'technique', 'chest', 'p', '##a', 'lateral', 'comparison', 'findings', 'cardiac', 'media', '##st', '##inal', 'hi', '##lar', 'con', '##tour', '##s', 'normal', 'pulmonary', 'v', '##as', '##cula', '##ture']\n",
      "\n",
      "\n",
      "[2999, 8682, 2330, 185, 1513, 12602, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 1675, 2967, 16973, 1562, 20266, 1286, 7209, 6456, 1286, 25984, 23298, 22869, 1116, 1145, 7160, 8351, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 22832, 1785, 102, 101, 1509, 2592, 8179, 2229, 15139, 1643, 12754, 175, 21810, 12104, 1965, 7577, 2229, 2070, 15241, 9505, 1423, 22172, 2458, 2229, 2136, 17811, 20994, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 3621, 2660, 16418, 2050, 14196, 27316, 2999, 1562, 2967, 16973, 20266, 1286, 7209, 6456, 1286, 25984, 23298, 22869, 1116, 1714, 1586, 1268, 23123, 2386, 1465, 7880, 20484, 1306, 1562, 8351, 12104, 4487, 1582, 6533, 6617, 1665, 1965, 102, 101, 1509, 2592, 12754, 1214, 1385, 1590, 172, 3161, 1197, 15342, 1548, 5531, 22172, 2229, 2070, 21217, 3836]\n",
      "['normal', 'lungs', 'clear', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'p', '##ne', '##um', '##oth', '##orax', 'present', 'multiple', 'clips', 'seen', 'projecting', 'left', 'breast', 'remote', 'left', '##sided', 'rib', 'fracture', '##s', 'also', 'demonstrated', 'impression', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'abnormal', '##ity', '[SEP]', '[CLS]', 'final', 'report', 'examination', 'chest', 'portable', '##p', 'indication', 'f', 'cough', 'acute', 'process', 'comparison', 'chest', 'radio', '##graph', 'findings', 'single', 'frontal', 'view', 'chest', 'provided', 'focal', 'consolidation', 'e', '##ff', '##usion', 'p', '##ne', '##um', '##oth', '##orax', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'normal', 'seen', 'multiple', 'clips', 'projecting', 'left', 'breast', 'remote', 'left', '##sided', 'rib', 'fracture', '##s', 'free', 'air', 'right', 'hem', '##id', '##ia', '##ph', '##rag', '##m', 'seen', 'impression', 'acute', '##tra', '##th', '##ora', '##ci', '##c', 'process', '[SEP]', '[CLS]', 'final', 'report', 'indication', 'year', 'old', 'woman', 'c', '##ir', '##r', '##hos', '##is', 'technique', 'frontal', 'chest', 'radio', '##graphs', 'obtained']\n",
      "\n",
      "\n",
      "[5351, 10170, 1700, 7577, 2070, 21217, 9505, 8682, 2330, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 185, 1673, 1818, 12858, 25632, 1762, 2060, 2999, 2394, 2050, 14196, 14255, 18834, 1116, 2999, 2967, 13467, 16973, 1933, 1286, 7209, 1385, 1286, 23298, 22869, 1116, 2382, 8351, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 101, 1509, 2592, 8179, 2229, 185, 1161, 2495, 1204, 12754, 177, 1183, 26466, 1465, 185, 1605, 22631, 7577, 3839, 9505, 185, 1161, 11937, 4696, 2229, 2136, 8682, 26449, 20725, 17811, 20994, 1286, 13093, 2259, 4903, 11937, 23123, 2386, 1465, 7880, 20484, 1306, 10496, 26557, 4035, 18791, 20512, 20557, 15328, 185, 1513, 12602, 3528, 4777, 3621, 2660, 16418, 2050, 14196, 27316, 9495, 12148, 1596, 9072, 11019, 1233, 6617, 11531, 1116, 1762, 1499, 2999, 2060, 8351]\n",
      "['patient', 'upright', 'position', 'comparison', 'radio', '##graphs', 'findings', 'lungs', 'clear', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'p', '##ne', '##um', '##oth', '##orax', 'heart', 'size', 'normal', 'media', '##st', '##inal', 'con', '##tour', '##s', 'normal', 'multiple', 'surgical', 'clips', 'project', 'left', 'breast', 'old', 'left', 'rib', 'fracture', '##s', 'noted', 'impression', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]', '[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'la', '##t', 'indication', 'h', '##y', '##pox', '##ia', 'p', '##na', '##piration', 'comparison', 'none', 'findings', 'p', '##a', 'lateral', 'views', 'chest', 'provided', 'lungs', 'adequately', '##erated', 'focal', 'consolidation', 'left', 'lung', 'base', 'adjacent', 'lateral', 'hem', '##id', '##ia', '##ph', '##rag', '##m', 'mild', 'vascular', 'en', '##gor', '##gement', 'bilateral', '##pical', 'p', '##le', '##ural', 'thick', '##ening', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'remarkable', '##ort', '##ic', 'arch', 'ca', '##l', '##ci', '##fication', '##s', 'heart', 'top', 'normal', 'size', 'impression']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunked_dfList = chunked_trainDataset.values.tolist()   # It is a big list containing 203085 samples (list)\n",
    "print(\"Length of the list:  \", len(chunked_dfList))\n",
    "print(type(chunked_dfList[0]))\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(chunked_dfList[i])\n",
    "    print(tokenizer.convert_ids_to_tokens(chunked_dfList[i]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c3c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
