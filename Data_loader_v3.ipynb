{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f9e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# DATA LOADING\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2fc45967",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# All imports \n",
    "##################\n",
    "\n",
    "#sys libs\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data manupulation libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "#pandarallel.initialize()\n",
    "\n",
    "#string manupulation libs\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import spacy\n",
    "\n",
    "#torch libs\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "# data manipulations\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import pydicom\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77019696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>50414267</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>53189527</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>53911762</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>56699142</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s56699142/ea030e7a-2e3b134...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>57375967</td>\n",
       "      <td>10000764</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000764/s57375967/096052b7-d256dc4...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  study_id  subject_id  split  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n",
       "1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n",
       "2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n",
       "3  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  56699142    10000032  train   \n",
       "4  096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4  57375967    10000764  train   \n",
       "\n",
       "                                                path  \\\n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...   \n",
       "1  files/p10/p10000032/s53189527/2a2277a9-b0ded15...   \n",
       "2  files/p10/p10000032/s53911762/68b5c4b1-227d048...   \n",
       "3  files/p10/p10000032/s56699142/ea030e7a-2e3b134...   \n",
       "4  files/p10/p10000764/s57375967/096052b7-d256dc4...   \n",
       "\n",
       "                                          raw_report  \n",
       "0                                   FINAL REPORT\\...  \n",
       "1                                   FINAL REPORT\\...  \n",
       "2                                   FINAL REPORT\\...  \n",
       "3                                   FINAL REPORT\\...  \n",
       "4                                   FINAL REPORT\\...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################\n",
    "# Prepare data\n",
    "#############################\n",
    "\n",
    "# Used to make the csv with raw reports, not really important anymore once we have the reports in a csv\n",
    "cxr_root_path = \"/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/\"\n",
    "df_record = pd.read_csv('{}/cxr-record-list.csv.gz'.format(cxr_root_path), sep=',')\n",
    "df_split = pd.read_csv('{}/mimic-cxr-2.0.0-split.csv.gz'.format(cxr_root_path))\n",
    "\n",
    "df_temp = df_split.merge(df_record, on=['subject_id', 'study_id', 'dicom_id'], how='left')\n",
    "\n",
    "df_sections = pd.read_csv('{}/mimic-cxr-sections/mimic_cxr_sectioned.csv'.format(cxr_root_path))\n",
    "\n",
    "# if you already have the df_raw_reports.csv just uncomment the following line and skipp to the vocabulary class\n",
    "\n",
    "# extract the csv into a dataframe\n",
    "df_raw_reports = pd.read_csv('df_raw_reports.csv')\n",
    "# discard duplicates of the reports as there were originally more rows than reports (as there is sometimes more images per study)\n",
    "df_raw_reports= df_raw_reports.drop_duplicates(subset = [\"raw_report\"])\n",
    "#restart the index column as it was filtered from larger data and the indices were messed up\n",
    "df_raw_reports.reset_index(inplace=True)\n",
    "#discard some unimportant columns\n",
    "df_raw_reports = df_raw_reports.drop('index', 1)\n",
    "df_raw_reports = df_raw_reports.drop('Unnamed: 0', 1)\n",
    "# print the head to check if looks like intended\n",
    "df_raw_reports.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a18fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_sections = df_sections.rename(columns={'study': 'study_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d03ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the raw report to the sections df\n",
    "\n",
    "\n",
    "paths = df_record[\"path\"].tolist()\n",
    "study_ids = df_record[\"study_id\"].tolist()\n",
    "\n",
    "study_ids_new=[]\n",
    "\n",
    "for study in study_ids:\n",
    "    \n",
    "    new_id = 's' + str(study)\n",
    "    study_ids_new.append(new_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e836968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p10\n",
      "p11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9ce47c1e5491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mdf_raw_reports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_raw_reports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9ce47c1e5491>\u001b[0m in \u001b[0;36msearchDirectory\u001b[0;34m(mainDirectory, df_raw_reports)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubDirectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch171/lib/python3.6/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creates dataframe with study id and raw reports, prints the name of the main directory on enter so you can see the progress\n",
    "# overall there is p10-p19 directories, should take around 25-30 min to run as far as I remember\n",
    "\n",
    "import ntpath\n",
    "\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def searchDirectory(mainDirectory,df_raw_reports):\n",
    "\n",
    "\n",
    "    directory = '/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/files/' + mainDirectory\n",
    "    report_ext = '.txt'\n",
    "    image_ext = '.jpg'\n",
    "    \n",
    "    print(mainDirectory)\n",
    "    for subDirectory in os.listdir(directory):\n",
    "        full_path = directory + '/' + subDirectory\n",
    "       \n",
    "        for root, dirs, files in os.walk(full_path):\n",
    "            \n",
    "            for filename in files:\n",
    "\n",
    "                if filename.endswith(report_ext):\n",
    "                    with open(os.path.join(root, filename), 'r') as report:\n",
    "                        contents = report.read()\n",
    "                    \n",
    "                    df_temp = {\"study_id\": path_leaf(filename)[1:9], \"raw_report\": contents}\n",
    "                    df_raw_reports = df_raw_reports.append(df_temp, ignore_index = True)\n",
    "        \n",
    "    return df_raw_reports\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "directories = ['p10','p11','p12','p13','p14','p15','p16','p17', 'p18', 'p19' ]\n",
    "\n",
    "\n",
    "df_raw_reports = pd.DataFrame([], columns = ['study_id', 'raw_report'])\n",
    "\n",
    "for directory in directories:\n",
    "    df_raw_reports = searchDirectory(directory, df_raw_reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0e8fd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>50414267</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>53189527</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>53911762</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>56699142</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s56699142/ea030e7a-2e3b134...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>57375967</td>\n",
       "      <td>10000764</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000764/s57375967/096052b7-d256dc4...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  study_id  subject_id  split  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n",
       "1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n",
       "2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n",
       "3  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  56699142    10000032  train   \n",
       "4  096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4  57375967    10000764  train   \n",
       "\n",
       "                                                path  \\\n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...   \n",
       "1  files/p10/p10000032/s53189527/2a2277a9-b0ded15...   \n",
       "2  files/p10/p10000032/s53911762/68b5c4b1-227d048...   \n",
       "3  files/p10/p10000032/s56699142/ea030e7a-2e3b134...   \n",
       "4  files/p10/p10000764/s57375967/096052b7-d256dc4...   \n",
       "\n",
       "                                          raw_report  \n",
       "0                                   FINAL REPORT\\...  \n",
       "1                                   FINAL REPORT\\...  \n",
       "2                                   FINAL REPORT\\...  \n",
       "3                                   FINAL REPORT\\...  \n",
       "4                                   FINAL REPORT\\...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_reports.head()\n",
    "df_raw_reports.to_csv('raw_reports.csv', index=False)\n",
    "\n",
    "\n",
    "df_raw_reports = pd.read_csv('df_raw_reports.csv')\n",
    "\n",
    "df_raw_reports= df_raw_reports.drop_duplicates(subset = [\"raw_report\"])\n",
    "df_raw_reports.reset_index(inplace=True)\n",
    "df_raw_reports = df_raw_reports.drop('index', 1)\n",
    "df_raw_reports = df_raw_reports.drop('Unnamed: 0', 1)\n",
    "df_raw_reports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "69e1fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Vocabulary class\n",
    "##############################\n",
    "\n",
    "class Vocabulary:\n",
    "  \n",
    "    '''\n",
    "    __init__ method is called by default as soon as an object of this class is initiated\n",
    "    we use this method to initiate our vocab dictionaries\n",
    "    '''\n",
    "    def __init__(self, freq_threshold, max_size):\n",
    "        '''\n",
    "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
    "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
    "        '''\n",
    "        #initiate the index to token dict\n",
    "        ## <PAD> -> padding, used for padding the shorter sentences in a batch to match the length of longest sentence in the batch\n",
    "        ## <SOS> -> start token, added in front of each sentence to signify the start of sentence\n",
    "        ## <EOS> -> End of sentence token, added to the end of each sentence to signify the end of sentence\n",
    "        ## <UNK> -> words which are not found in the vocab are replace by this token\n",
    "        self.itos = {0: '<PAD>', 1:'<SOS>', 2:'<EOS>', 3: '<UNK>'}\n",
    "        \n",
    "        #initiate the token to index dict\n",
    "        self.stoi = {k:j for j,k in self.itos.items()} \n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    '''\n",
    "    __len__ is used by dataloader later to create batches\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    '''\n",
    "    a simple tokenizer to split on space and converts the sentence to list of words\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def tokenizer_function(text):\n",
    "        #use the BERT tokenizer here instead!\n",
    "        cleanedReport = re.sub(r'[^\\w\\s]','',text)            # remove punctuation (not word characters and whitespace)\n",
    "        cleanedReport = re.sub('_', '', cleanedReport)        # remove __ in the report\n",
    "        cleanedReport = re.sub(r'[\\d-]', '', cleanedReport)   # remove numbers in the report \n",
    "        cleanedReport = re.sub('\\n', '', cleanedReport)\n",
    "        # need further Lemmatization?\n",
    "        \n",
    "        # change here to use different tokenizer models:\n",
    "        #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        tokenized_report = tokenizer.tokenize(cleanedReport)\n",
    "        \n",
    "        return tokenized_report\n",
    "    \n",
    "    '''\n",
    "    build the vocab: create a dictionary mapping of index to string (itos) and string to index (stoi)\n",
    "    output ex. for stoi -> {'the':5, 'a':6, 'an':7}\n",
    "    '''\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        # calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
    "        frequencies = {}  #init the freq dict\n",
    "        idx = 4 #index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk\n",
    "        \n",
    "        i=0\n",
    "        for sentence in sentence_list:\n",
    "            i+=1\n",
    "            if (i % 10000 == 0):\n",
    "                print(i) \n",
    "            for word in self.tokenizer_function(sentence):\n",
    "                if word not in frequencies.keys():\n",
    "                    frequencies[word]=1\n",
    "                else:\n",
    "                    frequencies[word]+=1\n",
    "              \n",
    "                    \n",
    "        #limit vocab by removing low freq words\n",
    "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
    "        \n",
    "        #limit vocab to the max_size specified\n",
    "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
    "            \n",
    "        #create vocab\n",
    "        for word in frequencies.keys():\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx+=1\n",
    "        return frequencies\n",
    "\n",
    "            \n",
    "    '''\n",
    "    convert the list of words to a list of corresponding indexes\n",
    "    '''    \n",
    "    def numericalize(self, text):\n",
    "        \n",
    "        # change here to use different tokenizer models:\n",
    "        #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        \n",
    "        tokenized_text = [\"[CLS]\"]\n",
    "        tokenized_text += text\n",
    "        tokenized_text.append(\"[SEP]\")\n",
    "        numericalized_report = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        \n",
    "#         # Another encoding method which works on bert-base-uncased tokenizer but not working well with not the others...\n",
    "#         text_str = ' '.join([str(elem) for elem in text])\n",
    "#         encoded = tokenizer.encode_plus(text=text_str,  # the sentence to be encoded\n",
    "#                                         add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "#                                         return_attention_mask = True,  # Generate the attention mask\n",
    "#                                         #return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "#                                         )\n",
    "        \n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "179bdc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Train dataset\n",
    "###############\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_train, target_column, transform=None, freq_threshold = 0,\n",
    "                vocab_max_size = 5000):\n",
    "       \n",
    "        self.df_train = df_train\n",
    "    \n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.target_text = df_train[target_column]\n",
    "\n",
    "        self.report_vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
    "        self.report_vocab.build_vocabulary(self.target_text.tolist())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "  \n",
    "        target_text = self.target_text[index]\n",
    "        tokenised_dict = self.report_vocab.build_vocabulary(self.target_text.tolist())      # already tokenized in build_vocabulary \n",
    "        \n",
    "        numericalized_target_text = []\n",
    "        \n",
    "        for key in tokenised_dict.keys():\n",
    "            numericalized_target_text.append(key)\n",
    "        \n",
    "        numericalized_report = self.report_vocab.numericalize(numericalized_target_text)\n",
    "        \n",
    "        # uncomment the following to convert the list to tensor and return (apparently BERT works without tensor)\n",
    "        # return torch.tensor(numericalized_report)\n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e928962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Validation dataset\n",
    "####################\n",
    "\n",
    "class Validation_Dataset:\n",
    "    def __init__(self, train_dataset, df, target_column, transform = None, freq_threshold = 0, vocab_max_size = 5000):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "        #train dataset will be used as lookup for vocab\n",
    "        self.train_dataset = train_dataset\n",
    "        \n",
    "        #get source and target texts\n",
    "        self.target_texts = self.df[target_column]\n",
    "        \n",
    "        self.report_vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        target_text = self.target_texts[index]\n",
    "        \n",
    "        tokenised_text = self.report_vocab.tokenizer_function(target_text)\n",
    "        numericalized_report = self.report_vocab.numericalize(tokenised_text)\n",
    "\n",
    "        #return torch.tensor(numericalized_report)\n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b8ca71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a little example on just 10 reports\n",
    "#df_train_test = df_raw_reports[0:10]\n",
    "df_train_test = df_raw_reports[0:10]\n",
    "\n",
    "train_dataset = TrainDataset(df_train_test, \"raw_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "55d75f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1103, 185, 1110, 2229, 1105, 1132, 1185, 1104, 1175, 1114, 1286, 13093, 1161, 2999, 170, 2070, 1107, 1509, 2592, 1116, 174, 7577, 9505, 17811, 3621, 2660, 2050, 14196, 8351, 1513, 12602, 3101, 17268, 25632, 1204, 12754, 11937, 1137, 1673, 1818, 12858, 1268, 1106, 8179, 1112, 20994, 12104, 26600, 21217, 175, 1207, 5531, 16418, 27316, 1166, 1965, 20673, 1111, 6873, 11769, 3105, 1607, 14255, 18834, 6617, 1665, 172, 10496, 1822, 2495, 16973, 7209, 16091, 13505, 7637, 1616, 2394, 2967, 1562, 20266, 23298, 22869, 15241, 23123, 1465, 7880, 6533, 1121, 26557, 1233, 6357, 27154, 20557, 7409, 20844, 5815, 8682, 1145, 2136, 2386, 20484, 1306, 1762, 2060, 1596, 2988, 11776, 1353, 11806, 1134, 1113, 6111, 16684, 27372, 2025, 1129, 3839, 5552, 4233, 1115, 1211, 1933, 1439, 3077, 1181, 14701, 1785, 3971, 2382, 1757, 17688, 2330, 1254, 6456, 25984, 7160, 15139, 1643, 21810, 22172, 1714, 1586, 2071, 4487, 1582, 1214, 1385, 1590, 13467, 1183, 1605, 22631, 4696, 1120, 2259, 4035, 18791, 20512, 3528, 4777, 12148, 2489, 17459, 5422, 2554, 24438, 2286, 5837, 7563, 13335, 25669, 2793, 22860, 20497, 3954, 19905, 5996, 3653, 1167, 1193, 25163, 1861, 1180, 1158, 2607, 1373, 1336, 193, 6447, 28117, 15415, 14375, 7501, 8974, 2620, 4248, 15070, 6719, 9046, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 16530, 5001, 10346, 1603, 2184, 191, 2225, 21608, 5332, 1675, 1231, 22832, 1423, 2458, 3161, 1197, 15342, 1548, 1127, 3836, 5351, 10170, 1700, 182, 177, 26466, 26449, 20725, 4903, 15328, 9495, 9072, 11019, 11531, 1499, 3566, 4311, 2129, 4742, 184, 11553, 2285, 4413, 9964, 1171, 1189, 3402, 1144, 1151, 2418, 14235, 1849, 14687, 5048, 14494, 6263, 15970, 180, 26055, 3375, 1396, 22460, 6766, 1404, 24633, 4143, 3013, 1149, 8702, 6066, 5498, 3533, 4455, 12601, 3118, 14880, 15447, 9739, 7648, 1618, 2243, 3313, 1227, 8419, 8830, 11124, 10880, 21329, 1218, 1883, 9356, 1269, 1285, 1907, 2845, 3525, 5408, 1295, 1668, 10552, 28060, 1296, 1133, 2567, 18535, 5173, 2200, 1649, 1901, 5871, 6482, 6506, 12173, 9584, 16388, 1136, 2423, 6858, 1142, 2112, 19807, 5838, 2095, 4138, 5783, 7977, 7688, 4060, 13541, 1780, 1168, 3084, 19840, 22339, 1216, 181, 25698, 4993, 26320, 4844, 2819, 1737, 14739, 1748, 1191, 1834, 7300, 12211, 11015, 20488, 9516, 20899, 1554, 1317, 1808, 1610, 16430, 7874, 14601, 3740, 11848, 13730, 8345, 15243, 4571, 7088, 1259, 4084, 20658, 2473, 20294, 2616, 4184, 22165, 6697, 1894, 25027, 16468, 1906, 3626, 102]\n"
     ]
    }
   ],
   "source": [
    "# we can see the results, e.g.\n",
    "example = train_dataset[0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "257e6b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n"
     ]
    }
   ],
   "source": [
    "print(len(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "64380403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'p', 'is', 'chest', 'and', 'are', 'no', 'of', 'there', 'with', 'left', 'lung', '##a', 'normal', 'a', 'radio', 'in', 'final', 'report', '##s', 'e', 'comparison', 'findings', 'focal', 'card', '##io', '##st', '##inal', 'impression', '##le', '##ural', '##ff', '##usion', '##orax', '##t', 'indication', 'lateral', 'or', '##ne', '##um', '##oth', 'right', 'to', 'examination', 'as', 'consolidation', 'acute', 'pulmonary', '##graphs', 'f', 'new', 'technique', '##media', 'silhouette', 'over', 'process', 'pneumonia', 'for', 'nod', 'op', 'upper', 'history', 'con', '##tour', '##ci', '##c', 'c', 'mild', 'low', 'la', 'clips', 'breast', '##pu', '##lm', '##ona', '##ry', 'media', 'multiple', 'seen', 'projecting', 'rib', 'fracture', '##graph', 'hem', '##ia', '##ph', '##ora', 'from', 'vascular', '##l', 'volumes', 'meta', 'bilateral', '##ac', 'hi', '##lar', 'lungs', 'also', 'provided', '##id', '##rag', '##m', 'heart', 'size', '##ic', 'prior', '##osis', 'small', '##ules', 'which', 'on', 'stable', 'unchanged', '##static', 'study', 'be', 'none', '##ular', '##ities', 'that', 'most', 'project', 'within', 'image', '##d', 'abdomen', '##ity', 'sixth', 'noted', '##ness', 'cardiac', 'clear', 'again', 'remote', '##sided', 'demonstrated', 'portable', '##p', 'cough', 'frontal', 'free', 'air', 'below', '##tra', '##th', 'year', 'old', 'woman', 'surgical', '##y', '##na', '##piration', 'views', 'at', 'base', 'en', '##gor', '##gement', 'thick', '##ening', '##ort', 'pain', 'evaluate', 'dated', 'evidence', 'th', 'mid', 'le', '##uk', '##oc', '##yt', 'recent', 'congestion', 'fi', '##ss', '##acity', 'suggest', 'disease', 'more', '##ly', 'lobe', 'similar', 'could', '##ing', 'changes', 'along', 'may', 'x', '##ray', 'su', 'onset', '##cite', '##val', 'infection', 'likely', 'represent', 'nipple', 'shadows', 'potentially', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', 'posterior', 'seventh', 'ribs', 'short', 'breath', 'v', '##as', '##cula', '##ture', 'present', 're', 'abnormal', 'single', 'view', '##ir', '##r', '##hos', '##is', 'were', 'obtained', 'patient', 'upright', 'position', 'm', 'h', '##pox', 'adequately', '##erated', 'adjacent', '##pical', 'remarkable', 'arch', 'ca', '##fication', 'top', 'possibly', 'representing', 'central', 'somewhat', 'o', '##sse', '##ous', 'structures', 'intact', 'back', 'made', 'compared', 'has', 'been', 'significant', 'interval', 'change', '##rank', 'ed', '##ema', 'limits', 'persistent', 'k', 'wed', '##ging', 've', '##rte', '##bra', 'body', '##grade', 'temperature', 'rule', 'out', '##fi', '##lt', '##rate', 'semi', '##up', '##right', 'remain', 'inn', '##ume', '##rable', 'scattered', 'better', 'middle', '##ure', 'known', '##sta', '##ses', 'presenting', 'fever', 'comparisons', 'well', '##el', '##vis', 'same', 'day', 'available', 'appear', 'allowing', 'differences', 'number', 'round', 'den', '##sities', 'each', 'but', 'numerous', 'discrete', 'visual', '##ized', 'however', 'addition', 'ha', '##zy', 'widespread', 'compatible', 'coin', '##cid', 'not', 'completely', 'characterized', 'this', 'post', '##oper', '##ative', 'wall', 'increasing', '##ification', 'reflect', 'super', '##im', '##posed', 'although', 'other', 'et', '##iol', '##ogies', 'such', 'l', '##ymph', '##ang', '##itic', 'pattern', 'spread', 'considered', 'helpful', 'further', 'if', 'needed', 'clinical', 'exam', '##old', '##pra', '##lav', '##icular', 'full', 'several', 'months', 'car', '##cin', '##oid', 'tumor', '##rt', '##uo', '##sity', 'posts', '##urg', '##ical', '##ith', 'including', '##ura', 'costa', 'surface', 'blunt', 'cost', '##op', '##hren', '##cus', 'red', '##emon', '##stra', '##ted', 'identified', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# You can convert it back from IDs to see the tokens, example:\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "print(tokenizer.convert_ids_to_tokens(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "20a144b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length \n",
    "# This is an error, should only be 512 (BERT models max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
