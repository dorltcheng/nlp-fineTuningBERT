{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d289c3a",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1e44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# All imports \n",
    "##################\n",
    "\n",
    "#sys libs\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data manupulation libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "#pandarallel.initialize()\n",
    "\n",
    "#string manupulation libs\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import spacy\n",
    "\n",
    "#torch libs\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "# data manipulations\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import pydicom\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a0aace",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9859bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>50414267</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>53189527</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>53911762</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>56699142</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s56699142/ea030e7a-2e3b134...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>57375967</td>\n",
       "      <td>10000764</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000764/s57375967/096052b7-d256dc4...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  study_id  subject_id  split  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n",
       "1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n",
       "2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n",
       "3  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  56699142    10000032  train   \n",
       "4  096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4  57375967    10000764  train   \n",
       "\n",
       "                                                path  \\\n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...   \n",
       "1  files/p10/p10000032/s53189527/2a2277a9-b0ded15...   \n",
       "2  files/p10/p10000032/s53911762/68b5c4b1-227d048...   \n",
       "3  files/p10/p10000032/s56699142/ea030e7a-2e3b134...   \n",
       "4  files/p10/p10000764/s57375967/096052b7-d256dc4...   \n",
       "\n",
       "                                          raw_report  \n",
       "0                                   FINAL REPORT\\...  \n",
       "1                                   FINAL REPORT\\...  \n",
       "2                                   FINAL REPORT\\...  \n",
       "3                                   FINAL REPORT\\...  \n",
       "4                                   FINAL REPORT\\...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used to make the csv with raw reports, not really important anymore once we have the reports in a csv\n",
    "cxr_root_path = \"/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/\"\n",
    "df_record = pd.read_csv('{}/cxr-record-list.csv.gz'.format(cxr_root_path), sep=',')\n",
    "df_split = pd.read_csv('{}/mimic-cxr-2.0.0-split.csv.gz'.format(cxr_root_path))\n",
    "\n",
    "df_temp = df_split.merge(df_record, on=['subject_id', 'study_id', 'dicom_id'], how='left')\n",
    "\n",
    "df_sections = pd.read_csv('{}/mimic-cxr-sections/mimic_cxr_sectioned.csv'.format(cxr_root_path))\n",
    "\n",
    "# if you already have the df_raw_reports.csv just uncomment the following line and skipp to the vocabulary class\n",
    "\n",
    "# extract the csv into a dataframe\n",
    "df_raw_reports = pd.read_csv('df_raw_reports.csv')\n",
    "# discard duplicates of the reports as there were originally more rows than reports (as there is sometimes more images per study)\n",
    "df_raw_reports= df_raw_reports.drop_duplicates(subset = [\"raw_report\"])\n",
    "#restart the index column as it was filtered from larger data and the indices were messed up\n",
    "df_raw_reports.reset_index(inplace=True)\n",
    "#discard some unimportant columns\n",
    "df_raw_reports = df_raw_reports.drop('index', 1)\n",
    "df_raw_reports = df_raw_reports.drop('Unnamed: 0', 1)\n",
    "# print the head to check if looks like intended\n",
    "df_raw_reports.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ed1a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377110, 4)\n",
      "5159\n"
     ]
    }
   ],
   "source": [
    "# Checking test dataset number\n",
    "df_split.head()\n",
    "print(df_split.shape)\n",
    "\n",
    "split = df_split[\"split\"].tolist()\n",
    "idx = 0\n",
    "for elem in split:\n",
    "    if elem == \"test\":\n",
    "        idx+=1\n",
    "\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977f5ad",
   "metadata": {},
   "source": [
    "### Creating raw reports dataframe (Skip if done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760bb9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_sections = df_sections.rename(columns={'study': 'study_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe34b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the raw report to the sections df\n",
    "\n",
    "\n",
    "paths = df_record[\"path\"].tolist()\n",
    "study_ids = df_record[\"study_id\"].tolist()\n",
    "\n",
    "study_ids_new=[]\n",
    "\n",
    "for study in study_ids:\n",
    "    \n",
    "    new_id = 's' + str(study)\n",
    "    study_ids_new.append(new_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c6cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p10\n",
      "p11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9ce47c1e5491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mdf_raw_reports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_raw_reports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9ce47c1e5491>\u001b[0m in \u001b[0;36msearchDirectory\u001b[0;34m(mainDirectory, df_raw_reports)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubDirectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch171/lib/python3.6/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creates dataframe with study id and raw reports, prints the name of the main directory on enter so you can see the progress\n",
    "# overall there is p10-p19 directories, should take around 25-30 min to run as far as I remember\n",
    "\n",
    "import ntpath\n",
    "\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def searchDirectory(mainDirectory,df_raw_reports):\n",
    "\n",
    "\n",
    "    directory = '/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/files/' + mainDirectory\n",
    "    report_ext = '.txt'\n",
    "    image_ext = '.jpg'\n",
    "    \n",
    "    print(mainDirectory)\n",
    "    for subDirectory in os.listdir(directory):\n",
    "        full_path = directory + '/' + subDirectory\n",
    "       \n",
    "        for root, dirs, files in os.walk(full_path):\n",
    "            \n",
    "            for filename in files:\n",
    "\n",
    "                if filename.endswith(report_ext):\n",
    "                    with open(os.path.join(root, filename), 'r') as report:\n",
    "                        contents = report.read()\n",
    "                    \n",
    "                    df_temp = {\"study_id\": path_leaf(filename)[1:9], \"raw_report\": contents}\n",
    "                    df_raw_reports = df_raw_reports.append(df_temp, ignore_index = True)\n",
    "        \n",
    "    return df_raw_reports\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "directories = ['p10','p11','p12','p13','p14','p15','p16','p17', 'p18', 'p19' ]\n",
    "\n",
    "\n",
    "df_raw_reports = pd.DataFrame([], columns = ['study_id', 'raw_report'])\n",
    "\n",
    "for directory in directories:\n",
    "    df_raw_reports = searchDirectory(directory, df_raw_reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d8f7182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>50414267</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>53189527</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>53911762</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>56699142</td>\n",
       "      <td>10000032</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000032/s56699142/ea030e7a-2e3b134...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>57375967</td>\n",
       "      <td>10000764</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10000764/s57375967/096052b7-d256dc4...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  study_id  subject_id  split  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n",
       "1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n",
       "2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n",
       "3  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  56699142    10000032  train   \n",
       "4  096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4  57375967    10000764  train   \n",
       "\n",
       "                                                path  \\\n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...   \n",
       "1  files/p10/p10000032/s53189527/2a2277a9-b0ded15...   \n",
       "2  files/p10/p10000032/s53911762/68b5c4b1-227d048...   \n",
       "3  files/p10/p10000032/s56699142/ea030e7a-2e3b134...   \n",
       "4  files/p10/p10000764/s57375967/096052b7-d256dc4...   \n",
       "\n",
       "                                          raw_report  \n",
       "0                                   FINAL REPORT\\...  \n",
       "1                                   FINAL REPORT\\...  \n",
       "2                                   FINAL REPORT\\...  \n",
       "3                                   FINAL REPORT\\...  \n",
       "4                                   FINAL REPORT\\...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_reports.head()\n",
    "df_raw_reports.to_csv('raw_reports.csv', index=False)\n",
    "\n",
    "\n",
    "df_raw_reports = pd.read_csv('df_raw_reports.csv')\n",
    "\n",
    "df_raw_reports= df_raw_reports.drop_duplicates(subset = [\"raw_report\"])\n",
    "df_raw_reports.reset_index(inplace=True)\n",
    "df_raw_reports = df_raw_reports.drop('index', 1)\n",
    "df_raw_reports = df_raw_reports.drop('Unnamed: 0', 1)\n",
    "df_raw_reports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23801d5",
   "metadata": {},
   "source": [
    "### Classes for the Data Loader\n",
    "- Include: loading data, text preprocessing, words frequency check, tokenization, tokens-IDs-conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c24566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Vocabulary class\n",
    "##############################\n",
    "\n",
    "class Vocabulary:\n",
    "  \n",
    "    '''\n",
    "    __init__ method is called by default as soon as an object of this class is initiated\n",
    "    we use this method to initiate our vocab dictionaries\n",
    "    '''\n",
    "    def __init__(self, freq_threshold, max_size):\n",
    "        '''\n",
    "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
    "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
    "        '''\n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    a simple tokenizer to split on space and converts the sentence to list of words\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def preprocessing(text):\n",
    "        cleanedReport = re.sub(r'[^\\w\\s]','',text)            # remove punctuation (not word characters and whitespace)\n",
    "        cleanedReport = re.sub('_', '', cleanedReport)        # remove __ in the report\n",
    "        cleanedReport = re.sub(r'[\\d-]', '', cleanedReport)   # remove numbers in the report \n",
    "        cleanedReport = re.sub('\\n', '', cleanedReport)\n",
    "        # need further Lemmatization?\n",
    "        return cleanedReport                                  # should be just a string\n",
    "    \n",
    "    '''\n",
    "    vocabulary frequency check:\n",
    "    '''\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        # calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
    "        frequencies = {}  #init the freq dict\n",
    "        #idx = 4 #index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk\n",
    "        idx = 4\n",
    "        \n",
    "        i=0\n",
    "        for sentence in sentence_list:\n",
    "            i+=1\n",
    "            if (i % 10000 == 0):\n",
    "                print(i) \n",
    "                \n",
    "            preprocessed = self.preprocessing(sentence)\n",
    "            preprocessedList = list(preprocessed.split(\" \"))\n",
    "            for word in preprocessedList:\n",
    "                if word not in frequencies.keys():\n",
    "                    frequencies[word]=1\n",
    "                else:\n",
    "                    frequencies[word]+=1\n",
    "              \n",
    "                    \n",
    "        #limit vocab by removing low freq words\n",
    "        frequencies = {k:v for k,v in frequencies.items() if v > self.freq_threshold} \n",
    "        \n",
    "        #limit vocab to the max_size specified\n",
    "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
    "            \n",
    "        return frequencies\n",
    "\n",
    "            \n",
    "    '''\n",
    "    tokenization and converting tokens to IDs\n",
    "    '''    \n",
    "    def tokenization(self, text):\n",
    "        \n",
    "        # change here to use different tokenizer models:\n",
    "        #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        tokenized_text = tokenizer.tokenize(text)\n",
    "        \n",
    "        tokenized_report = [\"[CLS]\"]\n",
    "        tokenized_report += tokenized_text\n",
    "        tokenized_report.append(\"[SEP]\")\n",
    "        numericalized_report = tokenizer.convert_tokens_to_ids(tokenized_report)\n",
    "        \n",
    "#         # Another encoding method which works on bert-base-uncased tokenizer but not working well with not the others...\n",
    "#         text_str = ' '.join([str(elem) for elem in text])\n",
    "#         encoded = tokenizer.encode_plus(text=text_str,  # the sentence to be encoded\n",
    "#                                         add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "#                                         return_attention_mask = True,  # Generate the attention mask\n",
    "#                                         #return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "#                                         )\n",
    "        \n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "03bafcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Train dataset\n",
    "###############\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_train, target_column, transform=None, freq_threshold = 0,\n",
    "                vocab_max_size = 5000):\n",
    "       \n",
    "        self.df_train = df_train\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.target_text = df_train[target_column]\n",
    "\n",
    "        self.report_vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
    "        self.frequencyDict = self.report_vocab.build_vocabulary(self.target_text.tolist())  # build vocab for whole thing (list of whole thing)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_train)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "  \n",
    "        target_text = self.target_text[index]                             # string\n",
    "        target_text = self.report_vocab.preprocessing(target_text)\n",
    "        \n",
    "        for word in target_text.split():\n",
    "            if word not in self.frequencyDict.keys():\n",
    "                target_text = target_text.replace(word, \"\")\n",
    "            \n",
    "        numericalized_report = self.report_vocab.tokenization(target_text)\n",
    "        \n",
    "        # uncomment the following to convert the list to tensor and return (apparently BERT works without tensor)\n",
    "        # return torch.tensor(numericalized_report)\n",
    "        return numericalized_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a676d8",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc537377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a little example on just 10 reports\n",
    "#df_train_test = df_raw_reports[0:10]\n",
    "df_train_test = df_raw_reports[0:10]\n",
    "\n",
    "train_dataset = TrainDataset(df_train_test, \"raw_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8562c013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 175, 1114, 1207, 15415, 1112, 14375, 1116, 174, 7501, 1111, 8974, 5531, 2229, 185, 1161, 1105, 11937, 7577, 3839, 9505, 1175, 1110, 1185, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 1115, 1211, 2620, 4248, 15070, 6719, 1103, 3621, 2660, 16418, 2050, 14196, 27316, 1110, 2999, 16973, 1933, 1166, 1103, 1286, 13093, 9046, 1439, 1103, 7209, 1103, 3077, 1181, 3105, 14701, 1110, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 1104, 1103, 16530, 1286, 3971, 1105, 5001, 10346, 1132, 2382, 8351, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102]\n",
      "\n",
      "\n",
      "Number of tokens:  114\n"
     ]
    }
   ],
   "source": [
    "# we can see the results, e.g.\n",
    "example = train_dataset[0]\n",
    "print(example)\n",
    "print(\"\\n\")\n",
    "print(\"Number of tokens: \", len(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88e379f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens conversion:\n",
      "['[CLS]', 'final', 'report', 'examination', 'chest', 'p', '##a', 'and', 'la', '##t', 'indication', 'f', 'with', 'new', 'onset', 'as', '##cite', '##s', 'e', '##val', 'for', 'infection', 'technique', 'chest', 'p', '##a', 'and', 'lateral', 'comparison', 'none', 'findings', 'there', 'is', 'no', 'focal', 'consolidation', 'p', '##le', '##ural', 'e', '##ff', '##usion', 'or', 'p', '##ne', '##um', '##oth', '##orax', 'bilateral', 'nod', '##ular', 'op', '##ac', '##ities', 'that', 'most', 'likely', 'represent', 'nipple', 'shadows', 'the', 'card', '##io', '##media', '##st', '##inal', 'silhouette', 'is', 'normal', 'clips', 'project', 'over', 'the', 'left', 'lung', 'potentially', 'within', 'the', 'breast', 'the', 'image', '##d', 'upper', 'abdomen', 'is', 'un', '##rem', '##ark', '##able', 'chronic', 'def', '##orm', '##ity', 'of', 'the', 'posterior', 'left', 'sixth', 'and', 'seventh', 'ribs', 'are', 'noted', 'impression', 'no', 'acute', 'card', '##io', '##pu', '##lm', '##ona', '##ry', 'process', '[SEP]']\n",
      "\n",
      "\n",
      "Decoding:\n",
      "[CLS] final report examination chest pa and lat indication f with new onset ascites eval for infection technique chest pa and lateral comparison none findings there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted impression no acute cardiopulmonary process [SEP]\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Conversion back to tokens:\n",
    "print(\"Tokens conversion:\")\n",
    "print(tokenizer.convert_ids_to_tokens(example))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Decode it back into sentences (tokens of subwords will be merged together)\n",
    "print(\"Decoding:\")\n",
    "print(tokenizer.decode(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce41d3c",
   "metadata": {},
   "source": [
    "### Chunking to unify token lengths of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b19a90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(dataset, df, chunk_size = 128):\n",
    "    \n",
    "    concat = []\n",
    "    for i in range(len(dataset)):     # Add all tokens together in one list\n",
    "        concat.extend(dataset[df.index[i]])\n",
    "        #print(i)      # print to check the progress\n",
    "        \n",
    "    total_length = len(concat)\n",
    "    print(\"Total concatenated length: \", total_length)\n",
    "    \n",
    "    chunks = lambda concat, chunk_size: [concat[i:i+chunk_size] for i in range(0, total_length, chunk_size)]  \n",
    "    chunked_text = chunks(concat, chunk_size)\n",
    "    \n",
    "    if len(chunked_text[-1]) < chunk_size:\n",
    "        print(\"Removed last chunk: \", len(chunked_text[-1]), \" tokens\")\n",
    "        chunked_text.pop()     # remove last element \n",
    "    \n",
    "    return chunked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4c095b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Total concatenated length:  563\n",
      "Removed last chunk:  51  tokens\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE:\n",
    "example_df = df_raw_reports.sample(n=5)\n",
    "example_dataset = TrainDataset(example_df, \"raw_report\")\n",
    "chunked_text = chunking(example_dataset, example_df)\n",
    "\n",
    "with open(\"chunked_example.txt\", \"w\") as output:\n",
    "    output.write(str(chunked_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "29bbd616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] final report examination chest pa and lat indication history m with left arm and right leg numbness ro chf pneumonia technique chest pa and lateral comparison findings the lungs are clearthe cardiac hilar and mediastinal contours are normalno pleural abnormality is seen intact median sternal wires considerable calcification of the descending thoracic aorta impression considerable calcification of the descending thoracic aorta but no evidence of edema or pneumonia [SEP] [CLS] final report portable ap chest from at clinical indication yearold with hcv and hcc who was noted to\n",
      "\n",
      "\n",
      "be hypoxic pneumonia versus edema comparison is made to the patients prior study of at portable upright chest film at is submitted impression persistent low lung volumes with interval appearance of patchy bibasilar opacities which could reflect patchy atelectasis although pneumonia or aspiration should also be considered no evidence of pulmonary edema although there is crowding of the pulmonary vasculature most likely related to the low lung volumes no pneumothorax no pleural effusions overall cardiac and mediastinal contours are likely stable [SEP] [CLS] final report examination chest portable ap indication year old man with trac\n",
      "\n",
      "\n",
      "##h interval change interval change impression in comparison with the study of there is some improvement in the opacification at the right base the right mid zone and left upper zone are little change tracheostomy tube remains in place [SEP] [CLS] final report examination chest pa and lat indication history m with increased confusion and weakness eval for pneumonia technique chest pa and lateral comparison chest radiograph findings lung volumes are low cardiac silhouette size is normal and unchanged mediastinal and hilar contours are similar pulmonary vasculature is normal linear opacities in the lung bases are compatible with areas of subsegmental atelectasis\n",
      "\n",
      "\n",
      "no focal consolidation pleural effusion or pneumothorax is detected impression low lung volumes with bibasilar subsegmental atelectasis no radiographic evidence for pneumonia [SEP] [CLS] final report examination chest portable ap indication year old woman with lle dry gangrene cardiopulmonary process surg tma cardiopulmonary process comparison chest radiographs and impression previous bibasilar consolidation has cleared pulmonary vascular congestion has increased and a new diffuse interstitial abnormality could be edema conventional chest radiographs with better resolution recommended to confirm that the diffuse abnormality\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunked_text:\n",
    "    print(tokenizer.decode(chunk))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5d73d6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.cuda.device object at 0x7f51f5d08cc0>\n",
      "GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ffb4663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For all training reports:\n",
    "# trainDf = df_raw_reports[0:len(df_raw_reports.index)]\n",
    "# train_dataset = TrainDataset(trainDf, \"raw_report\")\n",
    "# chunked_train_dataset = chunking(train_dataset, trainDf)    # chunk size = 128 \n",
    "\n",
    "# # Save as .txt file:\n",
    "# with open(\"chunked_trainDataset.txt\", \"w\") as output:\n",
    "#     output.write(str(chunked_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a273176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74238</th>\n",
       "      <td>35b729df-ba6798a6-84f4eed8-be928459-f6e1323b</td>\n",
       "      <td>58155159</td>\n",
       "      <td>13332955</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p13/p13332955/s58155159/35b729df-ba6798a...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69839</th>\n",
       "      <td>0f89f810-7ce5a44d-5b99a560-06f1c85a-a5aabc52</td>\n",
       "      <td>50936682</td>\n",
       "      <td>13145776</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p13/p13145776/s50936682/0f89f810-7ce5a44...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180909</th>\n",
       "      <td>61081955-7266d045-757eae92-978b5127-81ac4054</td>\n",
       "      <td>59716009</td>\n",
       "      <td>18128235</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p18/p18128235/s59716009/61081955-7266d04...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161458</th>\n",
       "      <td>292e12fa-9b7ceda8-60de0a25-2b73d563-82a30b05</td>\n",
       "      <td>57050873</td>\n",
       "      <td>17260918</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p17/p17260918/s57050873/292e12fa-9b7ceda...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13402</th>\n",
       "      <td>05b21ec9-575c84e1-731c557e-d7b188e9-33900cbd</td>\n",
       "      <td>54367541</td>\n",
       "      <td>10610208</td>\n",
       "      <td>train</td>\n",
       "      <td>files/p10/p10610208/s54367541/05b21ec9-575c84e...</td>\n",
       "      <td>FINAL REPORT\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            dicom_id  study_id  subject_id  \\\n",
       "74238   35b729df-ba6798a6-84f4eed8-be928459-f6e1323b  58155159    13332955   \n",
       "69839   0f89f810-7ce5a44d-5b99a560-06f1c85a-a5aabc52  50936682    13145776   \n",
       "180909  61081955-7266d045-757eae92-978b5127-81ac4054  59716009    18128235   \n",
       "161458  292e12fa-9b7ceda8-60de0a25-2b73d563-82a30b05  57050873    17260918   \n",
       "13402   05b21ec9-575c84e1-731c557e-d7b188e9-33900cbd  54367541    10610208   \n",
       "\n",
       "        split                                               path  \\\n",
       "74238   train  files/p13/p13332955/s58155159/35b729df-ba6798a...   \n",
       "69839   train  files/p13/p13145776/s50936682/0f89f810-7ce5a44...   \n",
       "180909  train  files/p18/p18128235/s59716009/61081955-7266d04...   \n",
       "161458  train  files/p17/p17260918/s57050873/292e12fa-9b7ceda...   \n",
       "13402   train  files/p10/p10610208/s54367541/05b21ec9-575c84e...   \n",
       "\n",
       "                                               raw_report  \n",
       "74238                                    FINAL REPORT\\...  \n",
       "69839                                    FINAL REPORT\\...  \n",
       "180909                                   FINAL REPORT\\...  \n",
       "161458                                   FINAL REPORT\\...  \n",
       "13402                                    FINAL REPORT\\...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_reports_sampled = df_raw_reports.sample(n=20000)\n",
    "print(df_raw_reports_sampled.shape)\n",
    "df_raw_reports_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db008f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20000 samples first:\n",
    "train_dataset = TrainDataset(df_raw_reports_sampled, \"raw_report\")\n",
    "chunked_train_dataset20000 = chunking(train_dataset, df_raw_reports_sampled)\n",
    "\n",
    "# Save as .txt file:\n",
    "with open(\"chunked_trainDataset20000.txt\", \"w\") as output:\n",
    "    output.write(str(chunked_train_dataset20000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429237d7",
   "metadata": {},
   "source": [
    "Result:\n",
    "- Total concatenated length:  2350254\n",
    "- Removed last chunk:  46  tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c8aa8b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  18361\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples: \", len(chunked_train_dataset20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13807b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
