{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0453bbd0",
   "metadata": {},
   "source": [
    "# Fine-tuning ClinicalBERT (Model training) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc47e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18090005",
   "metadata": {},
   "source": [
    "### 1. Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ebf570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5b7ef2585281ad5b\n",
      "Reusing dataset json (/media/SharedUsers/dlc19/home/.cache/huggingface/datasets/json/default-5b7ef2585281ad5b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cad02edc464b279f021db02049e50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 206331\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 3345\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\" : \"final_dataset_chunkedtrain.jsonl\",\n",
    "    \"test\" : \"final_dataset_chunkedtest.jsonl\"    \n",
    "}\n",
    "\n",
    "chunkedDataset = load_dataset(\"json\", data_files = data_files)\n",
    "chunkedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4144352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'list'>\n",
      "Length:  128\n",
      "[101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 175, 1114, 1207, 15415, 1112, 14375, 1116, 174, 7501, 1111, 8974, 5531, 2229, 185, 1161, 1105, 11937, 7577, 3839, 9505, 1175, 1110, 1185, 17811, 20994, 185, 1513, 12602, 174, 3101, 17268, 1137, 185, 1673, 1818, 12858, 25632, 20557, 6873, 5552, 11769, 7409, 4233, 1115, 1211, 2620, 4248, 15070, 6719, 1103, 3621, 2660, 16418, 2050, 14196, 27316, 1110, 2999, 16973, 1933, 1166, 1103, 1286, 13093, 9046, 1439, 1103, 7209, 1103, 3077, 1181, 3105, 14701, 1110, 8362, 16996, 23822, 1895, 13306, 19353, 24211, 1785, 1104, 1103, 16530, 1286, 3971, 1105, 5001, 10346, 1132, 2382, 8351, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 1965, 102, 101, 1509, 2592, 8179, 2229, 185, 1161, 1105, 2495, 1204, 12754, 1607, 175, 1114]\n"
     ]
    }
   ],
   "source": [
    "example = chunkedDataset[\"train\"][\"input_ids\"][0]\n",
    "print(\"Type: \", type(example))\n",
    "print(\"Length: \", len(example))\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec6e6f",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051dbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole-word-masking function should return our own default data collator \n",
    "# and then will use that data collator instead!!! \n",
    "# for now using the default data collator first \n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# replace this data_collator with whole-word-masking-data-collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b806e8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLS] final [MASK] examination chest pa [MASK] lat [MASK] [MASK] with [MASK] onset ascites eval for infectionzong chest pa and lateral [MASK] none findings there is no focal consolidation ple [MASK] effusion or pneumothorax bilateral nodular opac [MASK] that most likely represent nipple shadows the cardiomediastinal silhouette is [MASK] clips project over the left lung potentially within the breast [MASK] image [MASK] upper abdomen [MASK] unremark [MASK] chronic deformity of the posterior left sixth and seventh ribs are noted impression no acute cardiopu [MASK]onary process [SEP] [CLS] final report [MASK] chest [MASK]a and lat indication history f with\n",
      "\n",
      "[MASK]ness of breath [MASK] chest pa and lateral comparison findings the cardiac mediastinal and hilar con [MASK]s are normal pulmonary vascula [MASK] [MASK] [MASK] lungs are clear no pleural eff [MASK] or pneumothorax is present [MASK] clips are again seen projecting over the left breast remote left [MASK] rib fractures are also re demonstrated impression no acute cardiopulmonary abnormality [SEP] [CLS] final report examination chest [MASK] ap indication f with [MASK] Benton process comparison chest radiograph [MASK] single [MASK] view of the chest provided there is no focal consolidation e [MASK]usion or pneumothorax the cardiomediastinal\n"
     ]
    }
   ],
   "source": [
    "example_batch = [chunkedDataset[\"train\"][i] for i in range(2)]\n",
    "for sample in example_batch:\n",
    "    _ = sample.pop(\"word_ids\")  # limit it to only the required columns\n",
    "\n",
    "for chunk in data_collator(example_batch)[\"input_ids\"]:\n",
    "    print(f\"\\n{tokenizer.decode(chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0818bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate this source of randomness is to apply the masking once on the whole test set,\n",
    "# and then use the default data collator in ðŸ¤— Transformers to collect the batches during evaluation\n",
    "\n",
    "# replace data_collator here with the whole-word-masking ones\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3652fea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 206331\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3345\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkedDataset = chunkedDataset.remove_columns([\"word_ids\"])\n",
    "chunkedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db1c7f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /media/SharedUsers/dlc19/home/.cache/huggingface/datasets/json/default-5b7ef2585281ad5b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-64fc3915ad5805f3.arrow\n"
     ]
    }
   ],
   "source": [
    "# Apply this function to our test set and drop the unmasked columns so we can replace them with the masked ones\n",
    "eval_chunkedDataset = chunkedDataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=chunkedDataset[\"test\"].column_names,\n",
    ")\n",
    "eval_chunkedDataset = eval_chunkedDataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_token_type_ids\": \"token_type_ids\", \n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e56d",
   "metadata": {},
   "source": [
    "### 3. Set up Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed46268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    chunkedDataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator, # replace self-defined whole-word-masking-data-collator\n",
    ")\n",
    "\n",
    "# Use the default_data_collator from Transformers for the evaluation set\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_chunkedDataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde58a4",
   "metadata": {},
   "source": [
    "### 4. Steps for training with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ae41b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Is it correct lolllll?\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# https://huggingface.co/docs/transformers/model_doc/auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f85550e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer \n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfe5242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, \n",
    "    optimizer, \n",
    "    train_dataloader, \n",
    "    eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48758953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler:\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 2 # change this later\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9a56515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dorltcheng/CXR_BioClinicalBERT_v1'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### Saving onto Huggingface hub ###########\n",
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"CXR_BioClinicalBERT_v1\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0fac68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/SharedUsers/dlc19/home/codes/nlp-fineTuningBERT/CXR_BioClinicalBERT_v1 is already a clone of https://huggingface.co/dorltcheng/CXR_BioClinicalBERT_v1. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "output_dir = model_name\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217fbe5",
   "metadata": {},
   "source": [
    "### 5. Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f775f358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7fdfca3475c0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a5d1810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46265cf8158d419d939d84057a52c1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6448 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 1.7190158058929503\n",
      ">>> Epoch 1: Perplexity: 1.7190158058929503\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "perplexities = []\n",
    "\n",
    "for epoch in range(num_train_epochs): # for now try 2 epochs and see what happen\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_chunkedDataset)]\n",
    "    \n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "    \n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a19e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7190158058929503, 1.7190158058929503]\n",
      "tensor([0.4909, 0.4909, 0.4909,  ..., 0.6640, 0.6640, 0.6640], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(perplexities)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecf7f0",
   "metadata": {},
   "source": [
    "### 6. Using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37c0f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler_ft = pipeline(\n",
    "    \"fill-mask\", model=\"dorltcheng/CXR_BioClinicalBERT_v1\")\n",
    "\n",
    "mask_filler_original = pipeline(\n",
    "    \"fill-mask\", model=\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35d38877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for text1:\n",
      "1. Pre-trained Bio_ClinicalBERT\n",
      ">>> there are no signs of bleeding.\n",
      ">>> there are no signs of infection.\n",
      ">>> there are no signs of withdrawal.\n",
      ">>> there are no signs of change.\n",
      ">>> there are no signs of distress.\n",
      "\n",
      "2. Fine-tuned CXR_Bio_ClinicalBERT_v1\n",
      ">>> there are no signs of pneumonia.\n",
      ">>> there are no signs of complications.\n",
      ">>> there are no signs of failure.\n",
      ">>> there are no signs of tension.\n",
      ">>> there are no signs of congestion.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"There are no signs of [MASK].\"\n",
    "\n",
    "preds1_ft = mask_filler_ft(text1)\n",
    "preds1_org = mask_filler_original(text1)\n",
    "\n",
    "print(\"Predictions for text1:\")\n",
    "print(\"1. Pre-trained Bio_ClinicalBERT\")\n",
    "for pred in preds1_org:\n",
    "    print(f\">>> {pred['sequence']}\")\n",
    "print()\n",
    "print(\"2. Fine-tuned CXR_Bio_ClinicalBERT_v1\")\n",
    "for pred in preds1_ft:\n",
    "    print(f\">>> {pred['sequence']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9572361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for text2:\n",
      "1. Pre-trained Bio_ClinicalBERT\n",
      ">>> the patient suffered from?\n",
      ">>> the patient suffered from a\n",
      ">>> the patient suffered from pneumonia\n",
      ">>> the patient suffered from anxiety\n",
      ">>> the patient suffered from radiation\n",
      "\n",
      "2. Fine-tuned CXR_Bio_ClinicalBERT_v1\n",
      ">>> the patient suffered from pneumonia\n",
      ">>> the patient suffered from fall\n",
      ">>> the patient suffered from trauma\n",
      ">>> the patient suffered from diabetes\n",
      ">>> the patient suffered from surgery\n"
     ]
    }
   ],
   "source": [
    "text2 = \"The patient suffered from [MASK]. \"\n",
    "preds2_ft = mask_filler_ft(text2)\n",
    "preds2_org = mask_filler_original(text2)\n",
    "\n",
    "print(\"Predictions for text2:\")\n",
    "print(\"1. Pre-trained Bio_ClinicalBERT\")\n",
    "for pred in preds2_org:\n",
    "    print(f\">>> {pred['sequence']}\")\n",
    "print()\n",
    "print(\"2. Fine-tuned CXR_Bio_ClinicalBERT_v1\")\n",
    "for pred in preds2_ft:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726207ed",
   "metadata": {},
   "source": [
    "### For Trainer API (not in use) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Trainer API (not used I guess) ###########\n",
    "batch_size = 64\n",
    "\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = \"clinicalBERT\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-FT-mimicCXR-chunked\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",  #do_eval = True\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
